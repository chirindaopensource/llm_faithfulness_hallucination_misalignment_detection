{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUKrM2fmTdkn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "# Semantic Divergence Metrics for LLM Hallucination Detection\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![Pydantic](https://img.shields.io/badge/Pydantic-e92063.svg?style=flat&logo=pydantic&logoColor=white)](https://pydantic-docs.helpmanual.io/)\n",
        "[![OpenAI](https://img.shields.io/badge/OpenAI-412991.svg?style=flat&logo=openai&logoColor=white)](https://openai.com/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.10192-b31b1b.svg)](https://arxiv.org/abs/2508.10192)\n",
        "[![Research](https://img.shields.io/badge/Research-LLM%20Evaluation-green)](https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-NLP%20%26%20Info.%20Theory-blue)](https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Semantic%20Divergence-orange)](https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models\"** by:\n",
        "\n",
        "*   Igor Halperin\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for detecting faithfulness hallucinations (confabulations) in Large Language Models (LLMs). It moves beyond traditional prompt-agnostic methods by introducing a prompt-aware, ensemble-based approach that measures the semantic consistency of LLM responses across multiple, semantically equivalent paraphrases of a user's query. The goal is to provide a transparent, robust, and computationally efficient toolkit for researchers and practitioners to replicate, validate, and apply the Semantic Divergence Metrics (SDM) framework.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_sdm_analysis](#key-callable-execute_sdm_analysis)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models.\" The core of this repository is the iPython Notebook `faithfulness_hallucination_misalignment_detection_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial configuration validation to the final calculation of the SDM scores and a full suite of robustness checks.\n",
        "\n",
        "Traditional hallucination detection methods often measure the diversity of answers to a single, fixed prompt. This can fail to distinguish between a healthy, multifaceted answer and a genuinely unstable, confabulatory one. This project implements the SDM framework, which introduces a more rigorous, prompt-aware methodology.\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate and structure a complete experimental configuration using Pydantic.\n",
        "-   Automatically generate a high-quality corpus of semantically equivalent prompt paraphrases.\n",
        "-   Efficiently generate a matrix of LLM responses using fault-tolerant, asynchronous API calls.\n",
        "-   Transform the raw text corpus into a shared semantic topic space via joint embedding and hierarchical clustering.\n",
        "-   Calculate a full suite of information-theoretic (JSD, KL Divergence) and geometric (Wasserstein Distance) metrics.\n",
        "-   Aggregate these metrics into the final, interpretable scores for **Semantic Instability ($S_H$)** and **Semantic Exploration (KL)**.\n",
        "-   Execute a full suite of robustness checks to validate the stability of the framework itself.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in information theory, statistics, and natural language processing, providing a quantitative framework for measuring the alignment between a prompt and a response.\n",
        "\n",
        "**1. Ensemble-Based Testing:**\n",
        "The core innovation is to test for a deeper form of arbitrariness. Instead of just generating $N$ answers to a single prompt $Q$, the framework first generates $M$ semantically equivalent paraphrases $\\{Q_1, ..., Q_M\\}$. Then, for each $Q_m$, it generates $N$ answers. This $M \\times N$ response matrix allows for the measurement of consistency across both multiple answers *and* multiple prompt phrasings.\n",
        "\n",
        "**2. Joint Semantic Clustering:**\n",
        "All sentences from both the $M$ prompts and the $M \\times N$ answers are embedded into a common high-dimensional vector space. A single clustering algorithm (Hierarchical Agglomerative Clustering with Ward's linkage) is applied to this joint set of embeddings. This creates a shared, discrete \"topic space\" where semantically similar sentences are assigned the same topic label, regardless of whether they came from a prompt or a response.\n",
        "\n",
        "**3. Semantic Divergence Metrics:**\n",
        "From the topic assignments, topic probability distributions are created for the prompts ($P$) and the answers ($A$). The divergence between these is quantified using:\n",
        "-   **Jensen-Shannon Divergence ($D_{JS}$):** A symmetric, bounded measure of the dissimilarity between the prompt and answer topic distributions.\n",
        "    $$ D_{JS}(P||A) = \\frac{1}{2}(D_{KL}(P||M) + D_{KL}(A||M)), \\quad M = \\frac{1}{2}(P+A) $$\n",
        "-   **Wasserstein Distance ($W_d$):** A measure of the geometric shift between the raw embedding clouds, capturing changes in meaning that might not be reflected in the topic distributions.\n",
        "-   **Kullback-Leibler (KL) Divergence ($D_{KL}$):** An asymmetric measure of \"surprise.\" The paper identifies $D_{KL}(A||P)$ as a powerful indicator of **Semantic Exploration**—the degree to which the LLM must introduce new concepts not present in the prompt.\n",
        "\n",
        "**4. Final Aggregated Scores:**\n",
        "These components are combined into the final, normalized scores:\n",
        "-   **Semantic Instability ($S_H$):** The primary hallucination score.\n",
        "    $$ S_H = \\frac{w_{jsd} \\cdot D_{JS}^{ens} + w_{wass} \\cdot W_d}{H(P)} $$\n",
        "-   **Semantic Exploration (KL Score):**\n",
        "    $$ KL(\\text{Answer| |Prompt}) = \\frac{D_{KL}^{ens}(A || P)}{H(P)} $$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`faithfulness_hallucination_misalignment_detection_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Configuration Pipeline:** A robust, Pydantic-based validation system for all experimental parameters.\n",
        "-   **High-Performance Data Generation:** Asynchronous API calls for efficient generation of the paraphrase and response corpora, with built-in fault tolerance and retry logic.\n",
        "-   **Rigorous Analytics:** Elite-grade, modular functions for each stage of the analysis, from embedding and clustering to the final metric calculations, leveraging optimized libraries like `scipy` and `scikit-learn`.\n",
        "-   **Automated Orchestration:** A master function that runs the entire end-to-end workflow with a single call.\n",
        "-   **Comprehensive Validation:** A full suite of robustness checks to analyze the framework's sensitivity to hyperparameters, model substitutions, and statistical noise.\n",
        "-   **Full Research Lifecycle:** The codebase covers the entire research process from configuration to final, validated scores, providing a complete and transparent replication package.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Configuration Validation (Task 1):** The pipeline ingests a configuration dictionary and rigorously validates its schema, constraints, and content.\n",
        "2.  **Environment Setup (Task 2):** It establishes a deterministic, reproducible computational environment and initializes all models and clients.\n",
        "3.  **Paraphrase Generation (Task 3):** It generates and validates `M` semantically equivalent paraphrases of the original prompt.\n",
        "4.  **Response Generation (Task 4):** It generates and validates an `M x N` matrix of responses.\n",
        "5.  **Sentence Segmentation (Task 5):** It deconstructs all texts into a cataloged, sentence-level corpus.\n",
        "6.  **Embedding Generation (Task 6):** It transforms the sentence corpus into a validated, high-dimensional vector space.\n",
        "7.  **Clustering (Task 7):** It determines the optimal number of topics (`k*`) and partitions the embedding space into `k*` clusters.\n",
        "8.  **Distribution Construction (Task 8):** It translates the discrete cluster labels into numerically stable probability distributions.\n",
        "9.  **Metric Computation (Tasks 9-10):** It calculates the full suite of information-theoretic and geometric metrics.\n",
        "10. **Score Aggregation (Task 11):** It synthesizes all intermediate metrics into the final, interpretable SDM scores and validates them against paper benchmarks.\n",
        "11. **Orchestration & Robustness (Tasks 12-13):** Master functions orchestrate the main pipeline and the optional, full suite of robustness checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `faithfulness_hallucination_misalignment_detection_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 13 tasks.\n",
        "\n",
        "## Key Callable: execute_sdm_analysis\n",
        "\n",
        "The central function in this project is `execute_sdm_analysis`. It orchestrates the entire analytical workflow, providing a single entry point for either a standard analysis or a full robustness study.\n",
        "\n",
        "```python\n",
        "def execute_sdm_analysis(\n",
        "    experiment_config: Dict[str, Any],\n",
        "    perform_robustness_checks: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the main SDM analysis pipeline and optionally a full suite of robustness checks.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.8+\n",
        "-   An OpenAI API key set as an environment variable (`OPENAI_API_KEY`).\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `scikit-learn`, `pydantic`, `openai`, `sentence-transformers`, `nltk`, `tenacity`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection.git\n",
        "    cd llm_faithfulness_hallucination_misalignment_detection\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy scikit-learn pydantic \"openai>=1.0.0\" sentence-transformers nltk tenacity tqdm\n",
        "    ```\n",
        "\n",
        "4.  **Set your OpenAI API Key:**\n",
        "    ```sh\n",
        "    export OPENAI_API_KEY='your_secret_api_key_here'\n",
        "    ```\n",
        "\n",
        "5.  **Download NLTK data:**\n",
        "    Run the following in a Python interpreter:\n",
        "    ```python\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline is controlled by a single, comprehensive Python dictionary, `experiment_config`. A fully specified example, `FusedExperimentInput`, is provided in the notebook. This dictionary defines everything from the prompt text and model choices to hyperparameters and validation thresholds.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `faithfulness_hallucination_misalignment_detection_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Define your `experiment_config` dictionary. A complete template is provided.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function.\n",
        "\n",
        "    **For a standard, single analysis:**\n",
        "    ```python\n",
        "    # Returns a dictionary with the results of the main run\n",
        "    standard_results = execute_sdm_analysis(\n",
        "        experiment_config=FusedExperimentInput,\n",
        "        perform_robustness_checks=False\n",
        "    )\n",
        "    ```\n",
        "\n",
        "    **For a full robustness study (computationally expensive):**\n",
        "    ```python\n",
        "    # Returns a dictionary with main run results and robustness reports\n",
        "    full_study_results = execute_sdm_analysis(\n",
        "        experiment_config=FusedExperimentInput,\n",
        "        perform_robustness_checks=True\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the primary scores:\n",
        "    ```python\n",
        "    final_scores = full_study_results['main_run']['final_scores']\n",
        "    print(final_scores)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_sdm_analysis` function returns a single, comprehensive dictionary:\n",
        "-   `main_run`: A dictionary containing the `SDMFullResult` object from the primary analysis. This includes the final scores, all intermediate diagnostic metrics, and the validation report against paper benchmarks.\n",
        "-   `robustness_analysis` (optional): If `perform_robustness_checks=True`, this key will contain a dictionary of `pandas.DataFrame`s, with each DataFrame summarizing the results of a specific robustness test.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "llm_faithfulness_hallucination_misalignment_detection/\n",
        "│\n",
        "├── faithfulness_hallucination_misalignment_detection_draft.ipynb  # Main implementation notebook   \n",
        "├── requirements.txt                                                 # Python package dependencies\n",
        "├── LICENSE                                                          # MIT license file\n",
        "└── README.md                                                        # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `experiment_config` dictionary. Users can easily modify:\n",
        "-   The `original_prompt_text` to analyze any prompt.\n",
        "-   The `system_components` to target different LLMs or embedding models.\n",
        "-   All `hyperparameters`, including `M`, `N`, `temperature`, clustering settings, and final score weights.\n",
        "-   All `validation_protocols` to tighten or loosen quality control thresholds.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{halperin2025prompt,\n",
        "  title={Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models},\n",
        "  author={Halperin, Igor},\n",
        "  journal={arXiv preprint arXiv:2508.10192},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models\".\n",
        "GitHub repository: https://github.com/chirindaopensource/llm_faithfulness_hallucination_misalignment_detection\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Igor Halperin for the insightful and clearly articulated research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`, `scikit-learn`, `pydantic`) that makes this work possible.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `faithfulness_hallucination_misalignment_detection_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "b45kxFo33ZYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models*\"\n",
        "\n",
        "Authors: Igor Halperin\n",
        "\n",
        "E-Journal Submission Date: 13 August 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.10192\n",
        "\n",
        "Re-Formulated Abstract:\n",
        "\n",
        "The proliferation of Large Language Models (LLMs) is challenged by hallucinations—critical failure modes where models generate non-factual, nonsensical, or unfaithful text. This paper introduces **Semantic Divergence Metrics (SDM)**, a novel, lightweight framework for detecting a specific class of these errors: *faithfulness hallucinations*.\n",
        "\n",
        "**Core Problem:**\n",
        "We focus on *confabulations*, defined as responses that are both arbitrary and semantically misaligned with the user's query. Existing methods, such as Semantic Entropy, test for arbitrariness by measuring the diversity of answers to a single, fixed prompt, making them insufficiently prompt-aware.\n",
        "\n",
        "**Methodological Framework:**\n",
        "Our SDM framework provides a more rigorous, prompt-aware methodology with two key innovations:\n",
        "*   **Ensemble-Based Testing:** We test for a deeper form of arbitrariness by measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt.\n",
        "*   **Joint Semantic Clustering:** We use joint clustering on sentence embeddings to create a shared topic space for both prompts and answers, enabling a direct, quantitative comparison of their semantic content.\n",
        "\n",
        "**Key Metrics and Findings:**\n",
        "From this shared topic space, we compute a suite of information-theoretic metrics to measure the semantic divergence between prompts and responses.\n",
        "*   Our primary practical score, **$S_H$**, combines the Jensen-Shannon divergence and the Wasserstein distance to quantify this divergence, with a high score indicating a likely faithfulness hallucination.\n",
        "*   Furthermore, we identify the KL divergence, $KL(\\text{Answer} || \\text{Prompt})$, as a powerful indicator of **Semantic Exploration**—a key signal for distinguishing between different generative behaviors like factual recall versus creative elaboration.\n",
        "\n",
        "**Final Contribution:**\n",
        "These metrics are combined into the **Semantic Box**, a diagnostic framework for classifying distinct LLM response types, including the dangerous, yet common, \"confident confabulation.\" Our work provides a principled, prompt-aware methodology for the real-time detection of faithfulness hallucinations and semantic misalignment in black-box LLMs."
      ],
      "metadata": {
        "id": "dYYA3JirTqoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary of \"Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models\" by Igor Halperin**\n",
        "\n",
        "#### **1. The Core Problem: Redefining and Detecting \"Faithfulness Hallucinations\"**\n",
        "\n",
        "The paper begins by focusing on a specific and critical type of LLM failure: **intrinsic faithfulness hallucinations**. These are not errors of fact-checking against the real world, but rather instances where the model's response deviates semantically from the provided prompt and context.\n",
        "\n",
        "The author makes a key terminological choice, preferring the term **\"confabulation\"** from psychiatry. This term aptly describes a response that is arbitrary and semantically misaligned with the user's query, generated to \"fill in the gaps\" without deceptive intent. The central thesis is that a faithful response must remain semantically aligned with the \"world\" defined by the prompt. A confabulation is, therefore, a severe form of **semantic misalignment**.\n",
        "\n",
        "#### **2. Critique of Existing Methods (e.g., Semantic Entropy)**\n",
        "\n",
        "The paper identifies a critical limitation in existing black-box detection methods like Semantic Entropy (SE). SE works by generating multiple answers from a *single, fixed prompt* and measuring their semantic diversity (entropy).\n",
        "\n",
        "*   **The Flaw:** This approach is fundamentally **\"prompt-agnostic.\"** It cannot distinguish between:\n",
        "    *   **a)** A high-entropy, unstable, and likely confabulated set of responses.\n",
        "    *   **b)** A high-entropy, rich, and perfectly valid set of responses to a complex, multifaceted prompt.\n",
        "\n",
        "By failing to account for the prompt's own complexity, SE can mistakenly flag a good, detailed answer as a hallucination.\n",
        "\n",
        "#### **3. The Proposed Methodology: Semantic Divergence Metrics (SDM)**\n",
        "\n",
        "The SDM framework is designed to be explicitly **\"prompt-aware.\"** It operates in a series of well-defined steps:\n",
        "\n",
        "*   **Step 1: Data Generation & Prompt Augmentation:** To robustly test for arbitrariness, the method doesn't rely on a single prompt. Instead, it generates `M` semantically equivalent **paraphrases** of the original prompt. For each paraphrase, it then generates `N` answers from the target LLM (using temperature sampling to induce diversity). This creates a rich ensemble of prompt-answer pairs.\n",
        "\n",
        "*   **Step 2: Creating a Shared Semantic Space:** This is a crucial innovation. All sentences from *both* the paraphrased prompts and the generated answers are segmented and embedded into a high-dimensional vector space using a sentence-transformer model. Then, **joint clustering** (hierarchical agglomerative clustering) is performed on this combined set of embeddings.\n",
        "    *   **The Benefit:** This creates a single, shared \"topic space\" where a sentence from a prompt can be assigned to the same topic cluster as a semantically similar sentence from an answer. This provides a direct, quantitative basis for comparing the thematic content of the prompts and the responses.\n",
        "\n",
        "*   **Step 3: The Information-Theoretic Toolkit:** With prompts and answers represented as probability distributions over the shared topic clusters, the framework computes a suite of metrics:\n",
        "    *   **Jensen-Shannon Divergence (JSD):** A symmetric metric that measures the dissimilarity between the overall prompt topic distribution and the answer topic distribution.\n",
        "    *   **Wasserstein Distance (Earth Mover's Distance):** This metric operates directly on the raw embedding clouds (before clustering). It measures the geometric \"cost\" of transforming the prompt embeddings into the answer embeddings, capturing shifts in meaning even if the high-level topics remain similar.\n",
        "    *   **Kullback-Leibler (KL) Divergence:** The paper identifies the asymmetric `KL(Answer || Prompt)` as a particularly powerful diagnostic.\n",
        "\n",
        "#### **4. The Key Output Metrics: Instability and Exploration**\n",
        "\n",
        "The framework distills the analysis into two primary, interpretable scores:\n",
        "\n",
        "1.  **The Hallucination Score (`SH`):** This is the primary score for **Semantic Instability**. It is a weighted average of the Ensemble JSD and the Wasserstein Distance, normalized by the prompt's entropy (to control for its intrinsic complexity). A high `SH` score indicates a significant, unstable semantic drift between the prompt and the response, signaling a high risk of confabulation.\n",
        "\n",
        "2.  **The Semantic Exploration Score (`KL(Answer || Prompt)`):** This score quantifies the degree to which the LLM had to \"invent\" or \"explore\" new semantic territory not explicitly present in the prompt.\n",
        "    *   **Low KL:** Suggests a task of recall or synthesis within a closed conceptual space (e.g., summarizing a provided text).\n",
        "    *   **High KL:** Signals a task requiring creativity, interpretation, or invention, where the prompt acts as a \"generative scaffold\" (e.g., \"Write a story about...\").\n",
        "\n",
        "#### **5. The Diagnostic Framework: The \"Semantic Box\"**\n",
        "\n",
        "The paper's most significant practical contribution is combining these two scores into a 2x2 diagnostic grid called the **Semantic Box**. This allows for a nuanced classification of LLM behavior beyond a simple \"hallucination/not hallucination\" binary.\n",
        "\n",
        "*   **X-Axis:** Semantic Instability (`SH`)\n",
        "*   **Y-Axis:** Semantic Exploration (`KL`)\n",
        "\n",
        "The four quadrants represent distinct response modes:\n",
        "*   **Red Box (Low Instability, Low Exploration):** The most dangerous state. This is the signature of a **Confident Hallucination**, where the model converges on a stable, consistent, but entirely fabricated answer to a nonsensical or difficult prompt. It can also be a benign, simple echoic response.\n",
        "*   **Green Box (High Instability, Low Exploration):** **Faithful Factual Recall**. A good factual answer requires synthesizing diverse facts, leading to some \"healthy\" instability, but operates within the prompt's conceptual space (low exploration).\n",
        "*   **Yellow Box (Low Instability, High Exploration):** **Faithful Interpretation**. The model consistently generates new, relevant details for an interpretive task (e.g., analyzing a theme in *Hamlet*).\n",
        "*   **Orange Box (High Instability, High Exploration):** **Creative Generation**. The expected behavior for a purely creative task, characterized by high diversity and the introduction of many new concepts.\n",
        "\n",
        "#### **6. Experimental Validation and Key Findings**\n",
        "\n",
        "The authors conducted two sets of experiments that validated the framework:\n",
        "\n",
        "*   **Finding 1:** The SDM scores (`SH` and `KL`) correctly tracked a \"stability gradient\" from factual (Hubble) to interpretive (Hamlet) to creative (AGI Dilemma) prompts, while the baseline Semantic Entropy failed, producing counter-intuitive results.\n",
        "*   **Finding 2 (Most Critical):** In a second experiment, a deliberately nonsensical prompt (\"Forced Hallucination\" about QCD and Baroque music) produced the **lowest `SH` (instability) score** of all prompts. This empirically confirmed the signature of the \"Confident Hallucination\" in the Red Box: the model defaulted to a highly stable, consistent, but completely fabricated \"evasion strategy.\"\n",
        "\n",
        "#### **7. Conclusion and Future Directions**\n",
        "\n",
        "The paper concludes that the SDM framework provides a principled, prompt-aware, and multi-faceted method for detecting confabulations and diagnosing LLM behavior. It moves beyond a single, universal score, arguing that context (i.e., the nature of the prompt) is essential. Future work includes large-scale validation, developing a self-calibrating version of the framework, and adapting it to measure \"groundedness\" in Retrieval-Augmented Generation (RAG) systems."
      ],
      "metadata": {
        "id": "__cy8UJhlaMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules\n",
        "\n"
      ],
      "metadata": {
        "id": "5ktZV8NaKh6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Semantic Divergence Metrics (SDM) for Hallucination and Misalignment Detection\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Prompt-Response Semantic Divergence\n",
        "#  Metrics for Faithfulness Hallucination and Misalignment Detection in Large\n",
        "#  Language Models\" by Igor Halperin (2025). It delivers a lightweight,\n",
        "#  prompt-aware system for quantitatively auditing Large Language Models (LLMs)\n",
        "#  by measuring the semantic divergence between a user's input and the model's\n",
        "#  response, enabling the detection of confabulations and the classification of\n",
        "#  generative behaviors.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Ensemble-based data generation via prompt paraphrasing\n",
        "#  • Joint sentence embedding and hierarchical clustering to create a shared topic space\n",
        "#  • Computation of information-theoretic metrics (JSD, KL Divergence) on topic distributions\n",
        "#  • Computation of geometric metrics (Wasserstein Distance) on embedding clouds\n",
        "#  • Aggregation into final scores for Semantic Instability (S_H) and Semantic Exploration (KL)\n",
        "#  • The \"Semantic Box\" diagnostic framework for classifying response types\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Asynchronous, fault-tolerant API interaction for efficient data generation\n",
        "#  • Robust Pydantic-based configuration validation\n",
        "#  • Programmatic determination of optimal cluster count via the K-Means elbow method\n",
        "#  • Numerically stable probability and metric calculations via SciPy and epsilon smoothing\n",
        "#  • Sliced-Wasserstein distance for tractable geometric analysis in high dimensions\n",
        "#  • A modular, fully orchestrated pipeline from raw prompt to final scores\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Halperin, I. (2025). Prompt-Response Semantic Divergence Metrics for\n",
        "#  Faithfulness Hallucination and Misalignment Detection in Large Language Models.\n",
        "#  arXiv preprint arXiv:2508.10192.\n",
        "#  https://arxiv.org/abs/2508.10192\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Consolidated Imports for the SDM Framework\n",
        "#\n",
        "# This block contains all necessary imports for the entire SDM pipeline,\n",
        "# from configuration and validation to final robustness analysis.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import asyncio\n",
        "import copy\n",
        "import importlib.metadata\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "from dataclasses import asdict, dataclass\n",
        "from typing import Any, ClassVar, Dict, List, Optional, Tuple\n",
        "\n",
        "# --- Core Scientific Computing and Data Manipulation ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.linalg import norm\n",
        "from packaging.version import parse as parse_version\n",
        "from scipy import stats as sp_stats\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "from scipy.stats import wasserstein_distance as scipy_wasserstein_1d\n",
        "\n",
        "# --- Machine Learning and NLP Libraries ---\n",
        "import nltk\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# --- API Clients and Utilities ---\n",
        "import openai\n",
        "from openai import APIError, AsyncOpenAI, OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "from pydantic import (BaseModel, Field, ValidationError, root_validator,\n",
        "                      validator)\n",
        "from tenacity import (retry, retry_if_exception_type, stop_after_attempt,\n",
        "                      wait_exponential)\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "\n",
        "# Configure a professional logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n"
      ],
      "metadata": {
        "id": "RvSO2LxfKm3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "pHH_NWxKKn_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Discussion of the Inputs, Processes and Outputs (IPO) of Key callables\n",
        "\n",
        "This analysis serves as a technical specification, mapping each implemented function directly to its methodological role within the source paper. It is the definitive documentation that connects our code to its academic and theoretical foundations.\n",
        "\n",
        "### Final Callable Specification and Methodological Mapping\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 1: `validate_and_clean_sdm_config`**\n",
        "\n",
        "*   **Inputs:** A raw Python dictionary, `config: Dict[str, Any]`, intended to match the `FusedExperimentInput` structure.\n",
        "*   **Process:**\n",
        "    1.  Instantiates a series of nested Pydantic models, recursively validating the input dictionary's schema, keys, and data types against a strict, predefined structure.\n",
        "    2.  Executes custom validators to enforce specific mathematical constraints (e.g., $w_{jsd} + w_{wass} = 1.0$).\n",
        "    3.  Executes custom validators to check the structural integrity of prompt templates (e.g., presence of required placeholders).\n",
        "    4.  Performs a cleansing pipeline on the `original_prompt_text` (Unicode normalization, whitespace stripping, control character removal).\n",
        "    5.  Validates the word and character counts of the cleansed prompt against the metadata provided in the configuration.\n",
        "*   **Outputs:** A tuple `(bool, FusedExperimentInputModel | None, str)` containing a success flag, a validated and type-safe Pydantic model instance, and a detailed status message.\n",
        "*   **Data Transformation:** This function transforms an untrusted, unstructured `dict` into a trusted, structured, and partially cleansed `FusedExperimentInputModel` object. It enriches the input by creating and populating the `cleaned_prompt_text` field.\n",
        "*   **Role in Research Pipeline:** This function serves as the rigorous **pre-flight check** for the entire experiment. It ensures that all parameters and inputs conform to the paper's specifications *before* any computation begins, preventing silent failures due to misconfiguration and guaranteeing the fidelity of the experimental setup. It is the implementation of the procedural integrity required by any serious quantitative study.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 2: `initialize_environment_and_models`**\n",
        "\n",
        "*   **Inputs:** A validated `config: FusedExperimentInputModel`.\n",
        "*   **Process:**\n",
        "    1.  Sets global random seeds for `random`, `numpy`, and `torch`, and configures the `torch.backends.cudnn` for deterministic behavior.\n",
        "    2.  Validates that all required Python libraries meet the minimum version specifications.\n",
        "    3.  Selects the optimal computational device (`torch.device`) by checking for GPU availability and sufficient memory, with a fallback to CPU.\n",
        "    4.  Initializes the `openai.OpenAI` client and the `sentence_transformers.SentenceTransformer` model, implementing the specified primary/fallback logic for the latter.\n",
        "    5.  Ensures necessary NLP data (e.g., `nltk`'s 'punkt' tokenizer) is available.\n",
        "*   **Outputs:** An `SDMRuntimeEnvironment` dataclass instance containing the initialized `device`, `openai_client`, and `embedding_model`.\n",
        "*   **Data Transformation:** This function transforms static configuration details from the `config` object into live, operational objects in memory (e.g., a model loaded onto a GPU, an authenticated API client).\n",
        "*   **Role in Research Pipeline:** This function establishes a **reproducible and operational computational environment**. It is the practical implementation of the paper's stated dependencies (Section 6: \"using the gpt-4o model\"; Section 4.4: \"utilize the Qwen3-Embedding-0.6B model\") and the unstated but essential requirement for deterministic execution.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 3: `generate_validated_paraphrases`**\n",
        "\n",
        "*   **Inputs:** The `config: FusedExperimentInputModel` and the `runtime_env: SDMRuntimeEnvironment`.\n",
        "*   **Process:**\n",
        "    1.  Constructs a detailed prompt for the paraphrasing LLM, specifying the task, constraints, and required JSON output format.\n",
        "    2.  Executes a fault-tolerant API call to generate `M` paraphrases.\n",
        "    3.  Parses the JSON response robustly.\n",
        "    4.  Performs a multi-faceted quality validation on the generated paraphrases, checking semantic similarity against the original prompt and adherence to length constraints.\n",
        "    5.  If validation fails, the entire process is retried up to a configured maximum number of attempts.\n",
        "*   **Outputs:** A `List[str]` containing exactly `M` validated, high-quality paraphrases.\n",
        "*   **Data Transformation:** This function transforms the single `original_prompt_text` into a corpus of `M` semantically equivalent but lexically diverse strings.\n",
        "*   **Role in Research Pipeline:** This function implements the core methodological innovation of the SDM framework over prior art: **ensemble-based prompt generation**. As the paper states, \"Our SDM framework improves upon this by being more prompt-aware: we test for a deeper form of arbitrariness by measuring response consistency... across multiple, semantically-equivalent paraphrases of the original prompt.\" This function creates that essential input.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 4: `generate_validated_responses`**\n",
        "\n",
        "*   **Inputs:** The `validated_paraphrases: List[str]`, the `config: FusedExperimentInputModel`, and the `runtime_env: SDMRuntimeEnvironment`.\n",
        "*   **Process:**\n",
        "    1.  For each of the `M` paraphrases, it prepares `N` distinct API requests.\n",
        "    2.  It executes all `M*N` API calls concurrently using `asyncio` for high efficiency.\n",
        "    3.  Each generated response string is individually validated for integrity (non-empty, within length bounds). Failed responses are replaced with empty strings.\n",
        "    4.  The final, validated responses are structured into a 2D NumPy array.\n",
        "*   **Outputs:** A `validated_responses: np.ndarray` of shape `(M, N)`.\n",
        "*   **Data Transformation:** This function transforms the `M` prompt strings into an `(M, N)` matrix of response strings, creating the full prompt-response corpus for the experiment.\n",
        "*   **Role in Research Pipeline:** This function implements the **response generation** phase (Section 6: \"we generated 10 paraphrases and 4 responses per paraphrase\"). The use of temperature-based sampling introduces the stochasticity that the SDM framework is designed to measure.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 5: `segment_and_validate_corpus`**\n",
        "\n",
        "*   **Inputs:** The `validated_paraphrases: List[str]` and the `validated_responses: np.ndarray`.\n",
        "*   **Process:**\n",
        "    1.  Deconstructs every prompt and response string into its constituent sentences using `nltk.sent_tokenize`.\n",
        "    2.  Catalogs every single sentence in a `pandas.DataFrame`, meticulously tracking its `source_type` ('prompt' or 'answer') and its original indices (`paraphrase_idx`, `response_idx`).\n",
        "    3.  Filters the cataloged sentences to remove likely segmentation artifacts based on word count.\n",
        "*   **Outputs:** A `SegmentedCorpus` dataclass containing lists of the final validated sentences and their corresponding metadata DataFrames.\n",
        "*   **Data Transformation:** This function transforms the document-level corpus into a sentence-level corpus, which is the fundamental unit of analysis for the rest of the pipeline. It transforms unstructured lists and arrays into a highly structured, indexed DataFrame.\n",
        "*   **Role in Research Pipeline:** This implements the **Sentence Segmentation** step described in the abstract and methodology. The paper's analysis is explicitly sentence-level, and this function prepares the data in that required format.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 6: `generate_and_validate_embeddings`**\n",
        "\n",
        "*   **Inputs:** The `corpus: SegmentedCorpus` and the `runtime_env: SDMRuntimeEnvironment`.\n",
        "*   **Process:**\n",
        "    1.  Takes the lists of prompt and answer sentences and feeds them into the `SentenceTransformer` model in efficient batches.\n",
        "    2.  Performs rigorous validation on the output vectors, checking for correct dimensionality, `NaN`/`inf` values, and reasonable vector magnitudes.\n",
        "    3.  Concatenates the prompt and answer embedding matrices into a single `joint_embeddings` matrix.\n",
        "*   **Outputs:** An `EmbeddedCorpus` dataclass containing the separate and joint embedding matrices.\n",
        "*   **Data Transformation:** This function transforms the corpus from the linguistic domain (lists of strings) into a geometric domain (NumPy arrays of high-dimensional vectors).\n",
        "*   **Role in Research Pipeline:** This implements the **Joint Embedding** step (Section 4.4). The paper states, \"All sentences from both prompts and answers are segmented and then individually embedded into a common high-dimensional vector space... which are then combined for the joint clustering analysis.\" This function executes that process precisely.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 7: `perform_joint_clustering`**\n",
        "\n",
        "*   **Inputs:** The `embedded_corpus: EmbeddedCorpus` and the `config: FusedExperimentInputModel`.\n",
        "*   **Process:**\n",
        "    1.  Determines the optimal number of clusters, `k*`, by applying the K-Means elbow method to the `joint_embeddings`.\n",
        "    2.  Performs Hierarchical Agglomerative Clustering on the `joint_embeddings` using the determined `k*` and the `ward` linkage method, as specified.\n",
        "    3.  Validates the quality of the resulting clusters using the Silhouette Score and balance checks.\n",
        "    4.  Separates the resulting `joint_labels` array into `prompt_labels` and `answer_labels`.\n",
        "*   **Outputs:** A `ClusteringResult` dataclass containing the optimal `k`, the separated label arrays, and validation metrics.\n",
        "*   **Data Transformation:** This function transforms the continuous geometric space of embeddings into a discrete set of topic assignments (integer labels).\n",
        "*   **Role in Research Pipeline:** This is the implementation of the **Joint Semantic Clustering and Topic Estimation** (Section 4.5). The paper's key methodological choice is to \"pool their sentence embeddings into a single dataset... to identify shared semantic topics.\" This function's use of the `joint_embeddings` matrix is the direct implementation of that principle. The use of Ward's linkage is also a direct implementation of the paper's specification.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 8: `construct_topic_distributions`**\n",
        "\n",
        "*   **Inputs:** The `corpus: SegmentedCorpus` and the `clustering_result: ClusteringResult`.\n",
        "*   **Process:**\n",
        "    1.  Calculates the **Global** topic probability distributions (`P_global`, `A_global`) from the full sets of prompt and answer labels.\n",
        "    2.  Calculates the **Local (Ensemble)** topic distributions by iterating through each of the `M` paraphrase-answer pairs, filtering the labels for each, and computing their individual distributions (`P_m`, `A_m`).\n",
        "    3.  Calculates the **Averaged Joint** distribution by computing the outer product of each local pair and averaging the resulting `M` matrices.\n",
        "    4.  Applies epsilon smoothing throughout to ensure numerical stability.\n",
        "*   **Outputs:** A `TopicDistributions` dataclass containing all three types of distributions.\n",
        "*   **Data Transformation:** This function transforms the discrete integer cluster labels into continuous probability distributions (NumPy arrays that sum to 1.0).\n",
        "*   **Role in Research Pipeline:** This function prepares the direct inputs for the information-theoretic analysis. It is the implementation of the process described in Section 4.6, where the cluster assignments are used to \"quantify the semantic relationship between the prompt and response distributions.\" It computes the distributions $P$ and $A$ that are the arguments for the divergence formulas.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 9: `compute_information_theoretic_metrics`**\n",
        "\n",
        "*   **Inputs:** The `distributions: TopicDistributions` object.\n",
        "*   **Process:**\n",
        "    1.  Uses robust `scipy` implementations to calculate the core metrics of Entropy, KL Divergence, and JSD.\n",
        "    2.  Applies these functions to the global and local distributions to compute all specified divergence metrics (Global JSD, Ensemble JSD, Global KLs, Ensemble KL).\n",
        "    3.  Calculates the Ensemble Mutual Information using the formula $I^{ens}(X;Y) = H(Y) - H(Y|X)$.\n",
        "*   **Outputs:** An `InformationTheoreticMetrics` dataclass containing all computed scalar metrics.\n",
        "*   **Data Transformation:** This function transforms the probability distribution objects into a set of scalar metrics that quantify their relationships.\n",
        "*   **Role in Research Pipeline:** This is the direct implementation of the **Computing Topic-Based Alignment and Divergence Metrics** section (4.6) and the core formulas for JSD and KL divergence. It computes the components of the final scores, such as $D_{JS}^{ens}$ and $D_{KL}^{ens}(\\mathbf{A} \\| \\mathbf{P})$.\n",
        "    *   **JSD Equation:** $D_{JS}(P||A) = \\frac{1}{2}(D_{KL}(P||M) + D_{KL}(A||M))$, where $M = \\frac{1}{2}(P+A)$.\n",
        "    *   **KL Equation:** $D_{KL}(P||Q) = \\sum_{i} P(i) \\log_2(\\frac{P(i)}{Q(i)})$.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 10: `compute_geometric_distance`**\n",
        "\n",
        "*   **Inputs:** The `embedded_corpus: EmbeddedCorpus`.\n",
        "*   **Process:**\n",
        "    1.  Implements the Sliced-Wasserstein distance as a computationally tractable approximation of the 1-Wasserstein distance.\n",
        "    2.  Generates a set of reproducible random 1D projection vectors.\n",
        "    3.  Projects the high-dimensional prompt and answer embeddings onto these vectors.\n",
        "    4.  Calculates the 1D Wasserstein distance for each projection and averages the results.\n",
        "*   **Outputs:** A single scalar float, `wasserstein_distance`.\n",
        "*   **Data Transformation:** This function transforms the two high-dimensional point clouds (embedding matrices) into a single scalar value representing the geometric \"work\" needed to transform one into the other.\n",
        "*   **Role in Research Pipeline:** This function computes the **Global Distributional Shift ($W_d$)**, the second key component of the final $S_H$ score. As the paper notes, \"The Wasserstein distance... operates directly on the high-dimensional, continuous embedding space... It tells us the overall shift in the semantic content and meaning\" (Section 4.10).\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 11: `aggregate_and_validate_scores`**\n",
        "\n",
        "*   **Inputs:** The `it_metrics: InformationTheoreticMetrics` object, the `wasserstein_distance: float`, and the `config: FusedExperimentInputModel`.\n",
        "*   **Process:**\n",
        "    1.  Implements the final aggregation formulas to compute the three primary scores.\n",
        "    2.  Validates the computed scores against the benchmark ranges specified in the configuration for the given experiment.\n",
        "    3.  Compiles a comprehensive result object.\n",
        "*   **Outputs:** An `SDMFullResult` dataclass containing all final scores and diagnostics.\n",
        "*   **Data Transformation:** This function synthesizes all previously computed intermediate metrics into the final, interpretable, normalized scores.\n",
        "*   **Role in Research Pipeline:** This is the implementation of the final score definitions from Sections 4.9 and 4.10.\n",
        "    *   **$S_H$ Score Equation (7):** $S_H = \\frac{w_{jsd} \\cdot D_{JS}^{ens} + w_{wass} \\cdot W_d}{H(P)}$\n",
        "    *   **$\\Phi$ Score Equation (6):** $\\Phi = \\frac{H(Y|X)}{H(X)}$\n",
        "    *   **KL Score Equation (8):** $KL(\\text{Answer| |Prompt}) = \\frac{D_{KL}^{ens}(\\mathbf{A} \\| \\mathbf{P})}{H(P)}$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 12 & 13: Orchestrators**\n",
        "\n",
        "*   **`run_sdm_pipeline`:** This is the master process controller. Its role is purely procedural: to execute Tasks 1-11 in the correct sequence and manage the flow of data between them. It implements the overall experimental algorithm described in the paper.\n",
        "*   **`run_full_robustness_analysis`:** This is a meta-experimental function. Its role is to repeatedly execute the entire `run_sdm_pipeline` under perturbed conditions to validate the stability and robustness of the framework itself. This is a crucial step in any serious quantitative research for establishing the credibility of the results.\n",
        "*   **`execute_sdm_analysis`:** This is the final user-facing API. It provides a clean entry point to the entire framework, encapsulating the choice between a single analytical run and the full robustness suite.\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "We will now walk through a practical example of how to deploy the entire SDM pipeline using the top-level `execute_sdm_analysis` function. This will serve as the definitive user guide for the system we have constructed.\n",
        "\n",
        "### Example: Executing the End-to-End SDM Pipeline\n",
        "\n",
        "This example demonstrates how a user would interact with the complete framework to analyze a specific prompt. We will use the \"Complex Comparison (Keynes vs. Hayek)\" case study that has informed our development process.\n",
        "\n",
        "#### Step 1: Prerequisites\n",
        "\n",
        "Before executing the pipeline, two conditions must be met:\n",
        "\n",
        "1.  **Code Availability:** All the Python functions and dataclasses we have defined and audited (from `validate_and_clean_sdm_config` to `run_full_robustness_analysis` and all their helpers) must be imported or otherwise available in the Python execution scope.\n",
        "2.  **Environment Configuration:** The Python environment must have all the required libraries installed (as specified in the configuration). Crucially, for security and professional practice, the OpenAI API key must be set as an environment variable.\n",
        "\n",
        "```python\n",
        "# In your terminal, before running the Python script:\n",
        "# export OPENAI_API_KEY='your_secret_api_key_here'\n",
        "```\n",
        "\n",
        "#### Step 2: Defining the Input Parameters\n",
        "\n",
        "The `execute_sdm_analysis` function takes two parameters. We will define them now.\n",
        "\n",
        "**Parameter 1: `experiment_config: Dict[str, Any]`**\n",
        "\n",
        "This is the complete blueprint for the experiment. It is a comprehensive dictionary that controls every aspect of the pipeline, from the prompt text to the validation thresholds. For this example, we will use the exact `FusedExperimentInput` dictionary that has served as our specification throughout this process.\n",
        "\n",
        "```python\n",
        "# This is the primary input to our framework. It contains all settings and data.\n",
        "FusedExperimentInput = {\n",
        "    # I. Experiment Metadata and Computational Environment\n",
        "    \"experiment_metadata\": {\n",
        "        \"experiment_name\": \"Complex_Comparison_Keynes_vs_Hayek\",\n",
        "        \"experiment_set\": \"B\",\n",
        "        \"description\": \"SDM framework evaluation on interpretive comparison task\",\n",
        "        \"paper_reference\": \"Appendix B, Table 4\",\n",
        "        \"reproduction_fidelity\": \"HIGH_WITH_DOCUMENTED_ASSUMPTIONS\",\n",
        "        \"uuid\": \"f4a2b1e0-5d6c-4a8e-9b3f-2c1d7a6b0e9f\"\n",
        "    },\n",
        "    \"computational_environment\": {\n",
        "        \"python_version\": \">=3.8,<4.0\",\n",
        "        \"required_libraries\": {\n",
        "            \"sentence-transformers\": \">=2.2.0\", \"scikit-learn\": \">=1.2.0\",\n",
        "            \"numpy\": \">=1.21.0\", \"scipy\": \">=1.7.0\", \"pandas\": \">=1.3.0\", \"openai\": \">=1.0.0\"\n",
        "        },\n",
        "        \"device_requirements\": {\"primary\": \"cuda:0\", \"fallback\": \"cpu\", \"min_gpu_memory_gb\": 8},\n",
        "        \"api_dependencies\": {\"openai_api_version\": \"v1\", \"rate_limit_requests_per_minute\": 60, \"expected_api_cost_usd\": 15.0}\n",
        "    },\n",
        "    # II. Primary Input Data\n",
        "    \"primary_input_data\": {\n",
        "        \"original_prompt_text\": (\n",
        "            \"In about 100 words, compare and contrast the economic policies of John \"\n",
        "            \"Maynard Keynes and Friedrich Hayek. Discuss their core philosophies on \"\n",
        "            \"government intervention, free markets, and their proposed solutions to \"\n",
        "            \"economic downturns. Conclude with which theory is more influential in modern \"\n",
        "            \"Western economies.\"\n",
        "        ),\n",
        "        \"data_type\": \"str\", \"source_verification\": \"Paper Appendix B, Table 4 - Complex Comparison prompt\",\n",
        "        \"character_count\": 478, \"word_count\": 77 # Corrected counts for the actual prompt text\n",
        "    },\n",
        "    # III. System Components\n",
        "    \"system_components\": {\n",
        "        \"target_llm\": {\"model_identifier\": \"gpt-4o\", \"paper_specification\": \"EXACT_MATCH\"},\n",
        "        \"paraphrasing_llm\": {\"model_identifier\": \"gpt-4o\", \"paper_specification\": \"IMPLEMENTATION_ASSUMPTION\"},\n",
        "        \"sentence_embedding_model\": {\n",
        "            \"model_identifier\": \"sentence-transformers/all-mpnet-base-v2\", # Using a widely available model\n",
        "            \"paper_requirement\": \"Qwen3-Embedding-0.6B\", \"paper_specification\": \"REPRESENTATIVE_FALLBACK\",\n",
        "            \"fallback_model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"embedding_dimension\": 768\n",
        "        }\n",
        "    },\n",
        "    # IV. Hyperparameters\n",
        "    \"hyperparameters\": {\n",
        "        \"corpus_generation\": {\"num_paraphrases_M\": 10, \"num_answers_per_paraphrase_N\": 4},\n",
        "        \"llm_inference_params\": {\n",
        "            \"paraphrasing\": {\"temperature\": 0.8, \"top_p\": 1.0, \"max_tokens\": 2048},\n",
        "            \"response_generation\": {\"temperature\": 0.7, \"top_p\": 1.0, \"max_tokens\": 512}\n",
        "        },\n",
        "        \"text_processing\": {\"sentence_segmentation_method\": \"nltk.sent_tokenize\"},\n",
        "        \"clustering\": {\"k_determination_method\": \"elbow_method\", \"k_range_min\": 2, \"k_range_max\": 15, \"linkage_method\": \"ward\"},\n",
        "        \"final_score_weights\": {\"w_jsd\": 0.7, \"w_wass\": 0.3},\n",
        "        \"reproducibility\": {\"global_random_seed\": 42, \"numpy_seed\": 42, \"sklearn_random_state\": 42}\n",
        "    },\n",
        "    # V. Validation and Quality Control Protocols\n",
        "    \"validation_protocols\": {\n",
        "        \"paraphrase_validation\": {\"semantic_similarity_threshold\": 0.85},\n",
        "        \"response_validation\": {\"min_length_words\": 10, \"max_length_words\": 200},\n",
        "        \"embedding_validation\": {\"nan_check\": True, \"dimensionality_check\": True, \"magnitude_threshold\": [0.1, 10.0]},\n",
        "        \"clustering_validation\": {\"min_cluster_size\": 2, \"max_cluster_ratio\": 0.8, \"silhouette_score_threshold\": 0.1} # Adjusted for real-world text\n",
        "    },\n",
        "    # VI. Error Handling and Fallback Strategies\n",
        "    \"error_handling\": {\n",
        "        \"api_errors\": {\"max_retries\": 3},\n",
        "        \"clustering_failures\": {\"elbow_detection_failure\": \"use_k_5_default\"}\n",
        "    },\n",
        "    # VII. Expected Outputs and Validation Targets\n",
        "    \"expected_outputs\": {\n",
        "        \"paper_comparison_targets\": {\n",
        "            \"S_H_expected_range\": [0.10, 0.20], \"phi_expected_range\": [0.95, 1.10],\n",
        "            \"source\": \"Table 2, Complex Comparison (Keynes/Hayek) results\"\n",
        "        }\n",
        "    },\n",
        "    # VIII. Prompt Templates\n",
        "    \"prompt_templates\": {\n",
        "        \"paraphrase_generation\": {\n",
        "            \"system_prompt\": \"You are a linguistic expert. Your task is to generate multiple, high-quality paraphrases of a given text. Maintain absolute semantic equivalence while maximizing lexical and syntactic diversity. Adhere strictly to all constraints.\",\n",
        "            \"user_prompt_template\": (\n",
        "                \"Please generate exactly {num_paraphrases} paraphrases for the following text.\\n\\n\"\n",
        "                \"**Constraints:**\\n\"\n",
        "                \"1. **Semantic Equivalence:** Each paraphrase must mean exactly the same thing.\\n\"\n",
        "                \"2. **Lexical & Syntactic Diversity:** Use different vocabulary and sentence structures.\\n\"\n",
        "                \"3. **Format:** Output a single JSON object with one key, \\\"paraphrases\\\", containing a list of the string paraphrases.\\n\\n\"\n",
        "                \"**Original Text:**\\n\"\n",
        "                \"```\\n{original_prompt}\\n```\"\n",
        "            )\n",
        "        }\n",
        "    },\n",
        "    # IX. Execution Protocol (for documentation)\n",
        "    \"execution_protocol\": {\"main_pipeline_steps\": [\"validate\", \"initialize\", \"generate_paraphrases\", \"generate_responses\", \"segment\", \"embed\", \"cluster\", \"calculate_metrics\", \"aggregate_scores\"]}\n",
        "}\n",
        "```\n",
        "\n",
        "**Parameter 2: `perform_robustness_checks: bool`**\n",
        "\n",
        "This is a simple boolean flag that acts as a control switch for the scope of the analysis.\n",
        "\n",
        "*   `perform_robustness_checks=False` (Default): This will execute the `run_sdm_pipeline` function **once**. It provides a single, high-fidelity measurement of the SDM scores for the given configuration. This is the standard mode for analyzing a single prompt.\n",
        "*   `perform_robustness_checks=True`: This will execute the single run as above, and then proceed to execute the `run_full_robustness_analysis` function. This will trigger the entire suite of computationally expensive tests (parameter sensitivity, model substitution, statistical stability), involving many additional pipeline runs. This mode is used for deep validation of the framework itself.\n",
        "\n",
        "#### Step 3: Execution and Interpretation of Results\n",
        "\n",
        "We will now demonstrate how to call the function in both modes.\n",
        "\n",
        "**Scenario A: Standard Single Analysis**\n",
        "\n",
        "This is the most common use case: analyzing a single prompt to get its SDM scores.\n",
        "\n",
        "```python\n",
        "# Import the top-level orchestrator function\n",
        "# from sdm_framework.main import execute_sdm_analysis\n",
        "# import json # For pretty printing\n",
        "\n",
        "# --- Execute the Standard Analysis ---\n",
        "print(\"--- Starting Standard SDM Analysis ---\")\n",
        "# We set the flag to False to perform a single, efficient run.\n",
        "standard_results = execute_sdm_analysis(\n",
        "    experiment_config=FusedExperimentInput,\n",
        "    perform_robustness_checks=False\n",
        ")\n",
        "\n",
        "# --- Interpret the Output ---\n",
        "# The output is a dictionary containing the results of the main run.\n",
        "if 'error' not in standard_results:\n",
        "    print(\"\\n--- Standard Analysis Completed Successfully ---\")\n",
        "    \n",
        "    # Extract the main run results\n",
        "    main_run = standard_results.get('main_run', {})\n",
        "    \n",
        "    # Extract and print the final, most important scores\n",
        "    final_scores = main_run.get('final_scores', {})\n",
        "    print(\"\\n[Primary SDM Scores]\")\n",
        "    print(f\"  S_H (Semantic Instability) Score: {final_scores.get('s_h_score', 'N/A'):.4f}\")\n",
        "    print(f\"  Phi (Unexplained Complexity) Score: {final_scores.get('phi_score', 'N/A'):.4f}\")\n",
        "    print(f\"  KL (Semantic Exploration) Score: {final_scores.get('kl_exploration_score', 'N/A'):.4f}\")\n",
        "\n",
        "    # Extract and print the validation report against the paper's benchmarks\n",
        "    validation_report = main_run.get('validation_report', {})\n",
        "    print(\"\\n[Fidelity Check vs. Paper Benchmarks]\")\n",
        "    print(f\"  Source for Comparison: {validation_report.get('source', 'N/A')}\")\n",
        "    s_h_val = validation_report.get('s_h_score_validation', {})\n",
        "    print(f\"  S_H Score Validation: {'PASSED' if s_h_val.get('passed') else 'FAILED'}\")\n",
        "    print(f\"    - Computed: {s_h_val.get('computed_value', 'N/A')}, Expected Range: {s_h_val.get('expected_range', 'N/A')}\")\n",
        "    \n",
        "    # The full results object contains all intermediate metrics for deep analysis\n",
        "    # print(\"\\nFull Results Object:\")\n",
        "    # print(json.dumps(standard_results, indent=2))\n",
        "else:\n",
        "    print(f\"\\n--- Standard Analysis Failed ---\")\n",
        "    print(f\"Error: {standard_results['error']}\")\n",
        "\n",
        "```\n",
        "\n",
        "**Scenario B: Full Robustness Analysis**\n",
        "\n",
        "This is the use case for a deep validation study of the framework itself.\n",
        "\n",
        "```python\n",
        "# Import the top-level orchestrator function and pandas for displaying results\n",
        "# from sdm_framework.main import execute_sdm_analysis\n",
        "# import pandas as pd\n",
        "\n",
        "# --- Execute the Full Robustness Analysis ---\n",
        "print(\"\\n\\n--- Starting Full Robustness Analysis (This will take a significant amount of time) ---\")\n",
        "# We set the flag to True to trigger the entire suite of validation tests.\n",
        "full_results = execute_sdm_analysis(\n",
        "    experiment_config=FusedExperimentInput,\n",
        "    perform_robustness_checks=True\n",
        ")\n",
        "\n",
        "# --- Interpret the Output ---\n",
        "if 'error' not in full_results:\n",
        "    print(\"\\n--- Full Robustness Analysis Completed Successfully ---\")\n",
        "    \n",
        "    # The output dictionary now contains an additional key for the robustness checks.\n",
        "    robustness_report = full_results.get('robustness_analysis', {})\n",
        "    \n",
        "    # Display the results for each analysis\n",
        "    if 'parameter_sensitivity' in robustness_report:\n",
        "        print(\"\\n[Parameter Sensitivity Analysis Results (Temperature)]\")\n",
        "        print(robustness_report['parameter_sensitivity'].to_string())\n",
        "        \n",
        "    if 'model_substitution' in robustness_report:\n",
        "        print(\"\\n[Model Substitution Analysis Results (Embedding Model)]\")\n",
        "        print(robustness_report['model_substitution'].to_string())\n",
        "        \n",
        "    if 'statistical_robustness' in robustness_report:\n",
        "        print(\"\\n[Statistical Robustness Analysis Results (Multiple Seeds)]\")\n",
        "        print(robustness_report['statistical_robustness'].to_string())\n",
        "else:\n",
        "    print(f\"\\n--- Full Robustness Analysis Failed ---\")\n",
        "    print(f\"Error: {full_results['error']}\")\n",
        "```\n",
        "\n",
        "This concludes the practical demonstration. We have shown how to configure and execute the entire SDM framework through its single, entry point, and how to interpret the resulting outputs for both standard analysis and a full-scale robustness validation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "intTzWOrKqrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter Validation and Data Cleansing\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Parameter Validation and Data Cleansing\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module provides a rigorous, Pydantic-based validation and cleansing\n",
        "# system for the SDM framework's input configuration. It ensures that all\n",
        "# subsequent computations are based on a verified and sanitized input state,\n",
        "# preventing silent failures and ensuring methodological fidelity.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Nested Pydantic Models for Configuration Schema\n",
        "# ==============================================================================\n",
        "\n",
        "class ExperimentMetadata(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for experiment metadata, ensuring all identifying\n",
        "    information for the experimental run is present and correctly typed.\n",
        "\n",
        "    Attributes:\n",
        "        experiment_name (str): A unique name for the experiment.\n",
        "        experiment_set (str): The experiment set identifier (e.g., 'A', 'B').\n",
        "        description (str): A brief description of the experiment's purpose.\n",
        "        paper_reference (str): A citation or reference to the source paper section.\n",
        "        reproduction_fidelity (str): A tag indicating the fidelity of the reproduction.\n",
        "        uuid (str): A universally unique identifier for the specific run.\n",
        "    \"\"\"\n",
        "    # A unique, human-readable name for the experiment.\n",
        "    experiment_name: str\n",
        "\n",
        "    # The identifier for the set of experiments this run belongs to (e.g., 'A' or 'B').\n",
        "    experiment_set: str\n",
        "\n",
        "    # A short text description of the experiment's objective.\n",
        "    description: str\n",
        "\n",
        "    # A reference to the specific section or table in the source paper.\n",
        "    paper_reference: str\n",
        "\n",
        "    # A string indicating the level of fidelity to the original paper's methodology.\n",
        "    reproduction_fidelity: str\n",
        "\n",
        "    # A UUID to uniquely identify this specific execution instance.\n",
        "    uuid: str\n",
        "\n",
        "class ComputationalEnvironment(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for the computational environment, specifying software\n",
        "    and hardware requirements for reproducibility.\n",
        "\n",
        "    Attributes:\n",
        "        python_version (str): The required Python version range (e.g., '>=3.8,<4.0').\n",
        "        required_libraries (Dict[str, str]): A dictionary of required libraries and their version constraints.\n",
        "        device_requirements (Dict[str, Any]): Specifications for hardware, such as primary device and memory.\n",
        "        api_dependencies (Dict[str, Any]): Details about required external APIs, including versions and rate limits.\n",
        "    \"\"\"\n",
        "    # The compatible Python version range for this experiment.\n",
        "    python_version: str\n",
        "\n",
        "    # A dictionary mapping required library names to their version specifiers.\n",
        "    required_libraries: Dict[str, str]\n",
        "\n",
        "    # A dictionary specifying hardware needs, like preferred device ('cuda:0') and minimum GPU memory.\n",
        "    device_requirements: Dict[str, Any]\n",
        "\n",
        "    # A dictionary detailing external API dependencies, including rate limits and cost estimates.\n",
        "    api_dependencies: Dict[str, Any]\n",
        "\n",
        "class PrimaryInputData(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for the primary input prompt, including its metadata,\n",
        "    and integrates a validation pipeline for cleansing and verification.\n",
        "\n",
        "    Attributes:\n",
        "        original_prompt_text (str): The raw, unmodified prompt text from the source.\n",
        "        cleaned_prompt_text (str): The prompt text after normalization and cleaning. Populated by a validator.\n",
        "        data_type (str): The expected data type of the prompt ('str').\n",
        "        source_verification (str): A string verifying the source of the prompt.\n",
        "        character_count (int): The expected character count of the prompt.\n",
        "        word_count (int): The expected word count of the prompt.\n",
        "    \"\"\"\n",
        "    # The original, verbatim prompt text. Must be a non-empty string.\n",
        "    original_prompt_text: str = Field(..., min_length=1)\n",
        "\n",
        "    # A field to store the prompt after it has been cleaned. This is populated by the validator.\n",
        "    cleaned_prompt_text: str = \"\"\n",
        "\n",
        "    # The expected Python data type of the prompt text.\n",
        "    data_type: str\n",
        "\n",
        "    # A string that documents the source of the prompt for verification.\n",
        "    source_verification: str\n",
        "\n",
        "    # The expected character count of the prompt, used for validation.\n",
        "    character_count: int\n",
        "\n",
        "    # The expected word count of the prompt, used for validation.\n",
        "    word_count: int\n",
        "\n",
        "    @validator('original_prompt_text', pre=True, always=True)\n",
        "    def validate_and_clean_prompt(cls: ClassVar, v: str, values: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Performs a series of cleansing and validation steps on the input prompt text.\n",
        "\n",
        "        This Pydantic validator is triggered when the `PrimaryInputData` model is\n",
        "        instantiated. It first cleans the raw prompt text by normalizing Unicode,\n",
        "        stripping whitespace, and removing control characters. It then validates\n",
        "        the character and word counts of the cleaned text against the expected\n",
        "        values provided in the configuration, ensuring the input is consistent.\n",
        "\n",
        "        Args:\n",
        "            cls (ClassVar): The Pydantic model class.\n",
        "            v (str): The raw value of the `original_prompt_text` field.\n",
        "            values (Dict[str, Any]): A dictionary of other fields in the model.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the character or word count of the cleaned text\n",
        "                        is outside the specified tolerance of the expected values.\n",
        "\n",
        "        Returns:\n",
        "            str: The original, unmodified prompt text, preserving the raw input.\n",
        "                 The cleansed text is stored in the `cleaned_prompt_text` field.\n",
        "        \"\"\"\n",
        "        # Ensure the input value is a string before proceeding.\n",
        "        if not isinstance(v, str):\n",
        "            raise TypeError(\"original_prompt_text must be a string.\")\n",
        "\n",
        "        # --- Step 1.4.1: Clean the original prompt text ---\n",
        "        # Normalize Unicode characters to their canonical composite form (NFKC).\n",
        "        # This standardizes characters like ligatures for consistent processing.\n",
        "        cleaned_v = unicodedata.normalize('NFKC', v)\n",
        "\n",
        "        # Strip any leading or trailing whitespace from the text.\n",
        "        cleaned_v = cleaned_v.strip()\n",
        "\n",
        "        # Remove non-printable ASCII control characters (e.g., null, backspace) using a regex.\n",
        "        cleaned_v = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', cleaned_v)\n",
        "\n",
        "        # Store the result of the cleansing process in the `cleaned_prompt_text` field for later use.\n",
        "        values['cleaned_prompt_text'] = cleaned_v\n",
        "\n",
        "        # --- Step 1.4.1: Validate character and word counts against the cleansed text ---\n",
        "        # Define the allowed tolerance for character and word count validation.\n",
        "        char_tolerance = 5\n",
        "        word_tolerance = 3\n",
        "\n",
        "        # Calculate the character count of the fully cleansed text.\n",
        "        actual_char_count = len(cleaned_v)\n",
        "        # Retrieve the expected character count from the input configuration.\n",
        "        expected_char_count = values.get('character_count', 0)\n",
        "\n",
        "        # Check if the actual character count is within the allowed tolerance range.\n",
        "        if not (expected_char_count - char_tolerance <= actual_char_count <= expected_char_count + char_tolerance):\n",
        "            # If validation fails, raise a descriptive ValueError.\n",
        "            raise ValueError(\n",
        "                f\"Character count validation failed. \"\n",
        "                f\"Expected: {expected_char_count} (±{char_tolerance}), \"\n",
        "                f\"Actual (after cleaning): {actual_char_count}.\"\n",
        "            )\n",
        "\n",
        "        # Calculate the word count of the cleansed text by splitting on whitespace.\n",
        "        actual_word_count = len(cleaned_v.split())\n",
        "        # Retrieve the expected word count from the input configuration.\n",
        "        expected_word_count = values.get('word_count', 0)\n",
        "\n",
        "        # Check if the actual word count is within the allowed tolerance range.\n",
        "        if not (expected_word_count - word_tolerance <= actual_word_count <= expected_word_count + word_tolerance):\n",
        "            # If validation fails, raise a descriptive ValueError.\n",
        "            raise ValueError(\n",
        "                f\"Word count validation failed. \"\n",
        "                f\"Expected: {expected_word_count} (±{word_tolerance}), \"\n",
        "                f\"Actual (after cleaning): {actual_word_count}.\"\n",
        "            )\n",
        "\n",
        "        # Return the original, un-cleaned value to ensure the raw input is preserved in the model.\n",
        "        return v\n",
        "\n",
        "class SystemComponents(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for the core system components, including the language\n",
        "    models and the sentence embedding model.\n",
        "\n",
        "    Attributes:\n",
        "        target_llm (Dict[str, str]): Configuration for the LLM under evaluation.\n",
        "        paraphrasing_llm (Dict[str, str]): Configuration for the LLM used for paraphrasing.\n",
        "        sentence_embedding_model (Dict[str, Any]): Configuration for the sentence embedding model.\n",
        "    \"\"\"\n",
        "    # Configuration dictionary for the primary Large Language Model being tested.\n",
        "    target_llm: Dict[str, str]\n",
        "\n",
        "    # Configuration dictionary for the LLM used to generate paraphrases.\n",
        "    paraphrasing_llm: Dict[str, str]\n",
        "\n",
        "    # Configuration dictionary for the sentence-transformer model used for embeddings.\n",
        "    sentence_embedding_model: Dict[str, Any]\n",
        "\n",
        "class LLMInferenceParams(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for LLM inference parameters, ensuring they are within\n",
        "    valid ranges.\n",
        "\n",
        "    Attributes:\n",
        "        temperature (float): Controls randomness. Must be in [0.0, 2.0].\n",
        "        top_p (float): Nucleus sampling parameter. Must be in [0.0, 1.0].\n",
        "        max_tokens (int): Maximum number of tokens to generate. Must be positive.\n",
        "    \"\"\"\n",
        "    # The temperature parameter for sampling, controlling creativity. Range: [0.0, 2.0].\n",
        "    temperature: float = Field(..., ge=0.0, le=2.0)\n",
        "\n",
        "    # The top-p (nucleus) sampling parameter. Range: [0.0, 1.0].\n",
        "    top_p: float = Field(..., ge=0.0, le=1.0)\n",
        "\n",
        "    # The maximum number of tokens to generate in a single API call. Must be greater than 0.\n",
        "    max_tokens: int = Field(..., gt=0)\n",
        "\n",
        "class FinalScoreWeights(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for the final S_H score weights and validates the\n",
        "    critical mathematical constraint that they must sum to 1.0.\n",
        "\n",
        "    Attributes:\n",
        "        w_jsd (float): The weight for the Ensemble JSD component. Must be in [0.0, 1.0].\n",
        "        w_wass (float): The weight for the Wasserstein Distance component. Must be in [0.0, 1.0].\n",
        "    \"\"\"\n",
        "    # The weight assigned to the Jensen-Shannon Divergence component of the S_H score.\n",
        "    w_jsd: float = Field(..., ge=0.0, le=1.0)\n",
        "\n",
        "    # The weight assigned to the Wasserstein Distance component of the S_H score.\n",
        "    w_wass: float = Field(..., ge=0.0, le=1.0)\n",
        "\n",
        "    @root_validator(pre=False)\n",
        "    def check_weights_sum_to_one(cls: ClassVar, values: Dict[str, float]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Validates the mathematical constraint that the component weights must sum to 1.0.\n",
        "        This is a `root_validator` because it depends on multiple fields.\n",
        "        Equation from paper: w_jsd + w_wass = 1.0\n",
        "\n",
        "        Args:\n",
        "            cls (ClassVar): The Pydantic model class.\n",
        "            values (Dict[str, float]): The dictionary of field values for this model.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the sum of w_jsd and w_wass is not approximately 1.0.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: The validated dictionary of values.\n",
        "        \"\"\"\n",
        "        # Retrieve the weight values from the model's data dictionary.\n",
        "        w_jsd, w_wass = values.get('w_jsd'), values.get('w_wass')\n",
        "\n",
        "        # Proceed only if both weight values have been successfully parsed.\n",
        "        if w_jsd is not None and w_wass is not None:\n",
        "            # Use math.isclose() for a robust floating-point comparison to handle potential precision errors.\n",
        "            # An absolute tolerance of 1e-9 is sufficient for this purpose.\n",
        "            if not math.isclose(w_jsd + w_wass, 1.0, abs_tol=1e-9):\n",
        "\n",
        "                # If the sum is not 1.0, raise a specific ValueError.\n",
        "                raise ValueError(\n",
        "                    \"Final score weights w_jsd and w_wass must sum to 1.0. \"\n",
        "                    f\"Current sum: {w_jsd + w_wass}\"\n",
        "                )\n",
        "        # If validation passes, return the original values dictionary.\n",
        "        return values\n",
        "\n",
        "class Hyperparameters(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for the main hyperparameters block, nesting other\n",
        "    parameter models for a structured and validated configuration.\n",
        "\n",
        "    Attributes:\n",
        "        corpus_generation (Dict[str, Any]): Parameters for generating paraphrases and responses (M, N).\n",
        "        llm_inference_params (Dict[str, LLMInferenceParams]): Nested model for inference parameters.\n",
        "        text_processing (Dict[str, Any]): Parameters for text processing steps like segmentation.\n",
        "        clustering (Dict[str, Any]): Parameters for the clustering algorithm.\n",
        "        final_score_weights (FinalScoreWeights): Nested model for the final score weights.\n",
        "        reproducibility (Dict[str, Any]): Parameters for ensuring deterministic execution (e.g., seeds).\n",
        "    \"\"\"\n",
        "    # Parameters controlling the generation of the text corpus (M and N).\n",
        "    corpus_generation: Dict[str, Any]\n",
        "\n",
        "    # A nested dictionary mapping generation type ('paraphrasing', 'response_generation') to its parameters.\n",
        "    llm_inference_params: Dict[str, LLMInferenceParams]\n",
        "\n",
        "    # Parameters for text processing, such as the sentence segmentation method.\n",
        "    text_processing: Dict[str, Any]\n",
        "\n",
        "    # Parameters for the clustering stage, including algorithm, linkage, and k-determination method.\n",
        "    clustering: Dict[str, Any]\n",
        "\n",
        "    # A nested model containing the validated weights for the final S_H score.\n",
        "    final_score_weights: FinalScoreWeights\n",
        "\n",
        "    # Parameters to ensure reproducibility, such as global random seeds.\n",
        "    reproducibility: Dict[str, Any]\n",
        "\n",
        "class PromptTemplates(BaseModel):\n",
        "    \"\"\"\n",
        "    Defines the schema for prompt templates and validates their structural integrity.\n",
        "\n",
        "    Attributes:\n",
        "        paraphrase_generation (Dict[str, str]): A dictionary containing the system and user prompts for paraphrasing.\n",
        "    \"\"\"\n",
        "    # A dictionary containing the templates for the paraphrase generation task.\n",
        "    paraphrase_generation: Dict[str, str]\n",
        "\n",
        "    @validator('paraphrase_generation')\n",
        "    def validate_paraphrase_template(cls: ClassVar, v: Dict[str, str]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Validates the structure of the paraphrase generation prompt template.\n",
        "\n",
        "        This validator ensures that the user prompt template contains the necessary\n",
        "        placeholders (`{num_paraphrases}`, `{original_prompt}`) and the required\n",
        "        instruction for JSON output, which are critical for the automated\n",
        "        data generation pipeline.\n",
        "\n",
        "        Args:\n",
        "            cls (ClassVar): The Pydantic model class.\n",
        "            v (Dict[str, str]): The dictionary containing the prompt templates.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the template is missing, malformed, or lacks required components.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, str]: The validated dictionary of templates.\n",
        "        \"\"\"\n",
        "        # Retrieve the user prompt template from the input dictionary.\n",
        "        user_template = v.get(\"user_prompt_template\")\n",
        "\n",
        "        # The user template is essential; it must be a non-empty string.\n",
        "        if not user_template or not isinstance(user_template, str):\n",
        "            raise ValueError(\"user_prompt_template must be a non-empty string.\")\n",
        "\n",
        "        # Define the set of placeholders that are required for the template to be functional.\n",
        "        required_placeholders = {\"{num_paraphrases}\", \"{original_prompt}\"}\n",
        "\n",
        "        # Use a regular expression to find all substrings that match the placeholder format {word}.\n",
        "        found_placeholders = set(re.findall(r\"\\{(\\w+)\\}\", user_template))\n",
        "\n",
        "        # Check if the set of found placeholders is a superset of the required placeholders.\n",
        "        if not required_placeholders.issubset(found_placeholders):\n",
        "            # If not, determine which placeholders are missing and raise an informative error.\n",
        "            missing = required_placeholders - found_placeholders\n",
        "            raise ValueError(f\"user_prompt_template is missing required placeholders: {missing}\")\n",
        "\n",
        "        # The pipeline relies on the LLM producing a JSON output; verify the instruction is present.\n",
        "        if \"```json\" not in user_template:\n",
        "            raise ValueError(\"user_prompt_template must include the '```json' instruction for formatted output.\")\n",
        "\n",
        "        # If all checks pass, return the validated template dictionary.\n",
        "        return v\n",
        "\n",
        "class FusedExperimentInputModel(BaseModel):\n",
        "    \"\"\"\n",
        "    The root Pydantic model that aggregates all nested configuration models.\n",
        "    Instantiating this model with a configuration dictionary triggers the entire\n",
        "    cascade of validations defined throughout the schema.\n",
        "\n",
        "    Attributes:\n",
        "        experiment_metadata (ExperimentMetadata): Validated experiment metadata.\n",
        "        computational_environment (ComputationalEnvironment): Validated environment specs.\n",
        "        primary_input_data (PrimaryInputData): Validated and cleansed input data.\n",
        "        system_components (SystemComponents): Validated system component configurations.\n",
        "        hyperparameters (Hyperparameters): Validated hyperparameter settings.\n",
        "        validation_protocols (Dict[str, Any]): Validation protocols (passed through).\n",
        "        error_handling (Dict[str, Any]): Error handling strategies (passed through).\n",
        "        expected_outputs (Dict[str, Any]): Expected output targets (passed through).\n",
        "        prompt_templates (PromptTemplates): Validated prompt templates.\n",
        "        execution_protocol (Dict[str, Any]): Execution protocol steps (passed through).\n",
        "    \"\"\"\n",
        "    # The validated metadata for the experiment.\n",
        "    experiment_metadata: ExperimentMetadata\n",
        "\n",
        "    # The validated computational environment requirements.\n",
        "    computational_environment: ComputationalEnvironment\n",
        "\n",
        "    # The validated and cleansed primary input data.\n",
        "    primary_input_data: PrimaryInputData\n",
        "\n",
        "    # The validated configurations for all system components.\n",
        "    system_components: SystemComponents\n",
        "\n",
        "    # The validated set of all hyperparameters.\n",
        "    hyperparameters: Hyperparameters\n",
        "\n",
        "    # A dictionary defining validation protocols for various stages.\n",
        "    validation_protocols: Dict[str, Any]\n",
        "\n",
        "    # A dictionary defining error handling and fallback strategies.\n",
        "    error_handling: Dict[str, Any]\n",
        "\n",
        "    # A dictionary defining the expected ranges and types for key output metrics.\n",
        "    expected_outputs: Dict[str, Any]\n",
        "\n",
        "    # The validated prompt templates.\n",
        "    prompt_templates: PromptTemplates\n",
        "\n",
        "    # A dictionary outlining the sequence of steps in the experiment execution.\n",
        "    execution_protocol: Dict[str, Any]\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Fused Validator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_and_clean_sdm_config(\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, FusedExperimentInputModel | None, str]:\n",
        "    \"\"\"\n",
        "    Performs comprehensive validation and cleansing of the SDM input configuration.\n",
        "\n",
        "    This function serves as the single entry point for Task 1. It uses a series\n",
        "    of nested Pydantic models to rigorously validate the entire configuration\n",
        "    dictionary's structure, types, and mathematical constraints. It also\n",
        "    cleanses the primary input prompt text.\n",
        "\n",
        "    Args:\n",
        "        config: The raw input dictionary for the SDM experiment, conforming to\n",
        "                the FusedExperimentInput structure.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - bool: True if validation is successful, False otherwise.\n",
        "        - FusedExperimentInputModel | None: A validated and type-safe Pydantic\n",
        "          model instance of the configuration if successful, otherwise None.\n",
        "        - str: A detailed success or error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1.1.1: Validate the entire dictionary schema completeness and types.\n",
        "        # Pydantic recursively instantiates the nested models. This single line\n",
        "        # performs the schema validation, type checking, and runs all custom\n",
        "        # validators defined in the models above (for constraints, templates, etc.).\n",
        "        validated_model = FusedExperimentInputModel(**config)\n",
        "\n",
        "        # If instantiation is successful, all validations have passed.\n",
        "        # Prepare a success message.\n",
        "        success_message = (\n",
        "            \"Configuration validation successful. All structures, types, \"\n",
        "            \"constraints, and formats are correct. Prompt text has been cleansed.\"\n",
        "        )\n",
        "\n",
        "        # Return the success status, the validated model, and the message.\n",
        "        return True, validated_model, success_message\n",
        "\n",
        "    except ValidationError as e:\n",
        "        # If Pydantic's validation fails, it raises a ValidationError.\n",
        "        # This exception contains detailed information about all failures.\n",
        "        # We format this information into a clear, actionable error message.\n",
        "        error_message = f\"Configuration validation failed with {len(e.errors())} error(s):\\n\"\n",
        "\n",
        "        # Iterate through each validation error to provide specific details.\n",
        "        for error in e.errors():\n",
        "            # Get the location (path) of the error in the dictionary.\n",
        "            loc = \" -> \".join(map(str, error['loc']))\n",
        "            # Get the specific error message.\n",
        "            msg = error['msg']\n",
        "            # Append the formatted error to the main message.\n",
        "            error_message += f\"- Location: [{loc}], Message: {msg}\\n\"\n",
        "\n",
        "        # Return the failure status, None for the model, and the detailed error message.\n",
        "        return False, None, error_message\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during validation.\n",
        "        # This ensures the function is robust.\n",
        "        error_message = f\"An unexpected error occurred during validation: {e}\"\n",
        "\n",
        "        # Return the failure status, None for the model, and the error message.\n",
        "        return False, None, error_message\n"
      ],
      "metadata": {
        "id": "08DlT-nMKsUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Environment Setup and Model Initialization\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Environment Setup and Model Initialization\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module orchestrates the complete setup of the computational environment.\n",
        "# It ensures reproducibility by setting deterministic seeds, configures the\n",
        "# appropriate hardware, validates library dependencies, and initializes all\n",
        "# required models and API clients according to the validated configuration.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Initialized Components\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SDMRuntimeEnvironment:\n",
        "    \"\"\"\n",
        "    A container for all initialized components required for the SDM pipeline.\n",
        "\n",
        "    This dataclass provides a clean, type-safe way to pass the fully configured\n",
        "    runtime environment between different stages of the SDM framework.\n",
        "\n",
        "    Attributes:\n",
        "        device (torch.device): The selected computational device (e.g., 'cuda:0' or 'cpu').\n",
        "        openai_client (OpenAI): The initialized OpenAI API client.\n",
        "        embedding_model (SentenceTransformer): The loaded sentence embedding model.\n",
        "    \"\"\"\n",
        "    # The PyTorch device object configured for computation (e.g., CPU or a specific GPU).\n",
        "    device: torch.device\n",
        "    # An initialized and authenticated OpenAI client for API interactions.\n",
        "    openai_client: OpenAI\n",
        "    # The loaded and device-mapped SentenceTransformer model for embedding generation.\n",
        "    embedding_model: SentenceTransformer\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2.1: Deterministic Reproducibility Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "def _set_deterministic_seeds(config: FusedExperimentInputModel) -> None:\n",
        "    \"\"\"\n",
        "    Sets random seeds for all relevant libraries to ensure reproducibility.\n",
        "\n",
        "    This function takes the reproducibility parameters from the validated\n",
        "    configuration and applies them to `random`, `numpy`, and `torch`. It also\n",
        "    configures PyTorch's CuDNN backend for deterministic behavior when a GPU\n",
        "    is used.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "    \"\"\"\n",
        "    # Retrieve the reproducibility settings dictionary from the config.\n",
        "    repro_config = config.hyperparameters.reproducibility\n",
        "    # Extract the global random seed.\n",
        "    seed = repro_config['global_random_seed']\n",
        "\n",
        "    # Set the seed for Python's built-in `random` module.\n",
        "    random.seed(seed)\n",
        "    # Set the seed for NumPy's random number generators.\n",
        "    np.random.seed(seed)\n",
        "    # Set the seed for PyTorch's random number generators on both CPU and GPU.\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Check if a CUDA-enabled GPU is available.\n",
        "    if torch.cuda.is_available():\n",
        "        # Set the seed for all available GPUs to ensure deterministic initialization.\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Configure the CuDNN backend to use deterministic algorithms.\n",
        "        # This can impact performance but is crucial for reproducibility.\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        # Disable the CuDNN benchmark feature, which can introduce non-determinism.\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Log the successful application of the deterministic settings.\n",
        "    logging.info(f\"Set deterministic seeds to {seed} for all relevant libraries.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2.2: Computational Device Selection\n",
        "# ==============================================================================\n",
        "\n",
        "def _initialize_torch_device(config: FusedExperimentInputModel) -> torch.device:\n",
        "    \"\"\"\n",
        "    Selects and initializes the computational device (GPU or CPU).\n",
        "\n",
        "    This function follows the hierarchy specified in the configuration: it\n",
        "    attempts to use the primary GPU, validates its memory, and falls back\n",
        "    to the CPU if the requirements are not met.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: The selected and initialized PyTorch device object.\n",
        "    \"\"\"\n",
        "    # Retrieve the device requirements dictionary from the config.\n",
        "    device_reqs = config.computational_environment.device_requirements\n",
        "\n",
        "    # Get the preferred primary device identifier (e.g., 'cuda:0').\n",
        "    primary_device_name = device_reqs['primary']\n",
        "\n",
        "    # Check if the primary device is a CUDA device and if CUDA is available.\n",
        "    if primary_device_name.startswith('cuda') and torch.cuda.is_available():\n",
        "\n",
        "        # Attempt to use the specified primary GPU.\n",
        "        device = torch.device(primary_device_name)\n",
        "\n",
        "        # Get properties of the selected GPU.\n",
        "        gpu_properties = torch.cuda.get_device_properties(device)\n",
        "\n",
        "        # Get the available memory on the GPU in bytes.\n",
        "        _, free_memory_bytes = torch.cuda.mem_get_info(device)\n",
        "\n",
        "        # Convert available memory to Gigabytes.\n",
        "        free_memory_gb = free_memory_bytes / (1024 ** 3)\n",
        "\n",
        "        # Get the minimum required GPU memory from the config.\n",
        "        min_memory_gb = device_reqs['min_gpu_memory_gb']\n",
        "\n",
        "        # Check if the available memory meets the minimum requirement.\n",
        "        if free_memory_gb >= min_memory_gb:\n",
        "\n",
        "            # If memory is sufficient, log success and return the GPU device.\n",
        "            logging.info(\n",
        "                f\"Primary device '{primary_device_name}' selected. \"\n",
        "                f\"Name: {gpu_properties.name}, \"\n",
        "                f\"Available Memory: {free_memory_gb:.2f} GB.\"\n",
        "            )\n",
        "            return device\n",
        "        else:\n",
        "            # If memory is insufficient, log a warning and prepare to fall back.\n",
        "            logging.warning(\n",
        "                f\"Primary device '{primary_device_name}' has insufficient memory \"\n",
        "                f\"({free_memory_gb:.2f} GB available, {min_memory_gb} GB required). \"\n",
        "                f\"Falling back to CPU.\"\n",
        "            )\n",
        "    else:\n",
        "        # If the primary device is not CUDA or CUDA is not available, log this.\n",
        "        logging.warning(\n",
        "            f\"Primary device '{primary_device_name}' not available. \"\n",
        "            f\"Falling back to CPU.\"\n",
        "        )\n",
        "\n",
        "    # If any GPU check fails, select the fallback CPU device.\n",
        "    fallback_device = torch.device(device_reqs['fallback'])\n",
        "    # Log the fallback selection.\n",
        "    logging.info(f\"Fallback device '{fallback_device}' selected.\")\n",
        "    # Return the CPU device object.\n",
        "    return fallback_device\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2.3: Library Version Compatibility Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_library_versions(config: FusedExperimentInputModel) -> None:\n",
        "    \"\"\"\n",
        "    Validates that installed libraries meet the version requirements.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Raises:\n",
        "        EnvironmentError: If a required library is missing or its version is\n",
        "                          incompatible.\n",
        "    \"\"\"\n",
        "    # Retrieve the dictionary of required libraries and their versions.\n",
        "    required_libs = config.computational_environment.required_libraries\n",
        "    # Log the start of the validation process.\n",
        "    logging.info(\"Validating required library versions...\")\n",
        "\n",
        "    # A list to store any validation errors found.\n",
        "    errors = []\n",
        "\n",
        "    # Iterate through each required library and its version specifier.\n",
        "    for lib_name, required_version_str in required_libs.items():\n",
        "        try:\n",
        "            # Get the version of the installed package.\n",
        "            installed_version_str = importlib.metadata.version(lib_name)\n",
        "            # Parse the installed version string into a Version object.\n",
        "            installed_version = parse_version(installed_version_str)\n",
        "            # Parse the required version string.\n",
        "            required_version = parse_version(required_version_str.lstrip('>='))\n",
        "\n",
        "            # Check if the installed version meets the '>=' requirement.\n",
        "            if not (installed_version >= required_version):\n",
        "                # If not, add a descriptive error message to the list.\n",
        "                errors.append(\n",
        "                    f\"- Library '{lib_name}': Installed version {installed_version} \"\n",
        "                    f\"does not meet requirement >={required_version}.\"\n",
        "                )\n",
        "        except importlib.metadata.PackageNotFoundError:\n",
        "            # If the library is not found, add a corresponding error message.\n",
        "            errors.append(f\"- Library '{lib_name}' is not installed.\")\n",
        "\n",
        "    # After checking all libraries, see if any errors were found.\n",
        "    if errors:\n",
        "        # If there are errors, combine them into a single error message.\n",
        "        error_message = \"Environment validation failed:\\n\" + \"\\n\".join(errors)\n",
        "        # Raise an EnvironmentError, stopping the pipeline.\n",
        "        raise EnvironmentError(error_message)\n",
        "\n",
        "    # If no errors were found, log the success.\n",
        "    logging.info(\"All required library versions are compatible.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2.4: Model and Client Loading\n",
        "# ==============================================================================\n",
        "\n",
        "def _initialize_openai_client(config: FusedExperimentInputModel) -> OpenAI:\n",
        "    \"\"\"\n",
        "    Initializes and validates the OpenAI API client.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        OpenAI: An initialized OpenAI client instance.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the OPENAI_API_KEY environment variable is not set.\n",
        "    \"\"\"\n",
        "    # Log the initialization attempt.\n",
        "    logging.info(\"Initializing OpenAI API client...\")\n",
        "    # Retrieve the API key from environment variables for security.\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    # Check if the API key was found.\n",
        "    if not api_key:\n",
        "        # If not, raise a ValueError with instructions.\n",
        "        raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
        "\n",
        "    # Initialize the OpenAI client with the retrieved API key.\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    # Log success.\n",
        "    logging.info(\"OpenAI API client initialized successfully.\")\n",
        "    # Return the client instance.\n",
        "    return client\n",
        "\n",
        "def _initialize_embedding_model(\n",
        "    config: FusedExperimentInputModel,\n",
        "    device: torch.device\n",
        ") -> SentenceTransformer:\n",
        "    \"\"\"\n",
        "    Loads the sentence embedding model with a fallback mechanism.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "        device: The torch.device to which the model should be moved.\n",
        "\n",
        "    Returns:\n",
        "        SentenceTransformer: The loaded and device-mapped sentence embedding model.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If both the primary and fallback models fail to load.\n",
        "    \"\"\"\n",
        "    # Retrieve the embedding model configuration.\n",
        "    model_config = config.system_components.sentence_embedding_model\n",
        "    # Get the identifier for the primary model.\n",
        "    primary_model_id = model_config['model_identifier']\n",
        "    # Get the identifier for the fallback model.\n",
        "    fallback_model_id = model_config['fallback_model']\n",
        "\n",
        "    try:\n",
        "        # Attempt to load the primary sentence embedding model.\n",
        "        logging.info(f\"Attempting to load primary embedding model: '{primary_model_id}'...\")\n",
        "        # The SentenceTransformer library handles downloading from Hugging Face Hub.\n",
        "        model = SentenceTransformer(primary_model_id, device=device)\n",
        "        # Log success.\n",
        "        logging.info(f\"Successfully loaded primary model '{primary_model_id}'.\")\n",
        "        # Return the loaded model.\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        # If loading the primary model fails, log a warning.\n",
        "        logging.warning(\n",
        "            f\"Failed to load primary embedding model '{primary_model_id}': {e}. \"\n",
        "            f\"Attempting to load fallback model '{fallback_model_id}'.\"\n",
        "        )\n",
        "        try:\n",
        "            # Attempt to load the fallback model.\n",
        "            model = SentenceTransformer(fallback_model_id, device=device)\n",
        "            # Log success with the fallback model.\n",
        "            logging.info(f\"Successfully loaded fallback model '{fallback_model_id}'.\")\n",
        "            # Return the loaded fallback model.\n",
        "            return model\n",
        "        except Exception as e_fallback:\n",
        "            # If the fallback model also fails, raise a terminal RuntimeError.\n",
        "            error_message = (\n",
        "                \"Fatal: Failed to load both primary and fallback embedding models. \"\n",
        "                f\"Primary error: {e}, Fallback error: {e_fallback}\"\n",
        "            )\n",
        "            logging.error(error_message)\n",
        "            raise RuntimeError(error_message)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2.5: Text Processing Component Initialization\n",
        "# ==============================================================================\n",
        "\n",
        "def _initialize_text_processors(config: FusedExperimentInputModel) -> None:\n",
        "    \"\"\"\n",
        "    Initializes text processing components, specifically NLTK's tokenizer.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Raises:\n",
        "        ConnectionError: If the required NLTK data cannot be downloaded.\n",
        "    \"\"\"\n",
        "    # Retrieve the text processing configuration.\n",
        "    text_config = config.hyperparameters.text_processing\n",
        "    # Check if the specified method is NLTK's tokenizer.\n",
        "    if text_config['sentence_segmentation_method'] == 'nltk.sent_tokenize':\n",
        "        # Log the initialization attempt.\n",
        "        logging.info(\"Initializing NLTK sentence tokenizer...\")\n",
        "        try:\n",
        "            # Download the 'punkt' tokenizer models required by nltk.sent_tokenize.\n",
        "            # The 'quiet=True' flag suppresses verbose output on success.\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            # Log success.\n",
        "            logging.info(\"NLTK 'punkt' tokenizer data is available.\")\n",
        "        except Exception as e:\n",
        "            # If the download fails, raise a ConnectionError with instructions.\n",
        "            error_message = (\n",
        "                \"Failed to download NLTK 'punkt' data. \"\n",
        "                \"Please ensure internet connectivity or download manually. \"\n",
        "                f\"Error: {e}\"\n",
        "            )\n",
        "            logging.error(error_message)\n",
        "            raise ConnectionError(error_message)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def initialize_environment_and_models(\n",
        "    config: FusedExperimentInputModel\n",
        ") -> SDMRuntimeEnvironment:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire environment setup and model initialization process.\n",
        "\n",
        "    This function serves as the master controller for Task 2. It executes each\n",
        "    setup and initialization step in the correct order, ensuring that the\n",
        "    environment is reproducible, dependencies are met, and all necessary\n",
        "    models and clients are loaded and ready for the pipeline.\n",
        "\n",
        "    Args:\n",
        "        config: A validated Pydantic model instance of the experiment configuration,\n",
        "                typically the output from `validate_and_clean_sdm_config`.\n",
        "\n",
        "    Returns:\n",
        "        SDMRuntimeEnvironment: A dataclass instance containing all the initialized\n",
        "                               components needed for the subsequent SDM tasks.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates any exceptions raised during the setup process,\n",
        "                   halting the pipeline if the environment is not valid.\n",
        "    \"\"\"\n",
        "    # Log the start of the entire Task 2 process.\n",
        "    logging.info(\"--- Starting Task 2: Environment Setup and Model Initialization ---\")\n",
        "\n",
        "    # Execute Step 2.1: Set all random seeds for reproducibility.\n",
        "    _set_deterministic_seeds(config)\n",
        "\n",
        "    # Execute Step 2.3: Validate library versions before proceeding.\n",
        "    _validate_library_versions(config)\n",
        "\n",
        "    # Execute Step 2.2: Select the computational device (GPU/CPU).\n",
        "    device = _initialize_torch_device(config)\n",
        "\n",
        "    # Execute Step 2.4 (Part 1): Initialize the OpenAI API client.\n",
        "    openai_client = _initialize_openai_client(config)\n",
        "\n",
        "    # Execute Step 2.4 (Part 2): Load the sentence embedding model.\n",
        "    embedding_model = _initialize_embedding_model(config, device)\n",
        "\n",
        "    # Execute Step 2.5: Initialize text processing components like NLTK.\n",
        "    _initialize_text_processors(config)\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 2 Successfully Completed: Environment is ready. ---\")\n",
        "\n",
        "    # Assemble all initialized components into the runtime environment dataclass.\n",
        "    runtime_env = SDMRuntimeEnvironment(\n",
        "        device=device,\n",
        "        openai_client=openai_client,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    # Return the fully configured runtime environment.\n",
        "    return runtime_env\n"
      ],
      "metadata": {
        "id": "0gxLL7k8MEKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Paraphrase Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Paraphrase Generation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module is responsible for generating and validating a corpus of M\n",
        "# semantically equivalent paraphrases of the original prompt. It combines\n",
        "# precise prompt engineering, robust API interaction with fault tolerance,\n",
        "# and rigorous, multi-faceted quality validation to produce a high-fidelity\n",
        "# input set for the core SDM analysis.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3.1: Prompt Template Preparation and API Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "def _prepare_paraphrase_request(\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Tuple[List[Dict[str, str]], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepares the complete request payload for the paraphrase generation API call.\n",
        "\n",
        "    This function constructs the message list and inference parameters based on the\n",
        "    validated configuration. It injects the original prompt and the required\n",
        "    number of paraphrases into the prompt template.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - list: The list of messages for the OpenAI API chat completion endpoint.\n",
        "        - dict: A dictionary of inference parameters (model, temperature, etc.).\n",
        "    \"\"\"\n",
        "    # Retrieve the paraphrase generation template dictionary.\n",
        "    template_config = config.prompt_templates.paraphrase_generation\n",
        "    # Retrieve the LLM parameters specific to paraphrasing.\n",
        "    params_config = config.hyperparameters.llm_inference_params['paraphrasing']\n",
        "\n",
        "    # Format the user prompt by substituting the required number of paraphrases (M)\n",
        "    # and the cleansed original prompt text.\n",
        "    user_prompt = template_config['user_prompt_template'].format(\n",
        "        num_paraphrases=config.hyperparameters.corpus_generation['num_paraphrases_M'],\n",
        "        original_prompt=config.primary_input_data.cleaned_prompt_text\n",
        "    )\n",
        "\n",
        "    # Construct the message list in the format required by the OpenAI API.\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": template_config['system_prompt']},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Assemble the dictionary of parameters for the API call.\n",
        "    request_params = {\n",
        "        \"model\": config.system_components.paraphrasing_llm['model_identifier'],\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": params_config.temperature,\n",
        "        \"top_p\": params_config.top_p,\n",
        "        \"max_tokens\": params_config.max_tokens,\n",
        "        \"response_format\": {\"type\": \"json_object\"}, # Enforce JSON output\n",
        "    }\n",
        "\n",
        "    # Return the fully constructed request payload.\n",
        "    return messages, request_params\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3.2: Paraphrase Generation Execution and Parsing\n",
        "# ==============================================================================\n",
        "\n",
        "def _parse_paraphrase_response(\n",
        "    response: ChatCompletion,\n",
        "    expected_count: int\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Parses and validates the JSON response from the paraphrase generation API call.\n",
        "\n",
        "    This function attempts to parse the LLM's response as a JSON list of strings.\n",
        "    It includes robust fallback logic to extract a JSON object from surrounding\n",
        "    text if direct parsing fails.\n",
        "\n",
        "    Args:\n",
        "        response: The ChatCompletion object returned by the OpenAI API.\n",
        "        expected_count: The number of paraphrases expected (M).\n",
        "\n",
        "    Returns:\n",
        "        A list of strings containing the generated paraphrases.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If parsing fails, the response is not a list, or the\n",
        "                    number of paraphrases does not match the expected count.\n",
        "    \"\"\"\n",
        "    # Extract the text content from the first choice in the API response.\n",
        "    response_text = response.choices[0].message.content\n",
        "\n",
        "    # Input validation: Ensure response_text is not None or empty.\n",
        "    if not response_text:\n",
        "        raise ValueError(\"Received an empty response from the paraphrase generation API.\")\n",
        "\n",
        "    try:\n",
        "        # Attempt to directly parse the entire response text as JSON.\n",
        "        # The prompt requests a JSON object with a key, e.g., {\"paraphrases\": [...]}.\n",
        "        parsed_json = json.loads(response_text)\n",
        "        # Assume the list is the first value in the parsed dictionary.\n",
        "        # This is a robust way to handle variable key names from the LLM.\n",
        "        if isinstance(parsed_json, dict) and parsed_json:\n",
        "            paraphrases = next(iter(parsed_json.values()))\n",
        "        else:\n",
        "            raise ValueError(\"Parsed JSON is not a dictionary.\")\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        # If direct parsing fails, it may be due to extraneous text around the JSON.\n",
        "        logging.warning(\"Direct JSON parsing failed. Attempting regex extraction.\")\n",
        "        # Use a regex to find a JSON array '[...]' within the response text.\n",
        "        match = re.search(r'\\[\\s*\".*?\"\\s*\\]', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                # If a match is found, try to parse just that substring.\n",
        "                paraphrases = json.loads(match.group(0))\n",
        "            except json.JSONDecodeError:\n",
        "                # If even the extracted part is not valid JSON, parsing has failed.\n",
        "                raise ValueError(\"Failed to parse extracted JSON array from the API response.\")\n",
        "        else:\n",
        "            # If no JSON array is found at all, parsing has failed.\n",
        "            raise ValueError(\"No valid JSON array found in the API response.\")\n",
        "\n",
        "    # Validate the type of the parsed data. It must be a list.\n",
        "    if not isinstance(paraphrases, list):\n",
        "        raise ValueError(f\"Expected a list of paraphrases, but got type {type(paraphrases)}.\")\n",
        "\n",
        "    # Validate that every item in the list is a string.\n",
        "    if not all(isinstance(p, str) for p in paraphrases):\n",
        "        raise ValueError(\"The parsed list contains non-string elements.\")\n",
        "\n",
        "    # Validate that the number of generated paraphrases matches the expected count.\n",
        "    if len(paraphrases) != expected_count:\n",
        "        raise ValueError(\n",
        "            f\"Expected {expected_count} paraphrases, but received {len(paraphrases)}.\"\n",
        "        )\n",
        "\n",
        "    # Return the clean, validated list of paraphrases.\n",
        "    return paraphrases\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "    retry=retry_if_exception_type(Exception), # Retry on any API or parsing error\n",
        "    reraise=True # Reraise the exception if all retries fail\n",
        ")\n",
        "def _execute_paraphrase_generation(\n",
        "    openai_client: OpenAI,\n",
        "    request_params: Dict[str, Any],\n",
        "    expected_count: int\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Executes the API call to generate paraphrases with robust retry logic.\n",
        "\n",
        "    This function wraps the OpenAI API call with the `tenacity` library to\n",
        "    automatically handle transient errors (e.g., network issues, rate limits)\n",
        "    by retrying with exponential backoff.\n",
        "\n",
        "    Args:\n",
        "        openai_client: The initialized OpenAI client.\n",
        "        request_params: The dictionary of parameters for the API call.\n",
        "        expected_count: The number of paraphrases expected (M).\n",
        "\n",
        "    Returns:\n",
        "        A list of strings containing the generated paraphrases.\n",
        "    \"\"\"\n",
        "    # Log the attempt to call the API.\n",
        "    logging.info(f\"Generating {expected_count} paraphrases via API call...\")\n",
        "\n",
        "    # Make the API call to the chat completions endpoint.\n",
        "    response = openai_client.chat.completions.create(**request_params)\n",
        "\n",
        "    # Parse the response to extract and validate the paraphrases.\n",
        "    paraphrases = _parse_paraphrase_response(response, expected_count)\n",
        "\n",
        "    # Log the successful generation and parsing of the response.\n",
        "    logging.info(f\"Successfully generated and parsed {len(paraphrases)} paraphrases.\")\n",
        "\n",
        "    # Return the list of paraphrases.\n",
        "    return paraphrases\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3.3: Paraphrase Quality Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_paraphrases(\n",
        "    original_prompt: str,\n",
        "    paraphrases: List[str],\n",
        "    embedding_model: SentenceTransformer,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Performs rigorous quality validation on the generated paraphrases.\n",
        "\n",
        "    This function checks for semantic similarity to the original prompt and\n",
        "    ensures the paraphrases meet length constraints.\n",
        "\n",
        "    Args:\n",
        "        original_prompt: The original, cleansed prompt text.\n",
        "        paraphrases: The list of generated paraphrases.\n",
        "        embedding_model: The initialized sentence embedding model.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - bool: True if all paraphrases pass validation, False otherwise.\n",
        "        - str: A detailed report of the validation results.\n",
        "    \"\"\"\n",
        "    # Retrieve the validation parameters from the configuration.\n",
        "    validation_params = config.validation_protocols['paraphrase_validation']\n",
        "    sim_threshold = validation_params['semantic_similarity_threshold']\n",
        "\n",
        "    # Combine the original prompt with the paraphrases for batch embedding.\n",
        "    all_texts = [original_prompt] + paraphrases\n",
        "\n",
        "    # Generate embeddings for all texts in a single batch for efficiency.\n",
        "    logging.info(\"Generating embeddings for paraphrase validation...\")\n",
        "    embeddings = embedding_model.encode(all_texts, convert_to_numpy=True)\n",
        "\n",
        "    # Separate the embedding of the original prompt from the paraphrase embeddings.\n",
        "    original_embedding = embeddings[0]\n",
        "    paraphrase_embeddings = embeddings[1:]\n",
        "\n",
        "    # Calculate cosine similarity between the original prompt and each paraphrase.\n",
        "    # Cosine Similarity(A, B) = (A . B) / (||A|| * ||B||)\n",
        "    similarities = (paraphrase_embeddings @ original_embedding.T) / (\n",
        "        norm(paraphrase_embeddings, axis=1) * norm(original_embedding)\n",
        "    )\n",
        "\n",
        "    # A list to store detailed validation failure messages.\n",
        "    validation_failures = []\n",
        "\n",
        "    # Iterate through each paraphrase and its similarity score.\n",
        "    for i, (paraphrase, sim) in enumerate(zip(paraphrases, similarities)):\n",
        "        # Check if the similarity score meets the required threshold.\n",
        "        if sim < sim_threshold:\n",
        "            validation_failures.append(\n",
        "                f\"Paraphrase {i+1} failed semantic similarity check \"\n",
        "                f\"(Score: {sim:.4f}, Threshold: {sim_threshold}).\"\n",
        "            )\n",
        "\n",
        "        # Check the length deviation constraint.\n",
        "        len_original = len(original_prompt.split())\n",
        "        len_paraphrase = len(paraphrase.split())\n",
        "        # Ensure length is not zero to avoid division by zero.\n",
        "        if len_original > 0 and abs(len_paraphrase - len_original) / len_original > 0.5:\n",
        "            validation_failures.append(\n",
        "                f\"Paraphrase {i+1} failed length deviation check \"\n",
        "                f\"(Original: {len_original} words, Paraphrase: {len_paraphrase} words).\"\n",
        "            )\n",
        "\n",
        "    # Check if any failures were recorded.\n",
        "    if validation_failures:\n",
        "        # If so, compile a detailed report and return a failure status.\n",
        "        report = \"Paraphrase validation failed:\\n\" + \"\\n\".join(validation_failures)\n",
        "        return False, report\n",
        "    else:\n",
        "        # If all checks pass, create a success report.\n",
        "        report = (\n",
        "            f\"All {len(paraphrases)} paraphrases passed validation. \"\n",
        "            f\"Average semantic similarity: {np.mean(similarities):.4f}.\"\n",
        "        )\n",
        "        return True, report\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_validated_paraphrases(\n",
        "    config: FusedExperimentInputModel,\n",
        "    runtime_env: SDMRuntimeEnvironment\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end process of generating and validating paraphrases.\n",
        "\n",
        "    This function controls the entire workflow for Task 3, including prompt\n",
        "    preparation, API execution with retries, response parsing, and rigorous\n",
        "    quality validation with a regeneration loop.\n",
        "\n",
        "    Args:\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "        runtime_env: The dataclass containing initialized models and clients.\n",
        "\n",
        "    Returns:\n",
        "        A list of M validated, high-quality paraphrases.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If unable to generate a valid set of paraphrases after\n",
        "                      the maximum number of attempts.\n",
        "    \"\"\"\n",
        "    # Log the start of the entire Task 3 process.\n",
        "    logging.info(\"--- Starting Task 3: Paraphrase Generation and Validation ---\")\n",
        "\n",
        "    # Retrieve the maximum number of retries for the entire process from config.\n",
        "    max_attempts = config.error_handling['api_errors']['max_retries']\n",
        "\n",
        "    # Loop for a maximum number of attempts to get a valid set of paraphrases.\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        # Log the current attempt number.\n",
        "        logging.info(f\"Generation attempt {attempt}/{max_attempts}...\")\n",
        "\n",
        "        try:\n",
        "            # Step 3.1: Prepare the API request payload.\n",
        "            messages, request_params = _prepare_paraphrase_request(config)\n",
        "\n",
        "            # Step 3.2: Execute the API call to generate paraphrases.\n",
        "            # This function has its own internal retry logic for transient errors.\n",
        "            generated_paraphrases = _execute_paraphrase_generation(\n",
        "                openai_client=runtime_env.openai_client,\n",
        "                request_params=request_params,\n",
        "                expected_count=config.hyperparameters.corpus_generation['num_paraphrases_M']\n",
        "            )\n",
        "\n",
        "            # Step 3.3: Perform quality validation on the generated paraphrases.\n",
        "            is_valid, report = _validate_paraphrases(\n",
        "                original_prompt=config.primary_input_data.cleaned_prompt_text,\n",
        "                paraphrases=generated_paraphrases,\n",
        "                embedding_model=runtime_env.embedding_model,\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Log the detailed validation report.\n",
        "            logging.info(report)\n",
        "\n",
        "            # Check if the validation was successful.\n",
        "            if is_valid:\n",
        "                # If valid, log success and return the list of paraphrases.\n",
        "                logging.info(\"--- Task 3 Successfully Completed: Valid paraphrase set generated. ---\")\n",
        "                return generated_paraphrases\n",
        "\n",
        "        except Exception as e:\n",
        "            # If any step (generation, parsing, validation) fails, log the error.\n",
        "            logging.error(f\"Attempt {attempt} failed: {e}\")\n",
        "            # If this was the last attempt, break the loop to raise the final error.\n",
        "            if attempt == max_attempts:\n",
        "                break\n",
        "\n",
        "    # If the loop completes without returning, it means all attempts have failed.\n",
        "    # Raise a terminal RuntimeError.\n",
        "    final_error_message = \"Failed to generate a valid set of paraphrases after all attempts.\"\n",
        "    logging.error(final_error_message)\n",
        "    raise RuntimeError(final_error_message)\n"
      ],
      "metadata": {
        "id": "IkfQNovIRhyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Response Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Response Generation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module orchestrates the generation of the main response corpus. It takes\n",
        "# the validated paraphrases and systematically generates N responses for each of\n",
        "# the M paraphrases, resulting in an M x N matrix of responses. The process is\n",
        "# designed for efficiency using asynchronous API calls and includes rigorous\n",
        "# validation for each generated response.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4.1: Response Generation Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "def _prepare_response_request(\n",
        "    paraphrase: str,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Prepares the request payload for a single response generation API call.\n",
        "\n",
        "    Args:\n",
        "        paraphrase: The paraphrase text to be used as the prompt.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the complete request payload for the API call.\n",
        "    \"\"\"\n",
        "    # Retrieve the inference parameters for response generation.\n",
        "    params_config = config.hyperparameters.llm_inference_params['response_generation']\n",
        "\n",
        "    # Construct the message list for the API call. The paraphrase serves as the user prompt.\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": paraphrase}\n",
        "    ]\n",
        "\n",
        "    # Assemble the dictionary of parameters for the API call.\n",
        "    request_params = {\n",
        "        \"model\": config.system_components.target_llm['model_identifier'],\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": params_config.temperature,\n",
        "        \"top_p\": params_config.top_p,\n",
        "        \"max_tokens\": params_config.max_tokens,\n",
        "    }\n",
        "\n",
        "    # Return the fully constructed request payload.\n",
        "    return request_params\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4.2: Systematic and Asynchronous Response Generation\n",
        "# ==============================================================================\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=2, min=5, max=30),\n",
        "    retry=retry_if_exception_type(APIError),\n",
        "    reraise=True\n",
        ")\n",
        "async def _execute_single_response_generation(\n",
        "    async_client: AsyncOpenAI,\n",
        "    request_params: Dict[str, Any],\n",
        "    semaphore: asyncio.Semaphore\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Executes a single asynchronous API call to generate one response.\n",
        "\n",
        "    This function is designed to be run concurrently. It uses a semaphore to\n",
        "    limit the number of simultaneous requests and includes tenacity-based\n",
        "    retry logic for robustness against transient API errors.\n",
        "\n",
        "    Args:\n",
        "        async_client: The initialized asynchronous OpenAI client.\n",
        "        request_params: The dictionary of parameters for the API call.\n",
        "        semaphore: An asyncio.Semaphore to limit concurrency.\n",
        "\n",
        "    Returns:\n",
        "        The generated response text as a string, or None if an unrecoverable\n",
        "        error occurred.\n",
        "    \"\"\"\n",
        "    # Acquire the semaphore before making the API call to limit concurrency.\n",
        "    async with semaphore:\n",
        "        try:\n",
        "            # Make the asynchronous API call.\n",
        "            response = await async_client.chat.completions.create(**request_params)\n",
        "            # Extract the response text from the first choice.\n",
        "            response_text = response.choices[0].message.content\n",
        "            # Return the generated text.\n",
        "            return response_text\n",
        "        except Exception as e:\n",
        "            # If any error occurs during the API call, log it.\n",
        "            logging.error(f\"API call failed after retries: {e}. Request: {request_params['messages'][-1]['content'][:100]}...\")\n",
        "            # Return None to indicate failure for this specific request.\n",
        "            return None\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4.3: Response Validation and Quality Control\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_single_response(\n",
        "    response_text: Optional[str],\n",
        "    config: FusedExperimentInputModel\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Performs quality validation on a single generated response.\n",
        "\n",
        "    Checks for non-emptiness, length constraints, and content integrity.\n",
        "    A failed response is replaced with an empty string for safe downstream\n",
        "    processing.\n",
        "\n",
        "    Args:\n",
        "        response_text: The generated response text, or None if generation failed.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        The validated response text, or an empty string if validation fails.\n",
        "    \"\"\"\n",
        "    # Retrieve validation parameters.\n",
        "    validation_params = config.validation_protocols['response_validation']\n",
        "    min_words = validation_params['min_length_words']\n",
        "    max_words = validation_params['max_length_words']\n",
        "\n",
        "    # 1. Check for generation failure or empty response.\n",
        "    if not response_text or not response_text.strip():\n",
        "        logging.warning(\"Validation failed: Response is null or empty.\")\n",
        "        return \"\"\n",
        "\n",
        "    # 2. Validate word count.\n",
        "    word_count = len(response_text.split())\n",
        "    if not (min_words <= word_count <= max_words):\n",
        "        logging.warning(\n",
        "            f\"Validation failed: Word count {word_count} is outside the \"\n",
        "            f\"allowed range [{min_words}, {max_words}]. Response: '{response_text[:100]}...'\"\n",
        "        )\n",
        "        return \"\"\n",
        "\n",
        "    # 3. Validate UTF-8 encoding.\n",
        "    try:\n",
        "        response_text.encode('utf-8').decode('utf-8')\n",
        "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "        logging.warning(f\"Validation failed: Response contains invalid UTF-8 characters.\")\n",
        "        return \"\"\n",
        "\n",
        "    # 4. Check for obvious truncation (heuristic).\n",
        "    if not response_text.strip().endswith(('.', '!', '?', '\"', '}', ']')):\n",
        "         logging.debug(f\"Response does not end with standard punctuation: '{response_text[-50:]}'\")\n",
        "\n",
        "    # If all checks pass, return the original response text.\n",
        "    return response_text\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "async def _generate_responses_async(\n",
        "    validated_paraphrases: List[str],\n",
        "    config: FusedExperimentInputModel,\n",
        "    runtime_env: SDMRuntimeEnvironment\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Asynchronous orchestrator for generating all responses concurrently.\n",
        "\n",
        "    Args:\n",
        "        validated_paraphrases: The list of M validated paraphrases.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "        runtime_env: The dataclass containing initialized models and clients.\n",
        "\n",
        "    Returns:\n",
        "        A flat list of M*N raw response strings, preserving order.\n",
        "    \"\"\"\n",
        "    # Initialize an asynchronous OpenAI client.\n",
        "    async_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Create a semaphore to limit concurrency, e.g., to 10 simultaneous requests.\n",
        "    semaphore = asyncio.Semaphore(10)\n",
        "\n",
        "    # Retrieve the number of responses to generate per paraphrase (N).\n",
        "    num_answers_n = config.hyperparameters.corpus_generation['num_answers_per_paraphrase_N']\n",
        "\n",
        "    # Create a list of all asynchronous tasks to be executed.\n",
        "    tasks = []\n",
        "    # Iterate through each of the M paraphrases.\n",
        "    for paraphrase in validated_paraphrases:\n",
        "        # For each paraphrase, create N response generation tasks.\n",
        "        for _ in range(num_answers_n):\n",
        "            # Prepare the request payload for this specific response.\n",
        "            request_params = _prepare_response_request(paraphrase, config)\n",
        "            # Create an asyncio task for the API call and append it to the list.\n",
        "            task = _execute_single_response_generation(async_client, request_params, semaphore)\n",
        "            tasks.append(task)\n",
        "\n",
        "    # Execute all tasks concurrently and wait for them to complete.\n",
        "    # The tqdm_asyncio wrapper provides a progress bar for the long-running operation.\n",
        "    logging.info(f\"Executing {len(tasks)} response generation API calls concurrently...\")\n",
        "    raw_responses = await tqdm_asyncio.gather(*tasks)\n",
        "\n",
        "    # Return the flat list of raw responses. The order is preserved by asyncio.gather.\n",
        "    return raw_responses\n",
        "\n",
        "def generate_validated_responses(\n",
        "    validated_paraphrases: List[str],\n",
        "    config: FusedExperimentInputModel,\n",
        "    runtime_env: SDMRuntimeEnvironment\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end process of generating and validating the response matrix.\n",
        "\n",
        "    This function manages the entire workflow for Task 4. It uses an asynchronous\n",
        "    helper to generate all M*N responses efficiently, then validates each one,\n",
        "    and finally structures the results into a final M x N NumPy array.\n",
        "\n",
        "    Args:\n",
        "        validated_paraphrases: The list of M validated paraphrases from Task 3.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "        runtime_env: The dataclass containing initialized models and clients.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of strings with shape (M, N) containing the validated\n",
        "        responses. Failed responses are represented as empty strings.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 4 process.\n",
        "    logging.info(\"--- Starting Task 4: Response Generation and Validation ---\")\n",
        "\n",
        "    # --- Step 4.2: Systematic and Asynchronous Response Generation ---\n",
        "    # Run the asynchronous generation process to get a flat list of responses.\n",
        "    flat_raw_responses = asyncio.run(\n",
        "        _generate_responses_async(validated_paraphrases, config, runtime_env)\n",
        "    )\n",
        "\n",
        "    # --- Step 4.3: Response Validation and Quality Control ---\n",
        "    # Validate each raw response. The result is a flat list of clean responses or empty strings.\n",
        "    logging.info(\"Validating all generated responses...\")\n",
        "    flat_validated_responses = [\n",
        "        _validate_single_response(resp, config) for resp in flat_raw_responses\n",
        "    ]\n",
        "\n",
        "    # --- Step 4.4: Metadata Compilation and Structuring ---\n",
        "    # Reshape the flat list of validated responses into the desired M x N matrix.\n",
        "    num_paraphrases_m = config.hyperparameters.corpus_generation['num_paraphrases_M']\n",
        "    num_answers_n = config.hyperparameters.corpus_generation['num_answers_per_paraphrase_N']\n",
        "\n",
        "    # Convert the list to a NumPy array and reshape it.\n",
        "    response_matrix = np.array(flat_validated_responses).reshape((num_paraphrases_m, num_answers_n))\n",
        "\n",
        "    # Final validation of the output shape.\n",
        "    expected_shape = (num_paraphrases_m, num_answers_n)\n",
        "    if response_matrix.shape != expected_shape:\n",
        "        # This should not happen if the logic is correct, but it's a critical sanity check.\n",
        "        raise RuntimeError(\n",
        "            f\"Final response matrix shape mismatch. \"\n",
        "            f\"Expected: {expected_shape}, Got: {response_matrix.shape}\"\n",
        "        )\n",
        "\n",
        "    # Count how many responses failed validation for the final report.\n",
        "    num_failed = np.sum(response_matrix == \"\")\n",
        "    if num_failed > 0:\n",
        "        logging.warning(f\"{num_failed}/{response_matrix.size} responses failed validation and were replaced with empty strings.\")\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(f\"--- Task 4 Successfully Completed: Validated {response_matrix.shape} response matrix generated. ---\")\n",
        "\n",
        "    # Return the final, structured, and validated response matrix.\n",
        "    return response_matrix\n"
      ],
      "metadata": {
        "id": "VAvXwN_NSgPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Text Processing and Sentence Segmentation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Text Processing and Sentence Segmentation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module deconstructs the document-level prompts and responses into their\n",
        "# fundamental analytical unit: the sentence. It employs a systematic,\n",
        "# DataFrame-centric approach to ensure that every sentence is not only\n",
        "# extracted but also meticulously cataloged with its source metadata. This\n",
        "# process includes robust validation to filter out segmentation artifacts,\n",
        "# ensuring a high-quality corpus for the subsequent embedding and clustering stages.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Segmented Corpus\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SegmentedCorpus:\n",
        "    \"\"\"\n",
        "    A container for the results of the sentence segmentation and validation process.\n",
        "\n",
        "    This dataclass provides a structured and type-safe way to pass the sentence\n",
        "    corpus and its associated metadata to subsequent pipeline stages.\n",
        "\n",
        "    Attributes:\n",
        "        prompt_sentences (List[str]): A list of validated sentence strings from the prompts.\n",
        "        answer_sentences (List[str]): A list of validated sentence strings from the answers.\n",
        "        prompts_metadata_df (pd.DataFrame): A DataFrame containing metadata for each prompt sentence.\n",
        "        answers_metadata_df (pd.DataFrame): A DataFrame containing metadata for each answer sentence.\n",
        "        joint_metadata_df (pd.DataFrame): A DataFrame containing all sentences and metadata,\n",
        "                                          preserving the original joint order.\n",
        "    \"\"\"\n",
        "    # A clean list of all sentences extracted from the M paraphrased prompts.\n",
        "    prompt_sentences: List[str]\n",
        "    # A clean list of all sentences extracted from the M*N generated responses.\n",
        "    answer_sentences: List[str]\n",
        "    # A DataFrame holding metadata for each sentence in `prompt_sentences`.\n",
        "    prompts_metadata_df: pd.DataFrame\n",
        "    # A DataFrame holding metadata for each sentence in `answer_sentences`.\n",
        "    answers_metadata_df: pd.DataFrame\n",
        "    # A comprehensive DataFrame containing all sentences and their metadata.\n",
        "    joint_metadata_df: pd.DataFrame\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 5.1: Corpus Preparation and Segmentation\n",
        "# ==============================================================================\n",
        "\n",
        "def _segment_and_catalog_sentences(\n",
        "    validated_paraphrases: List[str],\n",
        "    validated_responses: np.ndarray\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Segments all prompts and responses into sentences and catalogs them in a DataFrame.\n",
        "\n",
        "    This function creates a unified, \"long-form\" DataFrame where each row\n",
        "    represents a single sentence, meticulously tracking its origin (prompt or\n",
        "    answer, and its specific index).\n",
        "\n",
        "    Args:\n",
        "        validated_paraphrases: The list of M validated paraphrases.\n",
        "        validated_responses: The (M, N) NumPy array of validated responses.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing all sentences and their source metadata.\n",
        "    \"\"\"\n",
        "    # A list to hold records for each sentence before creating the DataFrame.\n",
        "    sentence_records = []\n",
        "\n",
        "    # --- Process Prompts (Paraphrases) ---\n",
        "    # Iterate through each paraphrase with its index (0 to M-1).\n",
        "    for m, paraphrase_text in enumerate(validated_paraphrases):\n",
        "        # Ensure the text is not empty before attempting segmentation.\n",
        "        if paraphrase_text and paraphrase_text.strip():\n",
        "            # Use NLTK to segment the text into sentences.\n",
        "            sentences = nltk.sent_tokenize(paraphrase_text)\n",
        "            # For each extracted sentence, create a metadata record.\n",
        "            for sent in sentences:\n",
        "                sentence_records.append({\n",
        "                    \"sentence_text\": sent,\n",
        "                    \"source_type\": \"prompt\",\n",
        "                    \"paraphrase_idx\": m,\n",
        "                    \"response_idx\": -1  # Use -1 to indicate no response index.\n",
        "                })\n",
        "\n",
        "    # --- Process Responses ---\n",
        "    # Get the dimensions of the response matrix (M, N).\n",
        "    num_paraphrases_m, num_answers_n = validated_responses.shape\n",
        "    # Iterate through each paraphrase index (m).\n",
        "    for m in range(num_paraphrases_m):\n",
        "        # Iterate through each response index (n) for the current paraphrase.\n",
        "        for n in range(num_answers_n):\n",
        "            # Get the response text from the matrix.\n",
        "            response_text = validated_responses[m, n]\n",
        "            # Ensure the text is not empty (failed responses are empty strings).\n",
        "            if response_text and response_text.strip():\n",
        "                # Segment the response text into sentences.\n",
        "                sentences = nltk.sent_tokenize(response_text)\n",
        "                # For each extracted sentence, create a metadata record.\n",
        "                for sent in sentences:\n",
        "                    sentence_records.append({\n",
        "                        \"sentence_text\": sent,\n",
        "                        \"source_type\": \"answer\",\n",
        "                        \"paraphrase_idx\": m,\n",
        "                        \"response_idx\": n\n",
        "                    })\n",
        "\n",
        "    # If no sentences were generated at all, return an empty DataFrame.\n",
        "    if not sentence_records:\n",
        "        logging.warning(\"No sentences were extracted from the provided texts.\")\n",
        "        return pd.DataFrame(columns=[\"sentence_text\", \"source_type\", \"paraphrase_idx\", \"response_idx\"])\n",
        "\n",
        "    # Convert the list of records into a pandas DataFrame.\n",
        "    joint_df = pd.DataFrame(sentence_records)\n",
        "\n",
        "    # Return the comprehensive DataFrame.\n",
        "    return joint_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 5.2: Sentence Validation and Quality Control\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_and_filter_sentences(\n",
        "    joint_df: pd.DataFrame,\n",
        "    min_words: int = 3,\n",
        "    max_words: int = 150\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates and filters the sentence DataFrame based on quality criteria.\n",
        "\n",
        "    This function cleans sentence text by stripping whitespace and filters out\n",
        "    sentences that are likely segmentation artifacts or overly long, based on\n",
        "    word count.\n",
        "\n",
        "    Args:\n",
        "        joint_df: The DataFrame containing all segmented sentences.\n",
        "        min_words: The minimum number of words a sentence must have to be retained.\n",
        "        max_words: The maximum number of words a sentence can have to be retained.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only valid, high-quality sentences.\n",
        "    \"\"\"\n",
        "    # If the input DataFrame is empty, return it immediately.\n",
        "    if joint_df.empty:\n",
        "        return joint_df\n",
        "\n",
        "    # Create a copy to avoid SettingWithCopyWarning and preserve the original.\n",
        "    df = joint_df.copy()\n",
        "\n",
        "    # 1. Clean the sentence text by stripping leading/trailing whitespace.\n",
        "    df['sentence_text'] = df['sentence_text'].str.strip()\n",
        "\n",
        "    # 2. Calculate the word count for each sentence.\n",
        "    df['word_count'] = df['sentence_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "    # 3. Filter sentences based on word count.\n",
        "    # Keep only sentences where the word count is within the specified range.\n",
        "    initial_count = len(df)\n",
        "    filtered_df = df[df['word_count'].between(min_words, max_words)].copy()\n",
        "    final_count = len(filtered_df)\n",
        "\n",
        "    # Log the number of sentences that were dropped during filtering.\n",
        "    num_dropped = initial_count - final_count\n",
        "    if num_dropped > 0:\n",
        "        logging.info(\n",
        "            f\"Filtered out {num_dropped} sentences based on word count \"\n",
        "            f\"(min: {min_words}, max: {max_words}). Retained {final_count} sentences.\"\n",
        "        )\n",
        "\n",
        "    # Drop the temporary 'word_count' column as it's no longer needed.\n",
        "    filtered_df.drop(columns=['word_count'], inplace=True)\n",
        "\n",
        "    # Return the validated and filtered DataFrame.\n",
        "    return filtered_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def segment_and_validate_corpus(\n",
        "    validated_paraphrases: List[str],\n",
        "    validated_responses: np.ndarray,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> SegmentedCorpus:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end sentence segmentation and validation process.\n",
        "\n",
        "    This function manages the entire workflow for Task 5. It deconstructs the\n",
        "    prompt and response documents into sentences, catalogs them with rich\n",
        "    metadata in a DataFrame, validates them for quality, and finally organizes\n",
        "    them into a structured corpus object for downstream tasks.\n",
        "\n",
        "    Args:\n",
        "        validated_paraphrases: The list of M validated paraphrases from Task 3.\n",
        "        validated_responses: The (M, N) NumPy array of validated responses from Task 4.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        SegmentedCorpus: A dataclass instance containing the lists of sentences\n",
        "                         and their corresponding metadata DataFrames.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the process results in an empty corpus of sentences.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 5 process.\n",
        "    logging.info(\"--- Starting Task 5: Text Processing and Sentence Segmentation ---\")\n",
        "\n",
        "    # Step 5.1: Segment all texts and catalog them into a single DataFrame.\n",
        "    joint_metadata_df = _segment_and_catalog_sentences(\n",
        "        validated_paraphrases, validated_responses\n",
        "    )\n",
        "\n",
        "    # Step 5.2: Apply validation and filtering rules to the sentences.\n",
        "    validated_joint_df = _validate_and_filter_sentences(joint_metadata_df)\n",
        "\n",
        "    # Check if the corpus is empty after filtering.\n",
        "    if validated_joint_df.empty:\n",
        "        error_msg = \"The sentence corpus is empty after validation and filtering. Cannot proceed.\"\n",
        "        logging.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Step 5.3: Final Corpus Organization.\n",
        "    # Split the validated DataFrame into prompt and answer components.\n",
        "    prompts_metadata_df = validated_joint_df[\n",
        "        validated_joint_df['source_type'] == 'prompt'\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "    answers_metadata_df = validated_joint_df[\n",
        "        validated_joint_df['source_type'] == 'answer'\n",
        "    ].reset_index(drop=True)\n",
        "\n",
        "    # Extract the final, clean lists of sentence strings for the embedding model.\n",
        "    prompt_sentences = prompts_metadata_df['sentence_text'].tolist()\n",
        "    answer_sentences = answers_metadata_df['sentence_text'].tolist()\n",
        "\n",
        "    # Log the final counts of sentences.\n",
        "    logging.info(\n",
        "        f\"Segmentation complete. \"\n",
        "        f\"Generated {len(prompt_sentences)} prompt sentences and \"\n",
        "        f\"{len(answer_sentences)} answer sentences.\"\n",
        "    )\n",
        "\n",
        "    # Assemble the final results into the SegmentedCorpus dataclass.\n",
        "    corpus = SegmentedCorpus(\n",
        "        prompt_sentences=prompt_sentences,\n",
        "        answer_sentences=answer_sentences,\n",
        "        prompts_metadata_df=prompts_metadata_df,\n",
        "        answers_metadata_df=answers_metadata_df,\n",
        "        joint_metadata_df=validated_joint_df.reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 5 Successfully Completed: Validated sentence corpus created. ---\")\n",
        "\n",
        "    # Return the structured corpus object.\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "Gf9oR-MeTXo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Embedding Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Embedding Generation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module transforms the textual sentence corpus into a high-dimensional\n",
        "# vector space. It leverages a pre-trained sentence-transformer model to generate\n",
        "# dense embeddings for each sentence. The process is optimized for performance\n",
        "# via batching and includes a rigorous, multi-stage validation protocol to\n",
        "# ensure the numerical integrity and quality of the resulting embeddings, which\n",
        "# are the foundation for all subsequent quantitative analysis.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Embedded Corpus\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EmbeddedCorpus:\n",
        "    \"\"\"\n",
        "    A container for the results of the embedding generation and validation process.\n",
        "\n",
        "    This dataclass holds the numerical representations of the sentence corpus,\n",
        "    organized for downstream analysis. It maintains a clear separation between\n",
        "    prompt and answer embeddings while also providing a joint matrix for\n",
        "    clustering.\n",
        "\n",
        "    Attributes:\n",
        "        prompt_embeddings (np.ndarray): A matrix of shape (S_P, D) containing\n",
        "                                        embeddings for all prompt sentences.\n",
        "        answer_embeddings (np.ndarray): A matrix of shape (S_A, D) containing\n",
        "                                        embeddings for all answer sentences.\n",
        "        joint_embeddings (np.ndarray): A combined matrix of shape (S_P + S_A, D)\n",
        "                                       used for joint clustering.\n",
        "        prompt_sentence_count (int): The number of prompt sentences (S_P),\n",
        "                                     serving as the split index for the joint matrix.\n",
        "    \"\"\"\n",
        "    # A NumPy array where each row is the embedding of a prompt sentence.\n",
        "    prompt_embeddings: np.ndarray\n",
        "    # A NumPy array where each row is the embedding of an answer sentence.\n",
        "    answer_embeddings: np.ndarray\n",
        "    # A vertically stacked combination of prompt and answer embeddings.\n",
        "    joint_embeddings: np.ndarray\n",
        "    # The number of prompt sentences, S_P. This is the index at which answer embeddings begin in the joint matrix.\n",
        "    prompt_sentence_count: int\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 6.1: Systematic Embedding Generation\n",
        "# ==============================================================================\n",
        "\n",
        "def _generate_embeddings(\n",
        "    sentences: List[str],\n",
        "    embedding_model: SentenceTransformer,\n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates dense vector embeddings for a list of sentences using a transformer model.\n",
        "\n",
        "    Args:\n",
        "        sentences: A list of sentence strings to be embedded.\n",
        "        embedding_model: The initialized SentenceTransformer model.\n",
        "        batch_size: The number of sentences to process in a single batch for efficiency.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of shape (num_sentences, embedding_dimension) containing the embeddings.\n",
        "    \"\"\"\n",
        "    # Log the start of the embedding process for the given corpus.\n",
        "    logging.info(f\"Generating embeddings for {len(sentences)} sentences...\")\n",
        "\n",
        "    # Check if the input list is empty to prevent errors.\n",
        "    if not sentences:\n",
        "        # Return an empty array with the correct number of columns (dimension).\n",
        "        embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
        "        return np.empty((0, embedding_dim), dtype=np.float32)\n",
        "\n",
        "    # Use the model's encode method, which is highly optimized for this task.\n",
        "    # - batch_size: Controls how many sentences are processed at once, crucial for GPU performance.\n",
        "    # - show_progress_bar: Provides user feedback during this potentially long process.\n",
        "    # - convert_to_numpy: Ensures the output is a NumPy array as required.\n",
        "    embeddings = embedding_model.encode(\n",
        "        sentences,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the embedding generation.\n",
        "    logging.info(f\"Successfully generated {embeddings.shape[0]} embeddings with dimension {embeddings.shape[1]}.\")\n",
        "\n",
        "    # Return the resulting matrix of embeddings.\n",
        "    return embeddings\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 6.2: Embedding Quality Control and Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    expected_dim: int,\n",
        "    magnitude_range: Tuple[float, float],\n",
        "    corpus_name: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs a rigorous set of validations on a matrix of embeddings.\n",
        "\n",
        "    This function checks for correct dimensionality, numerical stability (no NaNs\n",
        "    or infinities), and reasonable vector magnitudes.\n",
        "\n",
        "    Args:\n",
        "        embeddings: The NumPy array of embeddings to validate.\n",
        "        expected_dim: The expected embedding dimension (e.g., 768).\n",
        "        magnitude_range: A tuple (min, max) for the valid L2 norm of an embedding vector.\n",
        "        corpus_name: A string identifier for the corpus being validated (e.g., \"Prompt\").\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the validation checks fail.\n",
        "    \"\"\"\n",
        "    # Log the start of the validation for the specified corpus.\n",
        "    logging.info(f\"Performing quality validation on {corpus_name} embeddings...\")\n",
        "\n",
        "    # If the embeddings array is empty, validation is trivially successful.\n",
        "    if embeddings.shape[0] == 0:\n",
        "        logging.info(f\"{corpus_name} embedding set is empty, validation skipped.\")\n",
        "        return\n",
        "\n",
        "    # 1. Dimensionality Check: Verify the number of columns matches the expected dimension.\n",
        "    if embeddings.shape[1] != expected_dim:\n",
        "        raise ValueError(\n",
        "            f\"[{corpus_name}] Dimensionality mismatch. \"\n",
        "            f\"Expected: {expected_dim}, Got: {embeddings.shape[1]}.\"\n",
        "        )\n",
        "\n",
        "    # 2. NaN Check: Ensure there are no 'Not a Number' values in the matrix.\n",
        "    if np.isnan(embeddings).any():\n",
        "        raise ValueError(f\"[{corpus_name}] NaN values detected in embeddings.\")\n",
        "\n",
        "    # 3. Infinity Check: Ensure there are no infinite values in the matrix.\n",
        "    if np.isinf(embeddings).any():\n",
        "        raise ValueError(f\"[{corpus_name}] Infinite values detected in embeddings.\")\n",
        "\n",
        "    # 4. Magnitude Check: Calculate the L2 norm (Euclidean length) for each vector.\n",
        "    magnitudes = norm(embeddings, axis=1)\n",
        "    min_mag, max_mag = magnitude_range\n",
        "\n",
        "    # Check if any vector's magnitude is outside the acceptable range.\n",
        "    if not np.all((magnitudes >= min_mag) & (magnitudes <= max_mag)):\n",
        "        # Find the number of outlier magnitudes for a more informative error.\n",
        "        outliers = np.sum((magnitudes < min_mag) | (magnitudes > max_mag))\n",
        "        raise ValueError(\n",
        "            f\"[{corpus_name}] {outliers} embedding(s) have magnitudes outside the \"\n",
        "            f\"valid range [{min_mag}, {max_mag}].\"\n",
        "        )\n",
        "\n",
        "    # Log the successful validation.\n",
        "    logging.info(f\"[{corpus_name}] All {embeddings.shape[0]} embeddings passed validation.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_and_validate_embeddings(\n",
        "    corpus: SegmentedCorpus,\n",
        "    runtime_env: SDMRuntimeEnvironment,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> EmbeddedCorpus:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end process of embedding and validating the sentence corpus.\n",
        "\n",
        "    This function manages the workflow for Task 6. It generates embeddings for\n",
        "    both prompt and answer sentences, subjects them to rigorous quality control,\n",
        "    and constructs the final joint embedding matrix required for clustering.\n",
        "\n",
        "    Args:\n",
        "        corpus: The SegmentedCorpus object from Task 5.\n",
        "        runtime_env: The dataclass containing the initialized embedding model.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        EmbeddedCorpus: A dataclass instance containing the validated embedding matrices.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 6 process.\n",
        "    logging.info(\"--- Starting Task 6: Embedding Generation and Validation ---\")\n",
        "\n",
        "    # Retrieve the embedding model from the runtime environment.\n",
        "    embedding_model = runtime_env.embedding_model\n",
        "    # Retrieve the validation parameters from the configuration.\n",
        "    validation_params = config.validation_protocols['embedding_validation']\n",
        "    expected_dim = config.system_components.sentence_embedding_model['embedding_dimension']\n",
        "    magnitude_range = tuple(validation_params['magnitude_threshold'])\n",
        "\n",
        "    # --- Step 6.1: Generate Embeddings ---\n",
        "    # Generate embeddings for the prompt sentences.\n",
        "    prompt_embeddings = _generate_embeddings(corpus.prompt_sentences, embedding_model)\n",
        "    # Generate embeddings for the answer sentences.\n",
        "    answer_embeddings = _generate_embeddings(corpus.answer_sentences, embedding_model)\n",
        "\n",
        "    # --- Step 6.2: Validate Embeddings ---\n",
        "    # Perform rigorous quality control on the prompt embeddings.\n",
        "    _validate_embeddings(prompt_embeddings, expected_dim, magnitude_range, \"Prompt\")\n",
        "    # Perform the same quality control on the answer embeddings.\n",
        "    _validate_embeddings(answer_embeddings, expected_dim, magnitude_range, \"Answer\")\n",
        "\n",
        "    # --- Step 6.3: Construct Joint Embedding Matrix ---\n",
        "    # Vertically stack the prompt and answer embeddings to create the joint matrix.\n",
        "    # This matrix is the primary input for the joint clustering algorithm.\n",
        "    logging.info(\"Constructing joint embedding matrix for clustering...\")\n",
        "    joint_embeddings = np.vstack((prompt_embeddings, answer_embeddings))\n",
        "\n",
        "    # Get the number of prompt sentences, which serves as the critical split index.\n",
        "    prompt_sentence_count = prompt_embeddings.shape[0]\n",
        "\n",
        "    # Assemble the final results into the EmbeddedCorpus dataclass.\n",
        "    embedded_corpus = EmbeddedCorpus(\n",
        "        prompt_embeddings=prompt_embeddings,\n",
        "        answer_embeddings=answer_embeddings,\n",
        "        joint_embeddings=joint_embeddings,\n",
        "        prompt_sentence_count=prompt_sentence_count\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 6 Successfully Completed: Validated embedding corpus created. ---\")\n",
        "\n",
        "    # Return the structured object containing all embedding matrices.\n",
        "    return embedded_corpus\n"
      ],
      "metadata": {
        "id": "nJ6EInL6UUOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Clustering and Topic Identification\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Clustering and Topic Identification\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module performs the core semantic topic identification. It partitions the\n",
        "# high-dimensional embedding space into a discrete set of clusters, where each\n",
        "# cluster represents a semantic topic. The process adheres strictly to the\n",
        "# paper's two-stage methodology: first, determining the optimal number of\n",
        "# clusters (k*) using the K-Means elbow method, and second, performing the\n",
        "# final partitioning using Hierarchical Agglomerative Clustering with Ward's\n",
        "# linkage. Rigorous quality validation is performed on the resulting clusters.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Clustering Results\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ClusteringResult:\n",
        "    \"\"\"\n",
        "    A container for the results of the clustering and validation process.\n",
        "\n",
        "    This dataclass holds the discrete topic assignments (cluster labels) for\n",
        "    each sentence, organized for the subsequent probability distribution\n",
        "    construction.\n",
        "\n",
        "    Attributes:\n",
        "        optimal_k (int): The optimal number of clusters (k*) determined by the elbow method.\n",
        "        prompt_labels (np.ndarray): An array of shape (S_P,) with cluster labels for prompt sentences.\n",
        "        answer_labels (np.ndarray): An array of shape (S_A,) with cluster labels for answer sentences.\n",
        "        joint_labels (np.ndarray): An array of shape (S_P + S_A,) with labels for all sentences.\n",
        "        silhouette_score (float): The silhouette score, a measure of clustering quality.\n",
        "    \"\"\"\n",
        "    # The optimal number of semantic topics (clusters) identified.\n",
        "    optimal_k: int\n",
        "    # The array of cluster assignments for each of the S_P prompt sentences.\n",
        "    prompt_labels: np.ndarray\n",
        "    # The array of cluster assignments for each of the S_A answer sentences.\n",
        "    answer_labels: np.ndarray\n",
        "    # The full array of cluster assignments for the joint corpus.\n",
        "    joint_labels: np.ndarray\n",
        "    # The calculated silhouette score for the final clustering solution.\n",
        "    silhouette_score: float\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 7.1: Optimal Cluster Number Determination\n",
        "# ==============================================================================\n",
        "\n",
        "def _determine_optimal_k(\n",
        "    embeddings: np.ndarray,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Determines the optimal number of clusters (k*) using the K-Means elbow method.\n",
        "\n",
        "    This function iterates through a range of k values, fits a K-Means model for\n",
        "    each, and identifies the \"elbow point\" in the inertia curve, which represents\n",
        "    the best trade-off between cluster cohesion and the number of clusters.\n",
        "\n",
        "    Args:\n",
        "        embeddings: The joint embedding matrix of shape (S, D).\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        The optimal integer value for k.\n",
        "    \"\"\"\n",
        "    # Retrieve clustering parameters from the configuration.\n",
        "    cluster_config = config.hyperparameters.clustering\n",
        "    k_min = cluster_config['k_range_min']\n",
        "    k_max = cluster_config['k_range_max']\n",
        "    random_state = config.hyperparameters.reproducibility['sklearn_random_state']\n",
        "\n",
        "    # Log the start of the process.\n",
        "    logging.info(f\"Determining optimal k using elbow method for k in [{k_min}, {k_max}]...\")\n",
        "\n",
        "    # A list to store the inertia (within-cluster sum of squares) for each k.\n",
        "    inertias = []\n",
        "    # Define the range of k values to test.\n",
        "    k_range = range(k_min, k_max + 1)\n",
        "\n",
        "    # Iterate through each possible number of clusters.\n",
        "    for k in k_range:\n",
        "        # Initialize and fit the K-Means model.\n",
        "        # `n_init='auto'` is the modern default for handling multiple initializations.\n",
        "        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init='auto')\n",
        "        kmeans.fit(embeddings)\n",
        "        # Append the model's inertia to our list.\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    # --- Geometric Elbow Detection ---\n",
        "    # We find the elbow by identifying the point with the maximum distance\n",
        "    # to the line connecting the first and last points of the inertia curve.\n",
        "    points = np.array([list(k_range), inertias]).T\n",
        "    # Define the first and last points of the line.\n",
        "    p1 = points[0]\n",
        "    p_last = points[-1]\n",
        "    # Calculate the distances of all points from the line segment.\n",
        "    line_vec = p_last - p1\n",
        "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
        "    vec_from_p1 = points - p1\n",
        "    scalar_product = np.sum(vec_from_p1 * np.tile(line_vec_norm, (len(points), 1)), axis=1)\n",
        "    vec_from_p1_parallel = np.outer(scalar_product, line_vec_norm)\n",
        "    vec_to_line = vec_from_p1 - vec_from_p1_parallel\n",
        "    dist_to_line = np.sqrt(np.sum(vec_to_line ** 2, axis=1))\n",
        "\n",
        "    # The optimal k is the one corresponding to the maximum distance.\n",
        "    optimal_k_index = np.argmax(dist_to_line)\n",
        "    optimal_k = k_range[optimal_k_index]\n",
        "\n",
        "    # Log the result.\n",
        "    logging.info(f\"Elbow method identified optimal k = {optimal_k}.\")\n",
        "\n",
        "    # Return the determined optimal number of clusters.\n",
        "    return optimal_k\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 7.2: Hierarchical Clustering Execution\n",
        "# ==============================================================================\n",
        "\n",
        "def _perform_hierarchical_clustering(\n",
        "    embeddings: np.ndarray,\n",
        "    optimal_k: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs Hierarchical Agglomerative Clustering with Ward's linkage.\n",
        "\n",
        "    Args:\n",
        "        embeddings: The joint embedding matrix of shape (S, D).\n",
        "        optimal_k: The desired number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of shape (S,) containing the cluster label for each sentence.\n",
        "    \"\"\"\n",
        "    # Log the start of the clustering process with the chosen parameters.\n",
        "    logging.info(f\"Performing Agglomerative Clustering with k={optimal_k} and Ward's linkage...\")\n",
        "\n",
        "    # Initialize the AgglomerativeClustering model.\n",
        "    # - n_clusters: The number of topics to find, determined by the elbow method.\n",
        "    # - linkage='ward': The method specified by the paper, which minimizes\n",
        "    #   the variance of the clusters being merged.\n",
        "    hac = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
        "\n",
        "    # Fit the model to the data and predict the cluster for each embedding.\n",
        "    cluster_labels = hac.fit_predict(embeddings)\n",
        "\n",
        "    # Log the successful completion of the clustering.\n",
        "    logging.info(\"Hierarchical clustering completed.\")\n",
        "\n",
        "    # Return the array of cluster labels.\n",
        "    return cluster_labels\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 7.3: Clustering Quality Validation and Label Separation\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_and_separate_clusters(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    prompt_sentence_count: int,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Validates the quality of the clustering and separates labels into prompt/answer groups.\n",
        "\n",
        "    Args:\n",
        "        embeddings: The joint embedding matrix used for clustering.\n",
        "        labels: The array of cluster labels for the joint corpus.\n",
        "        prompt_sentence_count: The number of prompt sentences (S_P).\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The calculated silhouette score.\n",
        "        - The NumPy array of prompt labels.\n",
        "        - The NumPy array of answer labels.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the clustering quality fails to meet validation thresholds.\n",
        "    \"\"\"\n",
        "    # Retrieve validation parameters from the configuration.\n",
        "    validation_params = config.validation_protocols['clustering_validation']\n",
        "\n",
        "    # 1. Quality Validation: Silhouette Score\n",
        "    score = silhouette_score(embeddings, labels)\n",
        "    logging.info(f\"Clustering Silhouette Score: {score:.4f}\")\n",
        "    if score < validation_params['silhouette_score_threshold']:\n",
        "        raise ValueError(\n",
        "            f\"Clustering quality is poor. Silhouette score {score:.4f} is below \"\n",
        "            f\"threshold {validation_params['silhouette_score_threshold']}.\"\n",
        "        )\n",
        "\n",
        "    # 2. Quality Validation: Cluster Balance\n",
        "    label_counts = pd.Series(labels).value_counts()\n",
        "    if label_counts.min() < validation_params['min_cluster_size']:\n",
        "        raise ValueError(\n",
        "            f\"Clustering failed balance check: smallest cluster has size {label_counts.min()}, \"\n",
        "            f\"below threshold {validation_params['min_cluster_size']}.\"\n",
        "        )\n",
        "    if (label_counts.max() / len(labels)) > validation_params['max_cluster_ratio']:\n",
        "        raise ValueError(\n",
        "            f\"Clustering failed balance check: largest cluster contains \"\n",
        "            f\"{100 * label_counts.max() / len(labels):.1f}% of data, \"\n",
        "            f\"exceeding threshold {100 * validation_params['max_cluster_ratio']:.1f}%.\"\n",
        "        )\n",
        "\n",
        "    # 3. Label Separation\n",
        "    # Use the prompt sentence count (S_P) to slice the joint labels array.\n",
        "    prompt_labels = labels[:prompt_sentence_count]\n",
        "    answer_labels = labels[prompt_sentence_count:]\n",
        "\n",
        "    # Sanity check the lengths of the separated arrays.\n",
        "    if len(prompt_labels) != prompt_sentence_count:\n",
        "         raise ValueError(\"Mismatch in prompt label count after separation.\")\n",
        "    if len(answer_labels) != (len(embeddings) - prompt_sentence_count):\n",
        "         raise ValueError(\"Mismatch in answer label count after separation.\")\n",
        "\n",
        "    logging.info(\"Clustering validation passed and labels separated successfully.\")\n",
        "\n",
        "    return score, prompt_labels, answer_labels\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def perform_joint_clustering(\n",
        "    embedded_corpus: EmbeddedCorpus,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> ClusteringResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end topic identification and clustering process.\n",
        "\n",
        "    This function manages the workflow for Task 7. It determines the optimal\n",
        "    number of clusters, performs hierarchical clustering as specified in the\n",
        "    paper, validates the quality of the result, and separates the final cluster\n",
        "    labels for prompts and answers.\n",
        "\n",
        "    Args:\n",
        "        embedded_corpus: The EmbeddedCorpus object from Task 6.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        ClusteringResult: A dataclass instance containing the cluster labels\n",
        "                          and validation metrics.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 7 process.\n",
        "    logging.info(\"--- Starting Task 7: Clustering and Topic Identification ---\")\n",
        "\n",
        "    # Extract the joint embeddings matrix, which is the primary input for this task.\n",
        "    joint_embeddings = embedded_corpus.joint_embeddings\n",
        "\n",
        "    # Step 7.1: Determine the optimal number of clusters, k*.\n",
        "    optimal_k = _determine_optimal_k(joint_embeddings, config)\n",
        "\n",
        "    # Step 7.2: Perform the final clustering using the determined k*.\n",
        "    joint_labels = _perform_hierarchical_clustering(joint_embeddings, optimal_k)\n",
        "\n",
        "    # Step 7.3: Validate the clustering quality and separate the labels.\n",
        "    silhouette, prompt_labels, answer_labels = _validate_and_separate_clusters(\n",
        "        embeddings=joint_embeddings,\n",
        "        labels=joint_labels,\n",
        "        prompt_sentence_count=embedded_corpus.prompt_sentence_count,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Assemble the final results into the ClusteringResult dataclass.\n",
        "    result = ClusteringResult(\n",
        "        optimal_k=optimal_k,\n",
        "        prompt_labels=prompt_labels,\n",
        "        answer_labels=answer_labels,\n",
        "        joint_labels=joint_labels,\n",
        "        silhouette_score=silhouette\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(f\"--- Task 7 Successfully Completed: Identified {optimal_k} semantic topics. ---\")\n",
        "\n",
        "    # Return the structured result object.\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "qfv1FhY2VHJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Probability Distribution Construction\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Probability Distribution Construction\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module translates the discrete cluster labels into the continuous domain\n",
        "# of probability theory. It constructs the three fundamental types of topic\n",
        "# distributions required for the information-theoretic analysis:\n",
        "# 1. Global Distributions: The overall topic mix for all prompts and all answers.\n",
        "# 2. Local (Ensemble) Distributions: The topic mix for each paraphrase-answer pair.\n",
        "# 3. Averaged Joint Distribution: The average topic co-occurrence structure.\n",
        "# Numerical stability is ensured through epsilon smoothing.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Probability Distributions\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TopicDistributions:\n",
        "    \"\"\"\n",
        "    A container for all constructed topic probability distributions.\n",
        "\n",
        "    This dataclass holds the various probability distributions derived from the\n",
        "    clustering results, organized and ready for the final metric calculations.\n",
        "\n",
        "    Attributes:\n",
        "        global_prompt_dist (np.ndarray): The global topic distribution for prompts (P_global).\n",
        "        global_answer_dist (np.ndarray): The global topic distribution for answers (A_global).\n",
        "        local_dist_pairs (List[Tuple[np.ndarray, np.ndarray]]): A list of (P_m, A_m)\n",
        "            tuples for each of the M paraphrase-answer pairs.\n",
        "        avg_joint_dist (np.ndarray): The (k, k) averaged joint probability matrix, P_avg(X, Y).\n",
        "    \"\"\"\n",
        "    # The (k,) vector representing the overall topic distribution of all prompt sentences.\n",
        "    global_prompt_dist: np.ndarray\n",
        "    # The (k,) vector representing the overall topic distribution of all answer sentences.\n",
        "    global_answer_dist: np.ndarray\n",
        "    # A list of M tuples, where each tuple contains the (prompt_dist, answer_dist) for one paraphrase.\n",
        "    local_dist_pairs: List[Tuple[np.ndarray, np.ndarray]]\n",
        "    # The (k, k) matrix representing the averaged joint probability of topic co-occurrence.\n",
        "    avg_joint_dist: np.ndarray\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 8.1: Global and Local Distribution Calculation\n",
        "# ==============================================================================\n",
        "\n",
        "def _calculate_probability_distribution(\n",
        "    labels: np.ndarray,\n",
        "    num_clusters: int,\n",
        "    epsilon: float = 1e-12\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Calculates a discrete probability distribution from a list of cluster labels.\n",
        "\n",
        "    This function efficiently counts label occurrences and normalizes them to form\n",
        "    a valid probability distribution. It includes epsilon smoothing to prevent\n",
        "    zero probabilities, ensuring numerical stability for downstream log-based metrics.\n",
        "\n",
        "    Args:\n",
        "        labels: A NumPy array of integer cluster labels.\n",
        "        num_clusters: The total number of clusters (k), defining the distribution's length.\n",
        "        epsilon: A small value to add to counts for numerical stability.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of shape (num_clusters,) representing the probability distribution.\n",
        "    \"\"\"\n",
        "    # If the labels array is empty, return a uniform distribution as a neutral default.\n",
        "    if labels.size == 0:\n",
        "        return np.full(num_clusters, 1.0 / num_clusters)\n",
        "\n",
        "    # Use np.bincount to efficiently count occurrences of each label.\n",
        "    # `minlength` ensures the output array has length `num_clusters`, even if some\n",
        "    # higher-indexed clusters are empty.\n",
        "    counts = np.bincount(labels, minlength=num_clusters)\n",
        "\n",
        "    # Apply epsilon smoothing: add a small constant to all counts.\n",
        "    # This prevents zero probabilities, which would cause issues with log calculations.\n",
        "    smoothed_counts = counts + epsilon\n",
        "\n",
        "    # Normalize the smoothed counts to sum to 1.0, forming a valid probability distribution.\n",
        "    # Equation: P(X=i) = (count(i) + epsilon) / (total_counts + k * epsilon)\n",
        "    distribution = smoothed_counts / np.sum(smoothed_counts)\n",
        "\n",
        "    # Return the final probability distribution vector.\n",
        "    return distribution\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 8.2 & 8.3: Ensemble and Joint Distribution Construction\n",
        "# ==============================================================================\n",
        "\n",
        "def _construct_ensemble_and_joint_distributions(\n",
        "    corpus: SegmentedCorpus,\n",
        "    clustering: ClusteringResult,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Tuple[List[Tuple[np.ndarray, np.ndarray]], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs the local (ensemble) distributions and the averaged joint distribution.\n",
        "\n",
        "    This function iterates through each of the M paraphrase-answer pairs to\n",
        "    calculate their individual topic distributions and then combines them to\n",
        "    form the averaged joint probability matrix.\n",
        "\n",
        "    Args:\n",
        "        corpus: The SegmentedCorpus object containing sentence metadata.\n",
        "        clustering: The ClusteringResult object containing cluster labels.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A list of M (P_m, A_m) local distribution pairs.\n",
        "        - The (k, k) averaged joint probability matrix.\n",
        "    \"\"\"\n",
        "    # Retrieve the total number of paraphrases (M) and clusters (k).\n",
        "    num_paraphrases_m = config.hyperparameters.corpus_generation['num_paraphrases_M']\n",
        "    optimal_k = clustering.optimal_k\n",
        "\n",
        "    # Add the cluster labels to the main metadata DataFrame for easy filtering.\n",
        "    # This creates a powerful structure for all subsequent analysis.\n",
        "    metadata_with_labels_df = corpus.joint_metadata_df.copy()\n",
        "    metadata_with_labels_df['cluster_label'] = clustering.joint_labels\n",
        "\n",
        "    # A list to store the local distribution pairs (P_m, A_m).\n",
        "    local_dist_pairs = []\n",
        "    # A list to store the local joint probability matrices P_m(X, Y).\n",
        "    local_joint_matrices = []\n",
        "\n",
        "    # Iterate through each paraphrase index from 0 to M-1.\n",
        "    for m in range(num_paraphrases_m):\n",
        "        # --- Calculate Local Distributions for paraphrase m ---\n",
        "        # Filter to get labels for prompt sentences of the m-th paraphrase.\n",
        "        prompt_labels_m = metadata_with_labels_df[\n",
        "            (metadata_with_labels_df['source_type'] == 'prompt') &\n",
        "            (metadata_with_labels_df['paraphrase_idx'] == m)\n",
        "        ]['cluster_label'].values\n",
        "\n",
        "        # Filter to get labels for answer sentences of the m-th paraphrase.\n",
        "        answer_labels_m = metadata_with_labels_df[\n",
        "            (metadata_with_labels_df['source_type'] == 'answer') &\n",
        "            (metadata_with_labels_df['paraphrase_idx'] == m)\n",
        "        ]['cluster_label'].values\n",
        "\n",
        "        # Calculate the local prompt distribution P_m.\n",
        "        p_m = _calculate_probability_distribution(prompt_labels_m, optimal_k)\n",
        "        # Calculate the local answer distribution A_m.\n",
        "        a_m = _calculate_probability_distribution(answer_labels_m, optimal_k)\n",
        "\n",
        "        # Store the pair of local distributions.\n",
        "        local_dist_pairs.append((p_m, a_m))\n",
        "\n",
        "        # --- Construct Local Joint Probability Matrix ---\n",
        "        # Assuming independence within the pair, the joint distribution is the outer product.\n",
        "        # Equation: P_m(X, Y) = P_m(X) ⊗ A_m(Y)\n",
        "        joint_m = np.outer(p_m, a_m)\n",
        "        local_joint_matrices.append(joint_m)\n",
        "\n",
        "    # --- Construct Averaged Joint Probability Matrix ---\n",
        "    # Average the M local joint matrices element-wise.\n",
        "    # Equation: P_avg(X, Y) = (1/M) * Σ P_m(X, Y)\n",
        "    avg_joint_dist = np.mean(np.array(local_joint_matrices), axis=0)\n",
        "\n",
        "    # Return the collected local distributions and the final averaged joint matrix.\n",
        "    return local_dist_pairs, avg_joint_dist\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def construct_topic_distributions(\n",
        "    corpus: SegmentedCorpus,\n",
        "    clustering: ClusteringResult,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> TopicDistributions:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction of all topic probability distributions.\n",
        "\n",
        "    This function manages the workflow for Task 8. It computes the global,\n",
        "    local (ensemble), and averaged joint distributions from the clustering\n",
        "    results, preparing all necessary probabilistic inputs for the final\n",
        "    information-theoretic and geometric metric calculations.\n",
        "\n",
        "    Args:\n",
        "        corpus: The SegmentedCorpus object containing sentence metadata.\n",
        "        clustering: The ClusteringResult object containing cluster labels.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        TopicDistributions: A dataclass instance containing all calculated\n",
        "                            probability distributions.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 8 process.\n",
        "    logging.info(\"--- Starting Task 8: Probability Distribution Construction ---\")\n",
        "\n",
        "    # Retrieve the optimal number of clusters from the clustering result.\n",
        "    optimal_k = clustering.optimal_k\n",
        "\n",
        "    # --- Step 8.1: Global Distribution Calculation ---\n",
        "    # Calculate the global topic distribution for all prompt sentences.\n",
        "    global_prompt_dist = _calculate_probability_distribution(\n",
        "        clustering.prompt_labels, optimal_k\n",
        "    )\n",
        "    # Calculate the global topic distribution for all answer sentences.\n",
        "    global_answer_dist = _calculate_probability_distribution(\n",
        "        clustering.answer_labels, optimal_k\n",
        "    )\n",
        "    logging.info(\"Successfully calculated global prompt and answer distributions.\")\n",
        "\n",
        "    # --- Step 8.2 & 8.3: Ensemble and Joint Distribution Construction ---\n",
        "    local_dist_pairs, avg_joint_dist = _construct_ensemble_and_joint_distributions(\n",
        "        corpus, clustering, config\n",
        "    )\n",
        "    logging.info(f\"Successfully calculated {len(local_dist_pairs)} local distribution pairs and the averaged joint distribution.\")\n",
        "\n",
        "    # Assemble the final results into the TopicDistributions dataclass.\n",
        "    distributions = TopicDistributions(\n",
        "        global_prompt_dist=global_prompt_dist,\n",
        "        global_answer_dist=global_answer_dist,\n",
        "        local_dist_pairs=local_dist_pairs,\n",
        "        avg_joint_dist=avg_joint_dist\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 8 Successfully Completed: All topic distributions constructed. ---\")\n",
        "\n",
        "    # Return the structured object containing all distributions.\n",
        "    return distributions\n"
      ],
      "metadata": {
        "id": "_t_r1wpFWNqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Information-Theoretic Metric Computation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Information-Theoretic Metric Computation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module computes the core information-theoretic metrics that quantify the\n",
        "# relationship between prompt and response topic distributions. It provides\n",
        "# numerically stable implementations of Entropy, Kullback-Leibler (KL)\n",
        "# Divergence, and Jensen-Shannon Divergence (JSD), leveraging the robust and\n",
        "# optimized algorithms from SciPy. These metrics form the basis for the final\n",
        "# SDM scores.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structure for Information-Theoretic Metrics\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class InformationTheoreticMetrics:\n",
        "    \"\"\"\n",
        "    A container for all calculated information-theoretic metrics.\n",
        "\n",
        "    This dataclass provides a structured and type-safe way to store the results\n",
        "    of the divergence, entropy, and mutual information calculations.\n",
        "\n",
        "    Attributes:\n",
        "        global_jsd (float): The Global Jensen-Shannon Divergence between P_global and A_global.\n",
        "        global_kl_prompt_answer (float): The Global KL Divergence D_KL(P_global || A_global).\n",
        "        global_kl_answer_prompt (float): The Global KL Divergence D_KL(A_global || P_global).\n",
        "        ensemble_jsd (float): The average JSD across all M local distribution pairs.\n",
        "        ensemble_kl_answer_prompt (float): The average KL Divergence D_KL(A_m || P_m) across all M pairs.\n",
        "        prompt_entropy (float): The Shannon entropy of the global prompt distribution, H(P).\n",
        "        answer_entropy (float): The Shannon entropy of the global answer distribution, H(A).\n",
        "        avg_conditional_entropy (float): The average conditional entropy H(Y|X) across the ensemble.\n",
        "        ensemble_mi (float): The Ensemble Mutual Information I(X;Y) = H(Y) - H(Y|X).\n",
        "    \"\"\"\n",
        "    # The symmetric divergence between the overall prompt and answer topic distributions.\n",
        "    global_jsd: float\n",
        "    # The asymmetric divergence from the global prompt distribution to the global answer distribution.\n",
        "    global_kl_prompt_answer: float\n",
        "    # The asymmetric divergence from the global answer distribution to the global prompt distribution.\n",
        "    global_kl_answer_prompt: float\n",
        "    # The average symmetric divergence across the M individual paraphrase-answer pairs.\n",
        "    ensemble_jsd: float\n",
        "    # The average asymmetric divergence from answer to prompt across the M pairs. Key for 'Semantic Exploration'.\n",
        "    ensemble_kl_answer_prompt: float\n",
        "    # The uncertainty or complexity of the global prompt topic distribution.\n",
        "    prompt_entropy: float\n",
        "    # The uncertainty or complexity of the global answer topic distribution.\n",
        "    answer_entropy: float\n",
        "    # The remaining uncertainty in the answer distribution, given the prompt distribution, averaged over the ensemble.\n",
        "    avg_conditional_entropy: float\n",
        "    # The reduction in uncertainty about the answer distribution from knowing the prompt distribution.\n",
        "    ensemble_mi: float\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 9.1: Core Metric Calculation Utilities\n",
        "# ==============================================================================\n",
        "\n",
        "def _calculate_jsd(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Jensen-Shannon Divergence (JSD) between two probability distributions.\n",
        "\n",
        "    This function uses the numerically stable and optimized implementation from SciPy.\n",
        "    Note: scipy.spatial.distance.jensenshannon returns the square root of the JSD,\n",
        "    so we must square the result to get the true JSD value.\n",
        "\n",
        "    Args:\n",
        "        p: A 1D NumPy array representing the first probability distribution.\n",
        "        q: A 1D NumPy array representing the second probability distribution.\n",
        "\n",
        "    Returns:\n",
        "        The JSD value, a float between 0 and 1 (for base 2).\n",
        "    \"\"\"\n",
        "    # Equation: D_JS(P||Q) = 0.5 * (D_KL(P||M) + D_KL(Q||M)), where M = 0.5 * (P+Q)\n",
        "    # We use SciPy's robust implementation for this calculation.\n",
        "    jsd_sqrt = jensenshannon(p, q, base=2.0)\n",
        "    # Square the result to get the actual JSD, not its square root.\n",
        "    return float(jsd_sqrt**2)\n",
        "\n",
        "def _calculate_kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Kullback-Leibler (KL) Divergence D_KL(p || q).\n",
        "\n",
        "    This function uses the numerically stable and optimized implementation from SciPy.\n",
        "\n",
        "    Args:\n",
        "        p: A 1D NumPy array representing the 'true' distribution.\n",
        "        q: A 1D NumPy array representing the 'approximating' distribution.\n",
        "\n",
        "    Returns:\n",
        "        The KL Divergence value, a non-negative float.\n",
        "    \"\"\"\n",
        "    # Equation: D_KL(P||Q) = Σ P(i) * log2(P(i) / Q(i))\n",
        "    # We use SciPy's entropy function, which calculates KL divergence when a second distribution `q` is provided.\n",
        "    return float(scipy_entropy(pk=p, qk=q, base=2.0))\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 9.2 & 9.3: Metric Computation Logic\n",
        "# ==============================================================================\n",
        "\n",
        "def _calculate_divergence_metrics(\n",
        "    distributions: TopicDistributions\n",
        ") -> Tuple[float, float, float, float, float]:\n",
        "    \"\"\"\n",
        "    Calculates all global and ensemble divergence metrics (JSD and KL).\n",
        "\n",
        "    Args:\n",
        "        distributions: The TopicDistributions object containing all probability distributions.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing: global_jsd, global_kl_p_a, global_kl_a_p, ensemble_jsd, ensemble_kl_a_p.\n",
        "    \"\"\"\n",
        "    # --- Global Divergences ---\n",
        "    # Calculate JSD between the two global distributions.\n",
        "    global_jsd = _calculate_jsd(distributions.global_prompt_dist, distributions.global_answer_dist)\n",
        "    # Calculate KL divergence D(Prompt || Answer).\n",
        "    global_kl_p_a = _calculate_kl_divergence(distributions.global_prompt_dist, distributions.global_answer_dist)\n",
        "    # Calculate KL divergence D(Answer || Prompt).\n",
        "    global_kl_a_p = _calculate_kl_divergence(distributions.global_answer_dist, distributions.global_prompt_dist)\n",
        "\n",
        "    # --- Ensemble Divergences ---\n",
        "    # Lists to store the local divergence values for each of the M pairs.\n",
        "    local_jsd_values = []\n",
        "    local_kl_a_p_values = []\n",
        "\n",
        "    # Iterate through each (P_m, A_m) pair in the local distributions list.\n",
        "    for p_m, a_m in distributions.local_dist_pairs:\n",
        "        # Calculate the JSD for the current local pair.\n",
        "        local_jsd_values.append(_calculate_jsd(p_m, a_m))\n",
        "        # Calculate the KL divergence D(A_m || P_m) for the current local pair.\n",
        "        local_kl_a_p_values.append(_calculate_kl_divergence(a_m, p_m))\n",
        "\n",
        "    # The final ensemble metrics are the average of the local values.\n",
        "    ensemble_jsd = float(np.mean(local_jsd_values))\n",
        "    ensemble_kl_a_p = float(np.mean(local_kl_a_p_values))\n",
        "\n",
        "    return global_jsd, global_kl_p_a, global_kl_a_p, ensemble_jsd, ensemble_kl_a_p\n",
        "\n",
        "def _calculate_entropy_and_mi_metrics(\n",
        "    distributions: TopicDistributions\n",
        ") -> Tuple[float, float, float, float]:\n",
        "    \"\"\"\n",
        "    Calculates all entropy and mutual information metrics.\n",
        "\n",
        "    Args:\n",
        "        distributions: The TopicDistributions object.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing: prompt_entropy, answer_entropy, avg_conditional_entropy, ensemble_mi.\n",
        "    \"\"\"\n",
        "    # --- Global Entropies ---\n",
        "    # Calculate the Shannon entropy of the global prompt distribution.\n",
        "    prompt_entropy = float(scipy_entropy(distributions.global_prompt_dist, base=2.0))\n",
        "    # Calculate the Shannon entropy of the global answer distribution.\n",
        "    answer_entropy = float(scipy_entropy(distributions.global_answer_dist, base=2.0))\n",
        "\n",
        "    # --- Ensemble Conditional Entropy and Mutual Information ---\n",
        "    # A list to store the local conditional entropy H(Y_m | X_m) for each pair.\n",
        "    local_conditional_entropies = []\n",
        "    # Iterate through each (P_m, A_m) pair.\n",
        "    for p_m, a_m in distributions.local_dist_pairs:\n",
        "        # The joint distribution P(X,Y) is the outer product P(X) ⊗ P(Y).\n",
        "        joint_m = np.outer(p_m, a_m)\n",
        "        # The entropy of the joint distribution H(X,Y).\n",
        "        h_xy = float(scipy_entropy(joint_m.flatten(), base=2.0))\n",
        "        # The entropy of the prompt distribution H(X).\n",
        "        h_x = float(scipy_entropy(p_m, base=2.0))\n",
        "        # The conditional entropy is H(Y|X) = H(X,Y) - H(X).\n",
        "        h_y_given_x = h_xy - h_x\n",
        "        local_conditional_entropies.append(h_y_given_x)\n",
        "\n",
        "    # The average conditional entropy is the mean of the local values.\n",
        "    avg_conditional_entropy = float(np.mean(local_conditional_entropies))\n",
        "\n",
        "    # The Ensemble Mutual Information is defined as I(X;Y) = H(Y) - H(Y|X).\n",
        "    ensemble_mi = answer_entropy - avg_conditional_entropy\n",
        "\n",
        "    return prompt_entropy, answer_entropy, avg_conditional_entropy, ensemble_mi\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_information_theoretic_metrics(\n",
        "    distributions: TopicDistributions\n",
        ") -> InformationTheoreticMetrics:\n",
        "    \"\"\"\n",
        "    Orchestrates the calculation of all information-theoretic metrics.\n",
        "\n",
        "    This function takes the computed probability distributions and calculates the\n",
        "    full suite of metrics specified in the paper, including global and ensemble\n",
        "    divergences, entropies, and mutual information.\n",
        "\n",
        "    Args:\n",
        "        distributions: A TopicDistributions object containing all necessary\n",
        "                       probability distributions from Task 8.\n",
        "\n",
        "    Returns:\n",
        "        InformationTheoreticMetrics: A dataclass instance containing all\n",
        "                                     calculated metrics.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 9 process.\n",
        "    logging.info(\"--- Starting Task 9: Information-Theoretic Metric Computation ---\")\n",
        "\n",
        "    # --- Step 9.2: Calculate all divergence metrics (JSD, KL) ---\n",
        "    (\n",
        "        global_jsd,\n",
        "        global_kl_p_a,\n",
        "        global_kl_a_p,\n",
        "        ensemble_jsd,\n",
        "        ensemble_kl_a_p\n",
        "    ) = _calculate_divergence_metrics(distributions)\n",
        "    logging.info(\"Successfully calculated all divergence metrics (JSD, KL).\")\n",
        "\n",
        "    # --- Step 9.3: Calculate all entropy and MI metrics ---\n",
        "    (\n",
        "        prompt_entropy,\n",
        "        answer_entropy,\n",
        "        avg_cond_entropy,\n",
        "        ensemble_mi\n",
        "    ) = _calculate_entropy_and_mi_metrics(distributions)\n",
        "    logging.info(\"Successfully calculated all entropy and mutual information metrics.\")\n",
        "\n",
        "    # Assemble the final results into the InformationTheoreticMetrics dataclass.\n",
        "    metrics = InformationTheoreticMetrics(\n",
        "        global_jsd=global_jsd,\n",
        "        global_kl_prompt_answer=global_kl_p_a,\n",
        "        global_kl_answer_prompt=global_kl_a_p,\n",
        "        ensemble_jsd=ensemble_jsd,\n",
        "        ensemble_kl_answer_prompt=ensemble_kl_a_p,\n",
        "        prompt_entropy=prompt_entropy,\n",
        "        answer_entropy=answer_entropy,\n",
        "        avg_conditional_entropy=avg_cond_entropy,\n",
        "        ensemble_mi=ensemble_mi\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 9 Successfully Completed: All information-theoretic metrics computed. ---\")\n",
        "\n",
        "    # Return the structured result object.\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "Vx3TFN5OW6ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Geometric Distance Computation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Geometric Distance Computation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module computes the geometric distance between the prompt and answer\n",
        "# embedding distributions using the 1-Wasserstein distance (Earth Mover's\n",
        "# Distance). To handle the high-dimensional nature of sentence embeddings in a\n",
        "# computationally tractable manner, this implementation uses the Sliced-Wasserstein\n",
        "# distance, a robust and scalable approximation. This metric provides a signal of\n",
        "# semantic drift that is complementary to the information-theoretic measures.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 10.1: Sliced-Wasserstein Distance Computation\n",
        "# ==============================================================================\n",
        "\n",
        "def _calculate_sliced_wasserstein_distance(\n",
        "    source_embeddings: np.ndarray,\n",
        "    target_embeddings: np.ndarray,\n",
        "    num_projections: int = 500,\n",
        "    random_seed: int = 42\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Sliced-Wasserstein distance between two high-dimensional distributions.\n",
        "\n",
        "    This method approximates the true Wasserstein distance by projecting the\n",
        "    high-dimensional points onto a series of random 1D lines and averaging the\n",
        "    1D Wasserstein distances of these projections. This is a computationally\n",
        "    efficient and robust technique for high-dimensional spaces.\n",
        "\n",
        "    Args:\n",
        "        source_embeddings: A NumPy array of shape (S_source, D) representing the first distribution.\n",
        "        target_embeddings: A NumPy array of shape (S_target, D) representing the second distribution.\n",
        "        num_projections: The number of random 1D projections to use for the approximation.\n",
        "        random_seed: A seed for the random number generator to ensure reproducible projections.\n",
        "\n",
        "    Returns:\n",
        "        The calculated Sliced-Wasserstein distance as a non-negative float.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure both embedding sets are not empty.\n",
        "    if source_embeddings.shape[0] == 0 or target_embeddings.shape[0] == 0:\n",
        "        logging.warning(\"One or both embedding sets are empty. Wasserstein distance is 0.\")\n",
        "        return 0.0\n",
        "    # Ensure dimensionality is consistent.\n",
        "    if source_embeddings.shape[1] != target_embeddings.shape[1]:\n",
        "        raise ValueError(\"Source and target embeddings must have the same dimension.\")\n",
        "\n",
        "    # Get the embedding dimension from the shape of the input array.\n",
        "    embedding_dim = source_embeddings.shape[1]\n",
        "\n",
        "    # --- Generate Random Projections ---\n",
        "    # Create a seeded random number generator for reproducibility.\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    # Generate `num_projections` random vectors on the unit hypersphere.\n",
        "    # This is done by generating vectors from a standard normal distribution\n",
        "    # and then normalizing them to have a unit length (L2 norm = 1).\n",
        "    projections = rng.randn(embedding_dim, num_projections)\n",
        "    projections /= norm(projections, axis=0)\n",
        "\n",
        "    # --- Project Embeddings onto 1D Lines ---\n",
        "    # Project the source embeddings onto the random lines using matrix multiplication.\n",
        "    # The result is a (S_source, num_projections) matrix of 1D values.\n",
        "    source_projections = source_embeddings @ projections\n",
        "    # Project the target embeddings onto the same random lines.\n",
        "    # The result is a (S_target, num_projections) matrix of 1D values.\n",
        "    target_projections = target_embeddings @ projections\n",
        "\n",
        "    # --- Calculate and Average 1D Wasserstein Distances ---\n",
        "    # A list to store the 1D Wasserstein distance for each projection.\n",
        "    wasserstein_distances = []\n",
        "    # Iterate through each of the `num_projections`.\n",
        "    for i in range(num_projections):\n",
        "        # Extract the 1D projected data for the current projection line.\n",
        "        source_proj_i = source_projections[:, i]\n",
        "        target_proj_i = target_projections[:, i]\n",
        "\n",
        "        # Calculate the 1-Wasserstein distance between the two 1D distributions.\n",
        "        # SciPy's implementation is highly efficient for the 1D case.\n",
        "        wd = scipy_wasserstein_1d(source_proj_i, target_proj_i)\n",
        "        wasserstein_distances.append(wd)\n",
        "\n",
        "    # The Sliced-Wasserstein distance is the average of the 1D distances.\n",
        "    sliced_wasserstein_distance = np.mean(wasserstein_distances)\n",
        "\n",
        "    # Return the final calculated distance.\n",
        "    return float(sliced_wasserstein_distance)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_geometric_distance(\n",
        "    embedded_corpus: EmbeddedCorpus,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of the geometric distance between prompt and answer embeddings.\n",
        "\n",
        "    This function serves as the master controller for Task 10. It calculates the\n",
        "    1-Wasserstein distance (approximated via the Sliced-Wasserstein method)\n",
        "    which is a key component of the final S_H hallucination score.\n",
        "\n",
        "    Args:\n",
        "        embedded_corpus: The EmbeddedCorpus object from Task 6, containing the\n",
        "                         validated prompt and answer embedding matrices.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        The calculated 1-Wasserstein distance as a non-negative float.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the calculated distance is not a valid, non-negative number.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 10 process.\n",
        "    logging.info(\"--- Starting Task 10: Geometric Distance Computation ---\")\n",
        "\n",
        "    # --- Step 10.1: Calculate Sliced-Wasserstein Distance ---\n",
        "    # Call the core function to compute the distance between the two embedding clouds.\n",
        "    wasserstein_dist = _calculate_sliced_wasserstein_distance(\n",
        "        source_embeddings=embedded_corpus.prompt_embeddings,\n",
        "        target_embeddings=embedded_corpus.answer_embeddings,\n",
        "        random_seed=config.hyperparameters.reproducibility['global_random_seed']\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Calculated 1-Wasserstein Distance: {wasserstein_dist:.4f}\")\n",
        "\n",
        "    # --- Step 10.2: Validation ---\n",
        "    # Validate the computed distance to ensure it's a valid metric.\n",
        "    if not (np.isfinite(wasserstein_dist) and wasserstein_dist >= 0):\n",
        "        raise ValueError(\n",
        "            f\"Wasserstein distance calculation resulted in an invalid value: {wasserstein_dist}. \"\n",
        "            \"Distance must be a non-negative, finite number.\"\n",
        "        )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 10 Successfully Completed: Geometric distance computed and validated. ---\")\n",
        "\n",
        "    # Return the final, validated distance value.\n",
        "    return wasserstein_dist\n"
      ],
      "metadata": {
        "id": "jgda2JbmYKUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Final Score Aggregation and Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Final Score Aggregation and Validation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module represents the culmination of the analytical pipeline. It aggregates\n",
        "# the information-theoretic and geometric metrics into the three primary scores\n",
        "# defined by the SDM framework: the S_H Hallucination Score, the Phi (NCE)\n",
        "# Score, and the KL Semantic Exploration Score. Finally, it performs a critical\n",
        "# validation of these scores against the benchmark results reported in the\n",
        "# source paper to assess the fidelity of the replication.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Data Structures for Final Results\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class FinalSDMScores:\n",
        "    \"\"\"\n",
        "    A container for the three primary, normalized scores of the SDM framework.\n",
        "\n",
        "    Attributes:\n",
        "        s_h_score (float): The final SDM Hallucination Score (S_H), indicating\n",
        "                           semantic instability.\n",
        "        phi_score (float): The Normalized Conditional Entropy (Φ), indicating\n",
        "                           unexplained response complexity.\n",
        "        kl_exploration_score (float): The KL(Answer || Prompt) score, indicating\n",
        "                                      the degree of semantic exploration.\n",
        "    \"\"\"\n",
        "    # The primary score for semantic instability, a weighted average of JSD and Wasserstein distance, normalized by prompt entropy.\n",
        "    s_h_score: float\n",
        "    # The score for unexplained response complexity, calculated as the conditional entropy normalized by prompt entropy.\n",
        "    phi_score: float\n",
        "    # The score for semantic exploration, calculated as the ensemble KL divergence normalized by prompt entropy.\n",
        "    kl_exploration_score: float\n",
        "\n",
        "@dataclass\n",
        "class SDMFullResult:\n",
        "    \"\"\"\n",
        "    A comprehensive container for all results and diagnostics of an SDM run.\n",
        "\n",
        "    This object encapsulates the entire output of the pipeline, making it a\n",
        "    self-contained, serializable artifact for logging, reporting, and further analysis.\n",
        "\n",
        "    Attributes:\n",
        "        final_scores (FinalSDMScores): The three primary SDM scores.\n",
        "        diagnostic_metrics (InformationTheoreticMetrics): The full suite of intermediate metrics.\n",
        "        wasserstein_distance (float): The calculated geometric distance.\n",
        "        validation_report (Dict[str, Any]): A report comparing results to paper benchmarks.\n",
        "    \"\"\"\n",
        "    # The final, high-level scores for interpretation.\n",
        "    final_scores: FinalSDMScores\n",
        "    # The detailed, unaggregated metrics for deep-dive analysis.\n",
        "    diagnostic_metrics: InformationTheoreticMetrics\n",
        "    # The geometric distance component of the S_H score.\n",
        "    wasserstein_distance: float\n",
        "    # A structured dictionary detailing the outcome of validation against paper benchmarks.\n",
        "    validation_report: Dict[str, Any]\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 11.1: Primary Metric Calculation\n",
        "# ==============================================================================\n",
        "\n",
        "def _calculate_sh_score(\n",
        "    ensemble_jsd: float,\n",
        "    wasserstein_distance: float,\n",
        "    prompt_entropy: float,\n",
        "    weights: Dict[str, float]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the primary SDM Hallucination Score (S_H).\n",
        "\n",
        "    Args:\n",
        "        ensemble_jsd: The average JSD across all M local distribution pairs.\n",
        "        wasserstein_distance: The calculated 1-Wasserstein distance.\n",
        "        prompt_entropy: The Shannon entropy of the global prompt distribution, H(P).\n",
        "        weights: A dictionary containing w_jsd and w_wass.\n",
        "\n",
        "    Returns:\n",
        "        The calculated S_H score.\n",
        "    \"\"\"\n",
        "    # Equation: S_H = (w_jsd * D_JS_ens + w_wass * W_d) / H(P)\n",
        "    # Check for the edge case of zero prompt entropy to prevent division by zero.\n",
        "    if prompt_entropy == 0:\n",
        "        # If prompt has no complexity, divergence is trivially zero.\n",
        "        return 0.0\n",
        "\n",
        "    # Retrieve the weights from the input dictionary.\n",
        "    w_jsd = weights['w_jsd']\n",
        "    w_wass = weights['w_wass']\n",
        "\n",
        "    # Calculate the weighted sum of the divergence components.\n",
        "    numerator = (w_jsd * ensemble_jsd) + (w_wass * wasserstein_distance)\n",
        "\n",
        "    # Normalize by the prompt entropy.\n",
        "    s_h_score = numerator / prompt_entropy\n",
        "\n",
        "    return s_h_score\n",
        "\n",
        "def _calculate_phi_score(\n",
        "    avg_conditional_entropy: float,\n",
        "    prompt_entropy: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Normalized Conditional Entropy (Φ) score.\n",
        "\n",
        "    Args:\n",
        "        avg_conditional_entropy: The average conditional entropy H(Y|X).\n",
        "        prompt_entropy: The Shannon entropy of the global prompt distribution, H(P).\n",
        "\n",
        "    Returns:\n",
        "        The calculated Φ score.\n",
        "    \"\"\"\n",
        "    # Equation: Φ = H(Y|X) / H(X)\n",
        "    # Check for the edge case of zero prompt entropy.\n",
        "    if prompt_entropy == 0:\n",
        "        # If prompt has no complexity, the ratio is undefined; we define it as 1.0\n",
        "        # as the response complexity perfectly matches the (zero) prompt complexity.\n",
        "        return 1.0\n",
        "\n",
        "    # Calculate the ratio.\n",
        "    phi_score = avg_conditional_entropy / prompt_entropy\n",
        "\n",
        "    return phi_score\n",
        "\n",
        "def _calculate_kl_exploration_score(\n",
        "    ensemble_kl_answer_prompt: float,\n",
        "    prompt_entropy: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the KL-based Semantic Exploration score.\n",
        "\n",
        "    Args:\n",
        "        ensemble_kl_answer_prompt: The average KL Divergence D_KL(A_m || P_m).\n",
        "        prompt_entropy: The Shannon entropy of the global prompt distribution, H(P).\n",
        "\n",
        "    Returns:\n",
        "        The calculated KL exploration score.\n",
        "    \"\"\"\n",
        "    # Equation: KL(Answer||Prompt) = D_KL_ens(A||P) / H(P)\n",
        "    # Check for the edge case of zero prompt entropy.\n",
        "    if prompt_entropy == 0:\n",
        "        # If prompt has no complexity, exploration is trivially zero.\n",
        "        return 0.0\n",
        "\n",
        "    # Normalize the ensemble KL divergence by the prompt entropy.\n",
        "    kl_score = ensemble_kl_answer_prompt / prompt_entropy\n",
        "\n",
        "    return kl_score\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 11.2: Score Validation and Paper Comparison\n",
        "# ==============================================================================\n",
        "\n",
        "def _validate_final_scores(\n",
        "    final_scores: FinalSDMScores,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates final scores against generic ranges and specific paper benchmarks.\n",
        "\n",
        "    Args:\n",
        "        final_scores: The dataclass containing the calculated final scores.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing a detailed validation report.\n",
        "    \"\"\"\n",
        "    # Log the start of the final validation process.\n",
        "    logging.info(\"Validating final scores against paper benchmarks...\")\n",
        "\n",
        "    # Retrieve the expected output targets from the configuration.\n",
        "    targets = config.expected_outputs['paper_comparison_targets']\n",
        "\n",
        "    # --- S_H Score Validation ---\n",
        "    s_h_range = targets['S_H_expected_range']\n",
        "    s_h_check = s_h_range[0] <= final_scores.s_h_score <= s_h_range[1]\n",
        "\n",
        "    # --- Phi Score Validation ---\n",
        "    phi_range = targets['phi_expected_range']\n",
        "    phi_check = phi_range[0] <= final_scores.phi_score <= phi_range[1]\n",
        "\n",
        "    # --- Construct the Report ---\n",
        "    report = {\n",
        "        \"source\": targets['source'],\n",
        "        \"s_h_score_validation\": {\n",
        "            \"computed_value\": round(final_scores.s_h_score, 4),\n",
        "            \"expected_range\": s_h_range,\n",
        "            \"passed\": bool(s_h_check)\n",
        "        },\n",
        "        \"phi_score_validation\": {\n",
        "            \"computed_value\": round(final_scores.phi_score, 4),\n",
        "            \"expected_range\": phi_range,\n",
        "            \"passed\": bool(phi_check)\n",
        "        },\n",
        "        \"overall_fidelity\": \"HIGH\" if s_h_check and phi_check else \"MEDIUM\"\n",
        "    }\n",
        "\n",
        "    # Log a summary of the validation outcome.\n",
        "    log_msg = (\n",
        "        f\"Benchmark Validation Summary: \"\n",
        "        f\"S_H Passed: {report['s_h_score_validation']['passed']}, \"\n",
        "        f\"Phi Passed: {report['phi_score_validation']['passed']}. \"\n",
        "        f\"Overall Fidelity: {report['overall_fidelity']}\"\n",
        "    )\n",
        "    if report['overall_fidelity'] == \"HIGH\":\n",
        "        logging.info(log_msg)\n",
        "    else:\n",
        "        logging.warning(log_msg)\n",
        "\n",
        "    return report\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def aggregate_and_validate_scores(\n",
        "    metrics: InformationTheoreticMetrics,\n",
        "    wasserstein_distance: float,\n",
        "    config: FusedExperimentInputModel\n",
        ") -> SDMFullResult:\n",
        "    \"\"\"\n",
        "    Orchestrates the final aggregation and validation of all computed metrics.\n",
        "\n",
        "    This function serves as the master controller for Task 11. It synthesizes\n",
        "    the intermediate metrics into the final SDM scores and then validates them\n",
        "    against the benchmarks provided in the source paper.\n",
        "\n",
        "    Args:\n",
        "        metrics: The InformationTheoreticMetrics object from Task 9.\n",
        "        wasserstein_distance: The geometric distance from Task 10.\n",
        "        config: The validated Pydantic model of the experiment configuration.\n",
        "\n",
        "    Returns:\n",
        "        SDMFullResult: A comprehensive dataclass containing all final scores,\n",
        "                       diagnostics, and validation results for the run.\n",
        "    \"\"\"\n",
        "    # Log the start of the Task 11 process.\n",
        "    logging.info(\"--- Starting Task 11: Final Score Aggregation and Validation ---\")\n",
        "\n",
        "    # --- Step 11.1: Primary Metric Calculation ---\n",
        "    # Calculate the S_H score.\n",
        "    s_h_score = _calculate_sh_score(\n",
        "        ensemble_jsd=metrics.ensemble_jsd,\n",
        "        wasserstein_distance=wasserstein_distance,\n",
        "        prompt_entropy=metrics.prompt_entropy,\n",
        "        weights=config.hyperparameters.final_score_weights.dict()\n",
        "    )\n",
        "    # Calculate the Phi score.\n",
        "    phi_score = _calculate_phi_score(\n",
        "        avg_conditional_entropy=metrics.avg_conditional_entropy,\n",
        "        prompt_entropy=metrics.prompt_entropy\n",
        "    )\n",
        "    # Calculate the KL exploration score.\n",
        "    kl_exploration_score = _calculate_kl_exploration_score(\n",
        "        ensemble_kl_answer_prompt=metrics.ensemble_kl_answer_prompt,\n",
        "        prompt_entropy=metrics.prompt_entropy\n",
        "    )\n",
        "\n",
        "    # Assemble the final scores into their dataclass.\n",
        "    final_scores = FinalSDMScores(\n",
        "        s_h_score=s_h_score,\n",
        "        phi_score=phi_score,\n",
        "        kl_exploration_score=kl_exploration_score\n",
        "    )\n",
        "    logging.info(f\"Calculated Final Scores: S_H={s_h_score:.4f}, Phi={phi_score:.4f}, KL_Explore={kl_exploration_score:.4f}\")\n",
        "\n",
        "    # --- Step 11.2: Score Validation and Paper Comparison ---\n",
        "    validation_report = _validate_final_scores(final_scores, config)\n",
        "\n",
        "    # --- Step 11.3: Results Compilation ---\n",
        "    # Assemble the final, comprehensive result object.\n",
        "    full_result = SDMFullResult(\n",
        "        final_scores=final_scores,\n",
        "        diagnostic_metrics=metrics,\n",
        "        wasserstein_distance=wasserstein_distance,\n",
        "        validation_report=validation_report\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"--- Task 11 Successfully Completed: Final scores aggregated and validated. ---\")\n",
        "\n",
        "    # Return the comprehensive result object.\n",
        "    return full_result\n"
      ],
      "metadata": {
        "id": "5G90FfTEZJyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Task 12: Master Orchestrator Function\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module provides the master orchestrator for the entire Semantic\n",
        "# Divergence Metrics (SDM) framework. It serves as the single entry point for\n",
        "# executing a full experimental run, from initial configuration validation to\n",
        "# the generation of final, validated scores. The orchestrator ensures a\n",
        "# robust, sequential execution of all modular tasks, managing the flow of data\n",
        "# and providing comprehensive error handling for the entire pipeline.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: The Master Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_sdm_pipeline(\n",
        "    experiment_config: Dict[str, Any]\n",
        ") -> SDMFullResult:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end Semantic Divergence Metrics (SDM) pipeline.\n",
        "\n",
        "    This master function orchestrates the entire workflow, from initial configuration\n",
        "    validation to the final aggregation of scores. It calls each modular task\n",
        "    orchestrator in sequence, passing the output of one stage as the input to the\n",
        "    next, ensuring a robust and auditable execution flow.\n",
        "\n",
        "    Args:\n",
        "        experiment_config: The raw input dictionary for the SDM experiment,\n",
        "                           conforming to the FusedExperimentInput structure.\n",
        "\n",
        "    Returns:\n",
        "        SDMFullResult: A comprehensive dataclass containing all final scores,\n",
        "                       diagnostics, and validation results for the run.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates any exception from any stage of the pipeline,\n",
        "                   indicating a failure in the run.\n",
        "    \"\"\"\n",
        "    # Start a timer for the entire pipeline execution.\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Configure logging for the pipeline run.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # Log the initiation of the pipeline.\n",
        "    experiment_name = experiment_config.get(\"experiment_metadata\", {}).get(\"experiment_name\", \"Unknown Experiment\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"🚀 STARTING SDM PIPELINE RUN: {experiment_name}\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- Task 1: Parameter Validation and Data Cleansing ---\n",
        "        # The first and most critical step: validate the entire input configuration.\n",
        "        is_valid, validated_config, message = validate_and_clean_sdm_config(experiment_config)\n",
        "        if not is_valid:\n",
        "            # If validation fails, log the detailed error and raise a ValueError.\n",
        "            logging.error(f\"Configuration validation failed. Halting pipeline. Reason:\\n{message}\")\n",
        "            raise ValueError(\"Invalid experiment configuration.\")\n",
        "        logging.info(message)\n",
        "\n",
        "        # --- Task 2: Environment Setup and Model Initialization ---\n",
        "        # Set up a deterministic and operational runtime environment.\n",
        "        runtime_env = initialize_environment_and_models(validated_config)\n",
        "\n",
        "        # --- Task 3: Paraphrase Generation ---\n",
        "        # Generate and validate the corpus of M paraphrases.\n",
        "        validated_paraphrases = generate_validated_paraphrases(validated_config, runtime_env)\n",
        "\n",
        "        # --- Task 4: Response Generation ---\n",
        "        # Generate and validate the M x N matrix of responses.\n",
        "        validated_responses = generate_validated_responses(validated_paraphrases, validated_config, runtime_env)\n",
        "\n",
        "        # --- Task 5: Text Processing and Sentence Segmentation ---\n",
        "        # Deconstruct texts into a validated, cataloged corpus of sentences.\n",
        "        corpus = segment_and_validate_corpus(validated_paraphrases, validated_responses, validated_config)\n",
        "\n",
        "        # --- Task 6: Embedding Generation ---\n",
        "        # Convert the sentence corpus into a validated, high-dimensional vector space.\n",
        "        embedded_corpus = generate_and_validate_embeddings(corpus, runtime_env, validated_config)\n",
        "\n",
        "        # --- Task 7: Clustering and Topic Identification ---\n",
        "        # Partition the embedding space into discrete semantic topics.\n",
        "        clustering_result = perform_joint_clustering(embedded_corpus, validated_config)\n",
        "\n",
        "        # --- Task 8: Probability Distribution Construction ---\n",
        "        # Translate cluster labels into numerically stable probability distributions.\n",
        "        distributions = construct_topic_distributions(corpus, clustering_result, validated_config)\n",
        "\n",
        "        # --- Task 9: Information-Theoretic Metric Computation ---\n",
        "        # Calculate the full suite of information-theoretic metrics.\n",
        "        it_metrics = compute_information_theoretic_metrics(distributions)\n",
        "\n",
        "        # --- Task 10: Geometric Distance Computation ---\n",
        "        # Calculate the Wasserstein distance between embedding clouds.\n",
        "        wasserstein_distance = compute_geometric_distance(embedded_corpus, validated_config)\n",
        "\n",
        "        # --- Task 11: Final Score Aggregation and Validation ---\n",
        "        # Synthesize all metrics into the final scores and validate against benchmarks.\n",
        "        final_result = aggregate_and_validate_scores(it_metrics, wasserstein_distance, validated_config)\n",
        "\n",
        "        # Stop the timer and calculate the total execution time.\n",
        "        end_time = time.time()\n",
        "        total_duration = end_time - start_time\n",
        "\n",
        "        # Log the successful completion of the entire pipeline.\n",
        "        logging.info(f\"======================================================================\")\n",
        "        logging.info(f\"✅ SDM PIPELINE RUN COMPLETED SUCCESSFULLY for '{experiment_name}'\")\n",
        "        logging.info(f\"   Total Execution Time: {total_duration:.2f} seconds\")\n",
        "        logging.info(f\"======================================================================\")\n",
        "\n",
        "        # Return the final, comprehensive result object.\n",
        "        return final_result\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any exception occurs at any stage, log it with a traceback.\n",
        "        logging.error(\"💥 PIPELINE EXECUTION FAILED 💥\", exc_info=True)\n",
        "        # Re-raise the exception to signal failure to the calling context.\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "uS7c5jYff0Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "# Step 13.1: Parameter Sensitivity Analysis\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module provides functions to perform a sensitivity analysis on the SDM\n",
        "# framework. It systematically perturbs key hyperparameters, re-runs the entire\n",
        "# pipeline for each perturbation, and collects the results to assess the\n",
        "# stability and robustness of the final SDM scores.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13.1 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_parameter_sensitivity_analysis(\n",
        "    baseline_config: Dict[str, Any],\n",
        "    temperature_variants: List[float]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a sensitivity analysis on the response generation temperature.\n",
        "\n",
        "    This function systematically perturbs the `temperature` hyperparameter,\n",
        "    executes the full SDM pipeline for each variant, and aggregates the\n",
        "    resulting primary scores into a summary DataFrame for analysis.\n",
        "\n",
        "    Args:\n",
        "        baseline_config: The baseline experiment configuration dictionary.\n",
        "        temperature_variants: A list of temperature values to test.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the sensitivity analysis results,\n",
        "                      with columns for the parameter, its value, and the\n",
        "                      resulting S_H, Phi, and KL scores.\n",
        "    \"\"\"\n",
        "    # Log the initiation of the sensitivity analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"🚀 STARTING PARAMETER SENSITIVITY ANALYSIS for 'temperature'\")\n",
        "    logging.info(f\"   Testing values: {temperature_variants}\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # A list to store the results of each experimental run.\n",
        "    results_log = []\n",
        "\n",
        "    # Iterate through each specified temperature variant.\n",
        "    for temp_value in temperature_variants:\n",
        "        # Log the start of the run for the current parameter value.\n",
        "        logging.info(f\"--- Running pipeline with temperature = {temp_value:.2f} ---\")\n",
        "\n",
        "        # Create a deep copy of the baseline configuration to prevent mutation.\n",
        "        # This is CRITICAL for ensuring that each run is independent.\n",
        "        perturbed_config = copy.deepcopy(baseline_config)\n",
        "\n",
        "        # Modify the specific hyperparameter in the copied configuration.\n",
        "        # This is a targeted perturbation of the system.\n",
        "        try:\n",
        "            perturbed_config['hyperparameters']['llm_inference_params']['response_generation']['temperature'] = temp_value\n",
        "        except KeyError as e:\n",
        "            # Handle cases where the config structure is unexpectedly different.\n",
        "            logging.error(f\"Configuration structure error: Could not set temperature. Missing key: {e}\")\n",
        "            continue # Skip to the next variant\n",
        "\n",
        "        try:\n",
        "            # Execute the entire SDM pipeline with the perturbed configuration.\n",
        "            result: SDMFullResult = run_sdm_pipeline(perturbed_config)\n",
        "\n",
        "            # Extract the final scores from the comprehensive result object.\n",
        "            final_scores = result.final_scores\n",
        "\n",
        "            # Append the results of this run to our log.\n",
        "            results_log.append({\n",
        "                \"parameter_name\": \"temperature\",\n",
        "                \"parameter_value\": temp_value,\n",
        "                \"s_h_score\": final_scores.s_h_score,\n",
        "                \"phi_score\": final_scores.phi_score,\n",
        "                \"kl_exploration_score\": final_scores.kl_exploration_score,\n",
        "                \"run_status\": \"SUCCESS\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            # If a pipeline run fails for any reason, log the error and continue.\n",
        "            # This ensures that one failed run does not halt the entire analysis.\n",
        "            logging.error(f\"Pipeline run failed for temperature = {temp_value:.2f}. Error: {e}\")\n",
        "\n",
        "            # Append a failure record to the log.\n",
        "            results_log.append({\n",
        "                \"parameter_name\": \"temperature\",\n",
        "                \"parameter_value\": temp_value,\n",
        "                \"s_h_score\": None,\n",
        "                \"phi_score\": None,\n",
        "                \"kl_exploration_score\": None,\n",
        "                \"run_status\": \"FAILURE\"\n",
        "            })\n",
        "\n",
        "    # Log the completion of the analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"✅ PARAMETER SENSITIVITY ANALYSIS COMPLETED\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # Convert the list of result dictionaries into a pandas DataFrame for easy analysis.\n",
        "    results_df = pd.DataFrame(results_log)\n",
        "\n",
        "    # Return the final summary DataFrame.\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "40De9uylmZ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "# Step 13.2: Model Substitution Robustness Testing\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module assesses the robustness of the SDM framework to changes in its\n",
        "# core components, specifically the sentence embedding model. It runs the full\n",
        "# pipeline with both the primary and fallback models and provides a quantitative\n",
        "# comparison of the final scores to measure the impact of the substitution.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13.2 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_model_substitution_analysis(\n",
        "    baseline_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a robustness analysis by substituting the sentence embedding model.\n",
        "\n",
        "    This function executes the full SDM pipeline twice: once with the primary\n",
        "    embedding model specified in the configuration, and a second time with the\n",
        "    specified fallback model. It then compares the final SDM scores from both\n",
        "    runs to assess the framework's sensitivity to this component change.\n",
        "\n",
        "    Args:\n",
        "        baseline_config: The baseline experiment configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame providing a side-by-side comparison of the\n",
        "                      primary scores from both runs and an assessment of the\n",
        "                      deviation.\n",
        "    \"\"\"\n",
        "    # Log the initiation of the model substitution analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"🚀 STARTING MODEL SUBSTITUTION ROBUSTNESS ANALYSIS\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # --- 1. Baseline Run ---\n",
        "    # Execute the pipeline with the original, unmodified configuration.\n",
        "    logging.info(\"--- Running pipeline with PRIMARY embedding model... ---\")\n",
        "    try:\n",
        "        # Run the pipeline and store the full result object.\n",
        "        baseline_result: SDMFullResult = run_sdm_pipeline(baseline_config)\n",
        "        # Extract the final scores for comparison.\n",
        "        baseline_scores = baseline_result.final_scores\n",
        "    except Exception as e:\n",
        "        # If the baseline run fails, the entire analysis cannot proceed.\n",
        "        logging.error(f\"Baseline pipeline run failed, cannot perform model substitution analysis. Error: {e}\")\n",
        "        # Re-raise the exception to halt execution.\n",
        "        raise\n",
        "\n",
        "    # --- 2. Fallback Model Run ---\n",
        "    # Log the start of the second run with the substituted model.\n",
        "    logging.info(\"--- Running pipeline with FALLBACK embedding model... ---\")\n",
        "\n",
        "    # Create a deep copy of the baseline configuration to modify.\n",
        "    fallback_config = copy.deepcopy(baseline_config)\n",
        "\n",
        "    try:\n",
        "        # Perturb the configuration to use the fallback model.\n",
        "        # This is a targeted substitution of a core system component.\n",
        "        fallback_model_name = fallback_config['system_components']['sentence_embedding_model']['fallback_model']\n",
        "        fallback_config['system_components']['sentence_embedding_model']['model_identifier'] = fallback_model_name\n",
        "\n",
        "        # Execute the pipeline with the modified configuration.\n",
        "        fallback_result: SDMFullResult = run_sdm_pipeline(fallback_config)\n",
        "        # Extract the final scores from the fallback run.\n",
        "        fallback_scores = fallback_result.final_scores\n",
        "\n",
        "    except Exception as e:\n",
        "        # If the fallback run fails, log the error but do not crash.\n",
        "        # We can still report the baseline results.\n",
        "        logging.error(f\"Fallback pipeline run failed. Analysis will be incomplete. Error: {e}\")\n",
        "        # Set fallback scores to None to indicate failure.\n",
        "        fallback_scores = None\n",
        "\n",
        "    # --- 3. Results Comparison and Analysis ---\n",
        "    # Prepare the data for the comparison DataFrame.\n",
        "    data = {\n",
        "        'Metric': ['S_H Score', 'Phi Score', 'KL Exploration Score'],\n",
        "        'Baseline_Value': [\n",
        "            baseline_scores.s_h_score,\n",
        "            baseline_scores.phi_score,\n",
        "            baseline_scores.kl_exploration_score\n",
        "        ],\n",
        "        'Fallback_Value': [\n",
        "            fallback_scores.s_h_score if fallback_scores else None,\n",
        "            fallback_scores.phi_score if fallback_scores else None,\n",
        "            fallback_scores.kl_exploration_score if fallback_scores else None\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Create the comparison DataFrame.\n",
        "    results_df = pd.DataFrame(data)\n",
        "\n",
        "    # Calculate the percentage deviation if the fallback run was successful.\n",
        "    if fallback_scores:\n",
        "        # Use a small epsilon to avoid division by zero if a baseline score is 0.\n",
        "        epsilon = 1e-9\n",
        "        # Calculate the percentage change: (New - Old) / |Old|\n",
        "        results_df['Percent_Deviation'] = 100 * (results_df['Fallback_Value'] - results_df['Baseline_Value']) / (abs(results_df['Baseline_Value']) + epsilon)\n",
        "    else:\n",
        "        # If the fallback run failed, indicate this in the deviation column.\n",
        "        results_df['Percent_Deviation'] = None\n",
        "\n",
        "    # Log the completion of the analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"✅ MODEL SUBSTITUTION ROBUSTNESS ANALYSIS COMPLETED\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # Print the final comparison table to the console for immediate review.\n",
        "    logging.info(\"Model Substitution Results:\\n\" + results_df.to_string(index=False))\n",
        "\n",
        "    # Return the final summary DataFrame.\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "7HR5pAxXnK8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "# Step 13.3: Statistical Robustness Validation\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module quantifies the statistical stability of the SDM framework.\n",
        "# By executing the entire pipeline multiple times with different random seeds,\n",
        "# it measures the inherent variance in the final scores that arises from the\n",
        "# stochastic nature of LLM sampling and clustering initializations. The output\n",
        "# provides confidence intervals and coefficients of variation, transforming\n",
        "# single-point estimates into statistically robust results.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13.3 Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_statistical_robustness_analysis(\n",
        "    baseline_config: Dict[str, Any],\n",
        "    num_runs: int = 5\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a statistical robustness analysis by running the pipeline multiple times.\n",
        "\n",
        "    This function executes the full SDM pipeline `num_runs` times, each time with\n",
        "    a different random seed. It then computes descriptive statistics (mean, std),\n",
        "    95% confidence intervals, and the coefficient of variation for the primary\n",
        "    SDM scores to quantify their stability.\n",
        "\n",
        "    Args:\n",
        "        baseline_config: The baseline experiment configuration dictionary.\n",
        "        num_runs: The number of times to run the pipeline with different seeds.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the statistical properties of the\n",
        "                      primary scores across all runs.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(num_runs, int) or num_runs < 2:\n",
        "        raise ValueError(\"`num_runs` must be an integer greater than 1 for statistical analysis.\")\n",
        "\n",
        "    # Log the initiation of the statistical robustness analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"🚀 STARTING STATISTICAL ROBUSTNESS ANALYSIS ({num_runs} runs)\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # A list to store the FinalSDMScores object from each successful run.\n",
        "    results_log: List[FinalSDMScores] = []\n",
        "    # Get the initial seed from the baseline configuration.\n",
        "    base_seed = baseline_config['hyperparameters']['reproducibility']['global_random_seed']\n",
        "\n",
        "    # Execute the pipeline `num_runs` times.\n",
        "    for i in range(num_runs):\n",
        "        # Define a new, unique seed for the current run.\n",
        "        current_seed = base_seed + i\n",
        "        logging.info(f\"--- Starting run {i+1}/{num_runs} with random seed {current_seed} ---\")\n",
        "\n",
        "        # Create a deep copy of the baseline configuration.\n",
        "        perturbed_config = copy.deepcopy(baseline_config)\n",
        "\n",
        "        # Perturb the random seeds in the configuration.\n",
        "        try:\n",
        "            perturbed_config['hyperparameters']['reproducibility']['global_random_seed'] = current_seed\n",
        "            perturbed_config['hyperparameters']['reproducibility']['numpy_seed'] = current_seed\n",
        "            perturbed_config['hyperparameters']['reproducibility']['sklearn_random_state'] = current_seed\n",
        "        except KeyError as e:\n",
        "            logging.error(f\"Configuration structure error: Could not set seed. Missing key: {e}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Execute the entire SDM pipeline with the perturbed configuration.\n",
        "            result: SDMFullResult = run_sdm_pipeline(perturbed_config)\n",
        "            # Append the final scores of this successful run to our log.\n",
        "            results_log.append(result.final_scores)\n",
        "\n",
        "        except Exception as e:\n",
        "            # If a run fails, log the error and continue to the next run.\n",
        "            logging.error(f\"Pipeline run {i+1}/{num_runs} failed with seed {current_seed}. Error: {e}\")\n",
        "\n",
        "    # --- Statistical Aggregation ---\n",
        "    # Check if there are enough successful runs to perform analysis.\n",
        "    if len(results_log) < 2:\n",
        "        logging.error(\"Fewer than 2 successful runs completed. Cannot perform statistical analysis.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert the list of dataclass objects into a DataFrame for easy computation.\n",
        "    results_df = pd.DataFrame([res.__dict__ for res in results_log])\n",
        "\n",
        "    # Calculate descriptive statistics for each primary score.\n",
        "    summary_stats = results_df.agg(['mean', 'std']).T\n",
        "\n",
        "    # Calculate the 95% confidence interval for the mean.\n",
        "    # For small sample sizes (n < 30), it is more accurate to use the t-distribution.\n",
        "    n = len(results_df)\n",
        "    # Degrees of freedom\n",
        "    dof = n - 1\n",
        "    # t-critical value for 95% confidence\n",
        "    t_crit = sp_stats.t.ppf(0.975, dof)\n",
        "    # Standard error of the mean\n",
        "    sem = summary_stats['std'] / np.sqrt(n)\n",
        "    # Margin of error\n",
        "    summary_stats['ci_95_margin'] = t_crit * sem\n",
        "    # Lower bound of the confidence interval\n",
        "    summary_stats['ci_95_lower'] = summary_stats['mean'] - summary_stats['ci_95_margin']\n",
        "    # Upper bound of the confidence interval\n",
        "    summary_stats['ci_95_upper'] = summary_stats['mean'] + summary_stats['ci_95_margin']\n",
        "\n",
        "    # Calculate the Coefficient of Variation (CV) = std / |mean|.\n",
        "    # This is a normalized measure of dispersion.\n",
        "    epsilon = 1e-9 # To avoid division by zero\n",
        "    summary_stats['coeff_of_variation'] = summary_stats['std'] / (abs(summary_stats['mean']) + epsilon)\n",
        "\n",
        "    # Log the completion of the analysis.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"✅ STATISTICAL ROBUSTNESS ANALYSIS COMPLETED ({len(results_log)} successful runs)\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # Print the final summary table to the console for immediate review.\n",
        "    logging.info(\"Statistical Robustness Summary:\\n\" + summary_stats.to_string())\n",
        "\n",
        "    # Return the final summary DataFrame.\n",
        "    return summary_stats\n"
      ],
      "metadata": {
        "id": "xVMdEa9FnyTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Robustness Analysis and Experimental Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Master Orchestrator for Robustness Analysis\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module provides the master orchestrator for the entire suite of\n",
        "# robustness and validation experiments. It systematically executes each\n",
        "# analysis—parameter sensitivity, model substitution, and statistical\n",
        "# stability—to provide a comprehensive characterization of the SDM framework's\n",
        "# behavior under perturbation.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Master Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_full_robustness_analysis(\n",
        "    baseline_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates and executes the complete suite of robustness analyses.\n",
        "\n",
        "    This master function serves as the single entry point for Task 13. It calls\n",
        "    each of the specialized robustness analysis functions in sequence:\n",
        "    1. Parameter Sensitivity Analysis (for temperature).\n",
        "    2. Model Substitution Analysis (for the embedding model).\n",
        "    3. Statistical Robustness Analysis (for multiple random seeds).\n",
        "\n",
        "    It collects the results from each analysis into a consolidated dictionary\n",
        "    of DataFrames, providing a comprehensive overview of the framework's stability.\n",
        "\n",
        "    Args:\n",
        "        baseline_config: The baseline experiment configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are the names of the analyses and values are\n",
        "        the corresponding pandas DataFrames with the results.\n",
        "    \"\"\"\n",
        "    # Start a timer for the entire analysis suite.\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Configure logging for the analysis run.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # Log the initiation of the full robustness suite.\n",
        "    experiment_name = baseline_config.get(\"experiment_metadata\", {}).get(\"experiment_name\", \"Unknown Experiment\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"🚀 STARTING FULL ROBUSTNESS ANALYSIS SUITE for '{experiment_name}'\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # A dictionary to store the results of all analyses.\n",
        "    full_results: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    # --- 1. Parameter Sensitivity Analysis ---\n",
        "    try:\n",
        "        # Define the temperature values to test for the sensitivity analysis.\n",
        "        temperature_variants = [0.5, 0.7, 0.9, 1.1]\n",
        "        # Execute the parameter sensitivity analysis.\n",
        "        param_sensitivity_df = run_parameter_sensitivity_analysis(\n",
        "            baseline_config, temperature_variants\n",
        "        )\n",
        "        # Store the resulting DataFrame.\n",
        "        full_results['parameter_sensitivity'] = param_sensitivity_df\n",
        "    except Exception as e:\n",
        "        # If the entire analysis fails, log the error and store an empty DataFrame.\n",
        "        logging.error(f\"Parameter Sensitivity Analysis failed catastrophically. Error: {e}\", exc_info=True)\n",
        "        full_results['parameter_sensitivity'] = pd.DataFrame()\n",
        "\n",
        "    # --- 2. Model Substitution Analysis ---\n",
        "    try:\n",
        "        # Execute the model substitution analysis.\n",
        "        model_sub_df = run_model_substitution_analysis(baseline_config)\n",
        "        # Store the resulting DataFrame.\n",
        "        full_results['model_substitution'] = model_sub_df\n",
        "    except Exception as e:\n",
        "        # If the analysis fails, log the error and store an empty DataFrame.\n",
        "        logging.error(f\"Model Substitution Analysis failed catastrophically. Error: {e}\", exc_info=True)\n",
        "        full_results['model_substitution'] = pd.DataFrame()\n",
        "\n",
        "    # --- 3. Statistical Robustness Analysis ---\n",
        "    try:\n",
        "        # Define the number of runs for the statistical analysis.\n",
        "        num_stat_runs = 5\n",
        "        # Execute the statistical robustness analysis.\n",
        "        stat_robustness_df = run_statistical_robustness_analysis(\n",
        "            baseline_config, num_runs=num_stat_runs\n",
        "        )\n",
        "        # Store the resulting DataFrame.\n",
        "        full_results['statistical_robustness'] = stat_robustness_df\n",
        "    except Exception as e:\n",
        "        # If the analysis fails, log the error and store an empty DataFrame.\n",
        "        logging.error(f\"Statistical Robustness Analysis failed catastrophically. Error: {e}\", exc_info=True)\n",
        "        full_results['statistical_robustness'] = pd.DataFrame()\n",
        "\n",
        "    # Stop the timer and calculate the total execution time.\n",
        "    end_time = time.time()\n",
        "    total_duration = end_time - start_time\n",
        "\n",
        "    # Log the successful completion of the entire suite.\n",
        "    logging.info(f\"======================================================================\")\n",
        "    logging.info(f\"✅ FULL ROBUSTNESS ANALYSIS SUITE COMPLETED\")\n",
        "    logging.info(f\"   Total Execution Time: {total_duration / 60:.2f} minutes\")\n",
        "    logging.info(f\"======================================================================\")\n",
        "\n",
        "    # Return the dictionary containing all result DataFrames.\n",
        "    return full_results\n"
      ],
      "metadata": {
        "id": "ai1zfbPSo_NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Task: Top-Level Master Orchestrator\n",
        "#\n",
        "# Author: CS Chirinda\n",
        "# Date: August 16, 2025\n",
        "#\n",
        "# This module provides the final, top-level entry point for the entire SDM\n",
        "# framework. It integrates the main analytical pipeline with the comprehensive\n",
        "# robustness analysis suite, offering the user a single, powerful function to\n",
        "# execute either a standard analysis or a full-scale validation study.\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Top-Level Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_sdm_analysis(\n",
        "    experiment_config: Dict[str, Any],\n",
        "    perform_robustness_checks: bool = False\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the main SDM analysis pipeline and optionally a full suite of robustness checks.\n",
        "\n",
        "    This function is the primary public API for the entire SDM framework. It provides\n",
        "    a single entry point to conduct a full analysis.\n",
        "\n",
        "    The function will always perform one complete run of the SDM pipeline to get the\n",
        "    primary results. If `perform_robustness_checks` is set to True, it will then\n",
        "    proceed to execute the entire suite of computationally intensive validation\n",
        "    analyses, including parameter sensitivity, model substitution, and statistical\n",
        "    stability tests.\n",
        "\n",
        "    Args:\n",
        "        experiment_config: The raw input dictionary for the SDM experiment,\n",
        "                           conforming to the FusedExperimentInput structure.\n",
        "        perform_robustness_checks: If True, the full suite of robustness\n",
        "                                   analyses will be executed after the main run.\n",
        "                                   Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the results. The key 'main_run' will always be\n",
        "        present, holding the SDMFullResult object of the primary analysis. If\n",
        "        robustness checks were performed, an additional key 'robustness_analysis'\n",
        "        will contain a dictionary of DataFrames with the results of those tests.\n",
        "    \"\"\"\n",
        "    # Configure logging for the entire execution.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - [%(levelname)s] - %(message)s',\n",
        "        datefmt='%Y-%-m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # A dictionary to hold all the final results.\n",
        "    final_output: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # --- Main Pipeline Execution ---\n",
        "        # A single run of the main pipeline is always performed.\n",
        "        # This provides the core SDM metrics for the given configuration.\n",
        "        main_run_result: SDMFullResult = run_sdm_pipeline(experiment_config)\n",
        "\n",
        "        # Store the comprehensive result object from the main run.\n",
        "        # We use asdict to convert the dataclass to a dictionary for a consistent output format.\n",
        "        final_output['main_run'] = asdict(main_run_result)\n",
        "\n",
        "        # --- Optional Robustness Analysis Execution ---\n",
        "        # Check if the user has requested the full suite of robustness checks.\n",
        "        if perform_robustness_checks:\n",
        "            # Log a clear warning about the increased time and computational cost.\n",
        "            logging.warning(\n",
        "                \"Parameter `perform_robustness_checks` is True. \"\n",
        "                \"Proceeding with the full robustness analysis suite. \"\n",
        "                \"This will significantly increase execution time and cost.\"\n",
        "            )\n",
        "\n",
        "            # Execute the master orchestrator for all robustness analyses.\n",
        "            robustness_results = run_full_robustness_analysis(experiment_config)\n",
        "\n",
        "            # Store the dictionary of result DataFrames.\n",
        "            final_output['robustness_analysis'] = robustness_results\n",
        "        else:\n",
        "            # If robustness checks were not requested, log this information.\n",
        "            logging.info(\"Skipping optional robustness analysis suite as per configuration.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any catastrophic failure from the pipeline or robustness checks.\n",
        "        logging.critical(f\"The SDM analysis execution failed catastrophically. Error: {e}\", exc_info=True)\n",
        "        # In case of failure, return the partial results obtained so far, along with an error message.\n",
        "        final_output['error'] = str(e)\n",
        "        return final_output\n",
        "\n",
        "    # Return the final, consolidated dictionary of all results.\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "RIGok9NRqy5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}